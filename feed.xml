<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Comfortably Numbered]]></title>
        <description><![CDATA[My blog.]]></description>
        <link>https://hardmath123.github.io</link>
        <image>
            <url>https://hardmath123.github.io/static/avatar.png</url>
            <title>Comfortably Numbered</title>
            <link>https://hardmath123.github.io</link>
        </image>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 24 Feb 2021 06:52:27 GMT</lastBuildDate>
        <atom:link href="https://hardmath123.github.io/feed.xml" rel="self" type="application/rss+xml"/>
        <author><![CDATA[Hardmath123]]></author>
        <language><![CDATA[en]]></language>
        <item>
            <title><![CDATA[Mr. Bentley's Blizzard-Blossoms]]></title>
            <description><![CDATA[<p>Variations on a theme; or: synthesizing snowf*akes; or: unsupervised learning at Zoom University</p>


<style>
input[type="range"] {
  display: block;
  width: 100%;
}

#imgbox {
  display: inline-block;
  position: relative;
}

#world {
  margin: 0;
  padding: 0;
  max-width: 100%;
}
</style>

<script src="/static/blizzard-blossoms/onnx.min.js"></script>
<script>

window.addEventListener('load', function() {
  var LATENT = 10;
  var fieldset = document.getElementById('sliders');

  sliders = [];
  for (var i = 0; i < LATENT; i++) {
    var slider = document.createElement('input');
    slider.type = 'range';
    slider.min = -4;
    slider.max = +4;
    slider.step = 0.5;
    fieldset.appendChild(slider);
    sliders.push(slider);
  }

  function randomize(suppress_flurry) {
    for (var i = 0; i < LATENT; i++) sliders[i].value = Math.random() * 6 - 3;
    sliders[9].value = +4;
    sliders[5].value = -4;
    update();
    if (!suppress_flurry) flurry();
  }

  document.getElementById('randomize').addEventListener(
    'click', function() {randomize(false)}
  );

  // create a session
  const myOnnxSession = new onnx.InferenceSession();

  function update() {

    // generate model input
    var values = [];
    for (var i = 0; i < LATENT; i++) {
      values.push(sliders[i].value);
    }
    const inferenceInputs = [
      new Tensor(new Float32Array(values), "float32", [1, LATENT])
    ];

    // execute the model
    myOnnxSession.run(inferenceInputs).then((output) => {

      // consume the output
      const outputTensor = output.values().next().value;

      var can = document.createElement('canvas');
      can.height = 32;
      can.width = 32;
      var ctx = can.getContext('2d');
      ctx.scale(can.width / outputTensor.dims[2], can.height / outputTensor.dims[3]);
      for (var x = 0; x < outputTensor.dims[2]; x++) {
        for (var y = 0; y < outputTensor.dims[3]; y++) {
          var value = outputTensor.get(0, 0, x, y);
          ctx.fillStyle = `rgba(100, 100, 255, ${value})`;
          ctx.fillRect(x, y, 1, 1);
        }
      }

      var img = document.getElementById('world');
      img.width = can.width;
      img.height = can.height;
      img.src = can.toDataURL('image/png');
    });

  }

  // load the ONNX model file
  myOnnxSession.loadModel("/static/blizzard-blossoms/model.onnx").then(function() {
    for (var i = 0; i < LATENT; i++) {
      sliders[i].addEventListener('input', update, false);
      sliders[i].addEventListener('change', function() {update(); flurry();}, false);
    }
    document.getElementById('loading').remove();
    randomize(true);
  });


  function flurry() {
    var flake = document.createElement('img');
    var img = document.getElementById('world');
    flake.height = img.height;
    flake.width = img.width;
    flake.src = img.src;
    flake.style.position = 'absolute';
    flake.style.top = img.offsetTop;
    flake.style.left = img.offsetLeft;
    document.getElementById('imgbox').appendChild(flake);

    var dx = 0;
    var xv = Math.random() - 0.5;
    var dy = 0;

    var dtheta = 0;
    var dphi = 0;

    function step_animation() {
      dx += xv;
      xv = xv * 0.99;
      dy += 1;

      dtheta += 1;
      dphi += 1;

      flake.style.transform = `translate(${dx}px, ${dy}px) rotateX(${dtheta}deg) rotateY(${dphi}deg)`;

      if (dy < 1000)
        window.requestAnimationFrame(step_animation);
      else
        flake.remove();
    }

    step_animation();
  }

});
</script>

<fieldset id="sliders">
<span id="loading">(Please be patient as we wait for the snowflake factory to load&hellip;)</span>
<center><span id="imgbox"><img id="world" height=32 width=32></img></span></center><br/>
<center><button id="randomize">Roll the dice</button></center><br/>
</fieldset>









<p>Snow and snowflakes have been on my mind lately. This is not entirely because
of the storm in Texas — rather, I have been thinking since Christmas about
the snowflake’s <em>shape:</em> those small, delicate, spiky circles we all know so
well (err, perhaps there is something of current events here after all…).
This is a blog post about snowflake shapes.</p>
<p>What shape <em>is</em> a snowflake? In principle, we all know what snowflakes look
like. We can all imagine the six-fold symmetry, and then we can imagine the
embellishments.  And yet— not so fast! As a little child I (and I do not
think I am alone in this) wanted to see the shape of a snowflake for myself.
Of course, I never quite could — at least, to the point where I could stuff
and saturate my eyes — because the delicate snowflakes melted away as soon as
my snow-blind eyes could focus on the crystal caught. Think about it, then:
where does your own conception of snowflake-shape arise from? Does it come from
firsthand experience, or from clip-art and wintertime papercrafts?</p>
<blockquote>
<p>(Of course, there <em>is</em> something to be said for indirect inference of
snowflake-shape. Years later, I would marvel at the fact that you could infer
that snowflakes are hexagonal from the existence of <a href="https://github.com/kach/rocket-flame-renderer">22º
haloes</a>, a kind of
crystallography at the heavenly scale, linking the macroscopic and
microscopic. For now however I am concerned with seeing <em>directly</em>, and face
to face, with a presence and visual immediacy that cannot be afforded by
imagination alone.)</p>
</blockquote>
<p>This is no small concern. A year or so ago I visited the California Academy of
Sciences and learned about Wilson Bentley, the first man to photograph a
snowflake. This was on January 15, 1885; he was just shy of 20 years old. To
appreciate this moment, you have to imagine a time <em>before</em> January 15, 1885,
when no such photograph existed. Until Bentley’s innovation, no man had seen a
single snowflake for more than a delicate glimpse. Perhaps spatially the
snowflake could be magnified under a microscope (as <a href="https://en.wikipedia.org/wiki/Timeline_of_snowflake_research">Hooke and his
contemporaries</a>
demonstrated in the 17th century), but how could anyone magnify it
<em>temporally?</em> Bentley’s Edgertonesque miracle — akin to Muybridge’s “horse in
motion,” captured just seven years earlier — is to hold still the moment to
reveal <em>more</em> — to give us the opportunity to gaze without urgency. Dare I
say, Bentley <em>freezes time?</em> The manipulation of time becomes an <a href="electric-guitar.html">act of
unconcealment</a>, and then furthermore an act of creation.
Bentley’s snowflakes outlived him, and they will outlive me.</p>
<hr>
<p>Wilson Bentley was not a scientist by training, and perhaps that served him
well. Over the course of his life he collected over 5,000 images of what he
called “ice flowers.” There is something marvelous about this devotion, which
spanned years and years, crystal after crystal, wonder undiminished. Surely the
5,000th snowflake brought Wilson as much delight as the 4,999th (and surely it
is the case that you know you are in love when you find yourself falling in
love afresh every day).</p>
<p>There is also something to the uniformity of Bentley’s collection. Each
snowflake is framed carefully in the center of a slide, on a black background.
The images are diligently cropped to squares. See for yourself! The University
of Wisconsin-Madison hosts an <a href="https://library.ssec.wisc.edu/bentley/">online
archive</a> of 1,181 of Bentley’s images,
courtesy the Schwerdtfeger Library. Bentley never sought copyright on his
images; they are in the public domain, and so I downloaded them, along with
their labeled categories. The categories have wonderful names, such as “crystal
with broad branches” and “lumped graupel.”</p>
<p>When displayed in a grid, the snowflakes look like a box of delicious holiday
sweets. Surely these delights are made of sugar and not ice.</p>
<p><img src="/static/blizzard-blossoms/snowflakes.png" alt="snowfakes"></p>
<p>What does one <em>do</em> with 1,181 snowflakes? One searches for patterns, the way
Bentley would never have been able to. Bentley’s images demand to be studied as
samples from a distribution, and they are strikingly well-suited to our
favorite statistical computer vision techniques. For example, though they are
scanned at a uniform 1000x1000 resolution, they still make sense when
dramatically downscaled, even to as low as 32x32. Notice also how the
snowflakes are highly amenable to <em>data augmentation</em> on account of their
intrinsic symmetry. Each snowflake can be reflected and rotated arbitrarily to
produce several more high-quality synthetic snowflakes “for free.” So, even
though there are nominally 1,181 unique images, there is much more juice here
than one might expect at first. You can’t always play this game: CIFAR images
can’t really be rotated, and most MNIST digits can’t even be flipped. But
snowflakes are snowflakes.</p>
<p>One February evening I trained — with a lot of help from my friends Gautam
and Jack — a baby variational autoencoder, which embeds the snowflakes in a
small low-dimensional latent space. Here, “low-dimensional” means
10-dimensional: it seems very reasonable to encode 2^10 data points with 10
parameters. In the image below, the top row consists of five randomly-selected
source images, and the bottom row is the VAE’s attempted reconstruction. The
contrastive green-and-purple colorscheme comes from matplotlib’s default, and
reminds me of the delightful pGLO experiments we did in high-school biology.</p>
<p><img src="/static/blizzard-blossoms/vae.png" alt="snowfakes"></p>
<p>Of course, as with all things deep-learning, this was easier said than
done. For a technique that falls broadly under “unsupervised learning,” a VAE
requires an awful lot of babysitting! —but then again, one of the joys of
online school is that <em>nobody can tell</em> if you’re training machine learning
models behind your Zoom tab. So I trained, and I trained. The current model
takes around 6 hours on my dusty old MacBook, though after the first 30 minutes
the results are already non-embarrassing.</p>
<blockquote>
<p><strong>Do you want to build a snowgan?</strong> Separately, I also spent some time trying
to build a model adversarially. The snowgan (so to speak) consists of two
rival sisters: the generator, Elsa, is of the same architecture as the VAE’s
decoder, and the discriminator, Anna, learns to distinguish Elsa’s output
from real snowflakes. Elsa and Anna compete, and as a result Elsa gets better
at camouflaging the fakes among the flakes. Alas, GAN convergence is finicky
enough that I lost interest after a few days… open problem!</p>
</blockquote>
<p>It turns out that the VAE’s decoder can be used to build a small-scale
snowflake factory. Using ONNX.js and only a little bit of guile, I ported the
VAE’s decoder to JavaScript and attached the latent space to some <code>&lt;input
type=&quot;range&quot;&gt;</code> sliders. By using the sliders to choose parameters in the latent
space and feeding these parameters to the decoder, one can interpolate to
hallucinate snowflakes that are <em>not</em> part of Bentley’s training set. With some
experimentation, I found the sliders that correspond to parts of the latent
space that map to outliers; those parameters I clamped to extreme values in the
non-outlier-ey direction. This interface is what you see at the top of this
blog post.</p>
<blockquote>
<p>At time of writing, there is only one Google search result for “this
snowflake does not exist,” which points to a 2017 <a href="http://etd.lib.metu.edu.tr/upload/12621180/index.pdf">master’s
thesis</a> in
elementary-school education by Eli̇f Dedebaş. The context is an activity where
children are making paper snowflakes, and investigating the relationship
between the number of folds and the symmetries. One child creates a
snowflake-like shape with <em>eight</em>-fold symmetry and is asked if she knows why
such a shape is unnatural. Shape!</p>
</blockquote>
<hr>
<p>I imagine this slider infrastructure is on par with the sort of fancy tooling
UI that they have up in the Great Snow Factory In The Sky. I’m reminded of
David Wiesner’s wonderful Caldecott-winning picture book <em>Sector 7</em>, which I
recall reading (“reading”) in my elementary school’s library. It is about a
cloud factory, accessed from the top of the Empire State Building. Notice how
the large sousaphone-bell-shaped openings emanating from the factory roof are
exactly a portrait of an autoencoder’s decoder.</p>
<p><img src="/static/blizzard-blossoms/sector-7.png" alt="snowfakes"></p>
<p>“Wiesner visited the Empire State Building on a zero-visibility day to research
this book,” reads the About-the-Author at the end, “He was the only person
there.” There is something of Wiesner in Bentley and Bentley in Wiesner, I
think. Think of them both, and yourself, too, as Friedrich’s <em>Wanderer</em>.</p>
<p><img src="/static/blizzard-blossoms/wanderer.png" alt="snowfakes"></p>
<hr>
<p>The results interpolated by the VAE are surprisingly convincing.  Here are some
snowflakes that do not exist (snowfakes?). I created these images by sliding
the sliders haphazardly and recording a trace of the changing image.  Often,
people display this kind of interpolation in animated form, but in relation
— and, indeed, deference — to Bentley’s life’s work, I would rather see
them spread spatially than temporally.</p>
<p><img src="/static/blizzard-blossoms/snowfakes.png" alt="snowfakes"></p>
<p>I enjoy trying to work out the exact moment of transition between different
kinds of snowflakes; there is a snowy saltation that occurs at certain
boundaries that I cannot quite point you to. A curious fact is that a t-SNE
plot in the VAE’s latent space shows no clustering with respect to the provided
labels; the VAE seems to have hallucinated its own classification scheme, one
which is perhaps more macroscopically morphological in nature than Magono and
Lee’s 1966 classification scheme.</p>
<hr>
<p>The BentleyBlizzardBlossoms (B3) dataset, along with some PyTorch utilities and
the source code and trained model of the baby autoencoder, is available <a href="https://github.com/kach/bentley-blizzard-blossoms">on
Github</a>. I think it would
make a wonderful alternative to MNIST for small experiments. If you use it for
anything, please let me know!</p>
]]></description>
            <link>https://hardmath123.github.io/bentley-blizzard-blossoms.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/bentley-blizzard-blossoms.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 23 Feb 2021 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Learning to Play the Chaos Game]]></title>
            <description><![CDATA[<p>I, Thomasina Coverly, have found a truly wonderful method whereby all the forms of nature must give up their numerical secrets and draw themselves through number alone…</p>


<p>It’s Christmastime again! Shall we talk about trees? This post is about my new
holiday hobby: hallucinating tree-shaped fractals using our old friend,
gradient descent.</p>
<p><img src="static/chaos-game/tree-building.gif" alt="Christmas tree"></p>
<hr>
<p>But — let me start at the beginning. For a variety of reasons, the cover
image of Douglas Hofstadter’s <a href="https://en.wikipedia.org/wiki/I_Am_a_Strange_Loop"><em>I am a Strange
Loop</em></a> has been on my mind
this month. I have been thinking a lot about this wonderful image created by
pointing a camera at a projector that displays what the camera is seeing. In
modern terms: what happens if you screen-share the screen sharing window? This
happens:</p>
<p><img src="static/chaos-game/strange-loop.png" alt="from the cover of the book"></p>
<p>Such “feedback loop” recursions often rapidly converge to breathtaking
fixed-points. The NYC MoMath even has an <a href="https://momath.org/25-feedback-fractals-1/">art
installation</a> that lets you play
with a camera-and-projector setup to make <a href="https://twitter.com/momath1/status/824740594564075520">wonderful
patterns</a>.</p>
<p>As you can imagine, there is a mathematical theory here. At risk of ruining the
mystery, I will tell you about it, though without writing any equations. :-)
The theory is the theory of <a href="https://en.wikipedia.org/wiki/Iterated_function_system">Iterated Function
Systems</a>, which is
almost exactly what it sounds like. Start with a set of affine functions
(affine because that’s what camera-projector-systems do) and repeatedly apply
them to a set of points, taking the union at each step. Regardless of where you
start, you will soon end up with a fractal structure which is the fixed point
of the system. Why fractal? —because the self-similarity comes from the
infinitely-nested composition of the affine functions.</p>
<p><img src="static/chaos-game/Ifs-construction.png" alt="from Wikipedia"></p>
<p>There are theorems that ensure convergence towards and uniqueness of this fixed
point in the limit, but… it’s believable enough, don’t you think? Following
your intuition, you can build Sierpinski Triangles and <a href="https://en.wikipedia.org/wiki/Barnsley_fern">Barnsley
Ferns</a> and all sorts of other
beautiful structures using an IFS. I refer you to Section 8.2 of <a href="http://algorithmicbotany.org/papers/#abop"><em>The
Algorithmic Beauty of Plants</em> by Prusinkiewicz and Lindenmayer (yes, <em>that</em>
Lindenmayer)</a> for more botanical
connections.</p>
<p><img src="static/chaos-game/Fractal_fern_explained.png" alt="from Wikipedia"></p>
<hr>
<p>Now here is the question that has been on my mind: if I give you a picture of a
fractal-fern, can you give me a set of affine functions whose fixed point is
that fern? This is the <a href="https://en.wikipedia.org/wiki/Iterated_function_system#The_inverse_problem"><em>inverse
problem</em></a>
for IFSes. According to Wikipedia, this problem has real-world applications in
image compression, but it is <em>hard</em> to do in general.</p>
<p>Hmm. Interesting. Can our favorite tool, gradient descent, come to the rescue?
At least in an approximate sense? Recall that a two-dimensional affine function
is really just a 3x3 matrix (with 6 free parameters) that encodes the details
of the transformation. If we could easily compute images of IFS fixed points
from a set of matrices, then perhaps we could try to optimize the parameters of
these matrices to make the fixed point look the way we want it to.</p>
<p>The challenge, of course, is efficiently <em>finding</em> the fixed point —
repeatedly rasterizing and compositing affine transformations on images sounds
expensive — and doing so differentiably sounds like a scalability nightmare!
But there is a wonderful solution to this problem, which is to play the
so-called “<a href="https://en.wikipedia.org/wiki/Chaos_game">chaos game</a>.” Instead of
<em>densely</em> applying all the functions to all the points on the plane, you can
<em>sparsely</em> approximate the fixed point as follows: Start with a point — any
point! — and <em>randomly</em> select one of the affine functions of the IFS and
apply it to the point. Repeat this process with the new point. It turns out
that in the limit, the “trail” left behind by this wandering point converges to
the fixed point of the IFS. (This, too, is a theorem, but again I think it is
believable enough that I will not demand a proof.) This is what the “chaos
game” looks like for the Sierpinski triangle; notice how the salt-and-pepper
spattering of points soon converges boldly into the fractal we know and love
(it’s an animation — stare for a few seconds).</p>
<p><img src="static/chaos-game/Sierpinski_chaos_animated.gif" alt="from Wikipedia"></p>
<p>So here is the plan: we start with a point, and play the chaos game with our
current IFS matrices to obtain a <em>point cloud</em> that stochastically approximates
the fixed-point. Then, we compare this <em>point cloud</em> to our target fern-image
to obtain a “loss.” Finally, we update our IFS matrix parameters to minimize
this loss, until at last the fixed-point converges to the target image. It’s
just “machine learning,” really: we’re learning to play the chaos game!</p>
<p>Here is an outline of the algorithm (with some details elided):</p>
<pre><code class="lang-python"># initialize IFS
F = [random_3x3_affine() for _ in range(4)]
o = torch.optim.Adam(F, lr=0.0001)

for step in range(100_000):
    o.zero_grad()
    # start at origin
    v = torch.tensor([0., 0., 1.])

    # play the chaos game once
    trace = []
    for _ in range(200):
          # applying an affine function is just
        # a matrix multiplication!
        v = torch.matmul(random.choice(F), v)
        trace.append(v)

    # treat the trace as a point cloud
    loss = compare(target_image, trace)

    # update parameters
    loss.backward()
    o.step()
</code></pre>
<p>Ah! Actually, there is <em>one</em> detail that I <em>should</em> explain: how do you compare
a point cloud and an image? My idea is to convert the target image to a
<em>second</em> point cloud by uniformly sampling points from it, for example by
rejection-sampling. Then you can compare the two point clouds by the so-called
“chamfer distance,” which is the mean distance from each point to its nearest
neighbor in the opposite point cloud. This is quadratic-time to compute, and
the most expensive part of the whole operation, but with ~100 points in each
set it is quite doable.</p>
<blockquote>
<p>A pedagogical aside: notice that the “trick” here is really a <em>change in
representation.</em> This optimization problem is easier to solve on point clouds
than on raster images! An important lesson, that applies across the
ML-for-graphics domain… and more broadly to all differentiable programming
enterprises.</p>
</blockquote>
<hr>
<p>Remarkably, this zany scheme works quite well! Let’s try it with a heart. (A
heart, because it’s an easy low-entropy shape, but also because it felt
symbolically appropriate in relation to Hofstadter’s broader philosophical
project of souls-within-souls-within-souls.)</p>
<p>When we initialize the system with 4 random affine transformations, the trail
left behind by the chaos game is very unimpressive. The sparsity is because I
simulate only 200 steps of the chaos game for each step of gradient descent —
there’s a tradeoff between noise and computation time, of course, as there is
with any form of SGD.</p>
<p><img src="static/chaos-game/heart-samples-early.png" alt="chaos game on a heart"></p>
<p>Okay. Time to optimize! Here is what it looks like after 100K steps of
optimization with an Adam optimizer (that’s about 40 minutes of wall-time
computation on my old MacBook). It looks pretty good, don’t you think? The
chamfer distance scheme works!</p>
<p><img src="static/chaos-game/heart-samples.png" alt="chaos game on a heart"></p>
<p>Now we can export the four affine matrices out from PyTorch and work out the
“true” fixed point with a more expensive offline computation (for example, by
simulating a few <em>million</em> steps of the chaos game). It looks like this — not
perfect, and indeed quite crayon-scribble-ey, but clearly <em>something</em>
interesting has happened, and the result is convincingly heartlike.</p>
<p><img src="static/chaos-game/heart-fractal.png" alt="extended chaos game on a heart"></p>
<p>Where is the fractal structure hidden in this heart? This needs some
vizualization tooling to see clearly. After a bit of JS-canvas-hacking: here is
a GIF of the heart that reveals its fixed-point structure.
Hearts-in-hearts-in-hearts!</p>
<p><img src="static/chaos-game/heart.gif" alt="Heart"></p>
<p>I will admit to blinking a few times when I first saw this animation play out
in full — I did <em>not</em> expect it to work, and even now I find myself marveling
at this creation? discovery? of a heart encoded in 24 numbers.</p>
<hr>
<p>But it still doesn’t look <em>quite right</em> — the fractalness isn’t <em>obvious</em>.
How come? It’s because our affine transformations stretch and squash the heart
into almost unrecognizable blobs. A final, natural improvement is to force our
affine transformations to be <em>rigid</em>, so that there isn’t any squishing or
skewing in the fractal. This is actually not too bad to implement: the solution
to problems of constraint are typically reparametrizations, and indeed in this
case we can simply reparametrize the matrices in terms of a rotation, an
(isotropic) scale, and a translation. Now we’re down from 6 parameters per
matrix to just 4 — even more magical! The result is a much more “obviously”
fractal-ey fixed point, simply because your eye can more easily pick out the
structural recursion when it is made of rigid motions.</p>
<p>Let’s take it for a — pardon — <em>spin!</em></p>
<p>Since I promised you trees, here is some fractal foliage. The target image was
a picture of a maple leaf! I’d never thought about this before, but indeed a
maple leaf contains within it an echo of the whole tree. (Full disclosure: it
takes a couple of randomized restarts to get a really impressive result — the
low dimensionality of the fractal’s parametrization leads to the same
stuck-in-a-local-optimum woes that differentiable rendering folks face all the
time…)</p>
<p><img src="static/chaos-game/foliage.gif" alt="Leaf"></p>
<p>This one uses just three matrices — experimentally, three matrices tend to be
enough to get really good results, and more just lead to very busy and crowded
fractals. (I don’t know if there is a word for number-of-maps–in-an-IFS, but
let’s call it the IFS’s <em>arity</em>. In this terminology, I like <em>ternary</em> IFSes.)</p>
<p>And finally, since I promised you Christmas trees…</p>
<p><img src="static/chaos-game/christmas-tree.gif" alt="Christmas tree"></p>
<p>This last animation is actually absolutely <em>baffling</em> to me. In part, this is
because of how <em>treelike</em> the fractal turned out — here it is overlayed
against the silhouette I optimized against. Can you imagine this is encoded by
just 12 parameters? Odd.</p>
<p><img src="static/chaos-game/christmas-tree-match.png" alt="Tree vs. silhouette"></p>
<p>But even more bafflingly: the actual geometry of the recursion is <em>nothing</em>
like the tree recursion geometry you and I are used to from CS106! Compare the
GIF above to Barnsley’s Fern: while Barnsley turns each full leaf into two
smaller leaves with “semantic” affine maps, this Christmas tree does all sorts
of <em>bizarre</em> uninterpretable cartwheels and somehow almost magically works
itself out in the end. It is mesmerizing to me, I can stare at it for minutes
at a time.</p>
<p>When you think about it, it is quite shocking that a Christmas tree is the
<em>unique</em> fixed point of these 3 rigid affine maps. “Beautiful” isn’t the word
for it — neither is “grotesque” (though both words have been used by various
people I have shared this with). There is something just unsettlingly
fascinating about the way something magical <em>pops out</em> from three rectangles.
Maybe I should have demanded a proof of that theorem after all…</p>
<hr>
<p>There is so much more to say about all of this. What shapes are easiest to
IFSify, and why? In higher dimensions, can we treat the IFS as a kind of
SVD-esque dimensionality reduction scheme for data (i.e. literal point clouds)?
What kind of data would it make sense to do this for — where do you find
spatial self-similarity? Do point clouds have an inherent IFS “dimensionality”
(in terms of, say, the number of affine transformations you need to get a good
approximation)? With some more work, could SGD be a legitimate solution to the
open problem of fractal compression?</p>
<p>It’s a lot to think about — but for now… well, there is a Calvin and Hobbes
gag that begins with Calvin saying “I’ve been thinking” and Hobbes interrupting
“On a weekend?” That’s how I’ve been feeling this winter break!  Now it is time
to shutdown the jupyter kernel and get some rest — Comfortably Numbered will
return in 2021!</p>
<p>(Jupyter notebook for this blog post available <a href="https://github.com/kach/chaos-game-fractal-foliage">on
Github</a>.)</p>
<hr>
<p>Update (Dec 27): Check out some
<a href="https://news.ycombinator.com/item?id=25551557">improvements</a> made by a reader!</p>
<p>Update (Dec 29): I was pointed to <a href="http://demo.cs.brandeis.edu/papers/wcci98.pdf">a 1998
paper</a> (Melnik and Pollack) that
(independently!) tells <em>exactly</em> the same story. I’m struck by how closely the
two expositions align. There is something deeply comforting about rediscovering
a place someone else has visited once-upon-a-time, like finding a flag buried
under the snow on a mountaintop — a kind of storybook enchantment that sits
across the table from loneliness.</p>
]]></description>
            <link>https://hardmath123.github.io/chaos-game-fractal-foliage.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/chaos-game-fractal-foliage.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 25 Dec 2020 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Systems for Coordinating Coordinate Systems]]></title>
            <description><![CDATA[<p>Making 3D graphics code safer by telling vectors where home is</p>


<p>When I took Pat and Matt’s wonderful graphics class CS348B (yay PBRT!), one of
the most frustrating debugging journeys I had was related to interpreting
vectors in the wrong coordinate systems. At the time I’d come up with an idea
to catch such bugs statically, and now, 1.5 years later, I finally have found
time to write down the idea.</p>
<p>Here is the problem: A vector only has “meaning” with respect to a coordinate
system. In graphics, a vector could be in all sorts of coordinate systems:
<em>world space</em>, or <em>object space</em>, or <em>camera space</em>, or <em>screen space</em>, or
<em>normalized device coordinate (NDC) space,</em> or <em>raster space</em> (some of these
are self-explanatory, but see <a href="http://www.pbr-book.org/3ed-2018/Camera_Models/Projective_Camera_Models.html">PBR
&sect;6.1.1-6.2</a>
for definitions). This leads to bugs. If you interpret a “camera-space” vector
as if it were in “world space,” your math will be <em>all wrong!</em>  You need to
apply the affine transformation <code>cameraToWorld</code> before doing any world-space
computation on a camera-space vector. But what if you forget to do that? Your
C++ code will happily compile, but your poor bunny might look like chicken soup
instead… (I learned this the hard way!).</p>
<p><img src="static/pbrt-coordinate-system-diagram.png" alt="some pbrt coordinate systems"></p>
<p>This got me thinking: perhaps a safer <code>Vector3f</code> should know <em>at the type
level</em> what coordinate system it is in. Rather than a <code>Vector3f</code>, perhaps what
we really want is a <code>Vector3f&lt;WorldSpace&gt;</code>. You simply can’t add a
<code>Vector3f&lt;CameraSpace&gt;</code> to a <code>Vector3f&lt;WorldSpace&gt;</code> — that’s a <em>type error!</em>
The compiler <em>makes</em> you call <code>cameraToWorld</code> if you want to do anything with
these two vectors. (All this is very much in the spirit of
<a href="https://github.com/kach/torchsaber">torchsaber</a>, by the way.)</p>
<p>I think there are broadly two ways you might want to implement something like
this. One way is to treat <code>CameraSpace</code> as a purely formal symbol, just a
<em>name</em> or an <em>annotation</em>. The compiler’s job is to check these annotations.
You’d have to explicitly break this abstraction inside the implementation of
<code>cameraToWorld</code>, but from a pedagogical perspective I think that is exactly
what you want.</p>
<p>The other way is for the tag <code>CameraSpace</code> to somehow encode the actual
geometry of what <em>camera space</em> “means.” For example, each vector could also
carry around the <code>Transformation</code> that gets you to its host coordinate system
from <code>WorldSpace</code>. Given this information, the compiler could even be able to
<em>infer</em> the implementation of <code>cameraToWorld</code> for <em>free!</em> Of course, this comes
at a cost, which is that the compiler might not be able to statically check
equality of <code>Transformations</code>, because the matrices might not be materialized
at compile-time.</p>
<hr>
<p>In the rest of this post I’ll quickly literate-ly program option (1) as a proof
of concept. It is short and sweet (at least, as “sweet” as any bit of C++
hacking can be).</p>
<p>First, we define our coordinate system “annotations” as dummy classes.</p>
<pre><code class="lang-c++">class Space {};
class WorldSpace : public Space {};
class CameraSpace : public Space {};
</code></pre>
<p>Next, we design a <code>Vec3</code> template class that takes in such a “tag” as a
template parameter (<code>CoordinateSystem</code>), and enforces that the tags are the
same on operations. I’ve implemented the “<code>+</code>“ operator but you can imagine the
rest — there is nothing sneaky going on here! In fact, this is exactly how
one implements a generic <code>Vector3</code> that can be specialized over numeric types
(e.g.  <code>Vector3&lt;float&gt;</code>, <code>Vector3&lt;int&gt;</code>, and so on).</p>
<pre><code class="lang-c++">template &lt;class CoordinateSystem&gt;
class Vec3 {
  static_assert(
    std::is_base_of&lt;Space, CoordinateSystem&gt;::value,
    &quot;Vec3 annotation must derive from Space&quot;
  );

public:
  double x, y, z;
  Vec3(double x, double y, double z)
    : x(x), y(y), z(z) {};

  // the &#39;+&#39; operator!
  Vec3&lt;CoordinateSystem&gt; operator+(
    Vec3&lt;CoordinateSystem&gt;&amp; other
  ) {
    return Vec3(
      this-&gt;x + other.x,
      this-&gt;y + other.y,
      this-&gt;z + other.z
    );
  }
};
</code></pre>
<p>(Note that you need to <code>#include &lt;type_traits&gt;</code> to get <code>std::is_base_of</code>, which
is there just to protect you from trying to make a <code>Vec3&lt;string&gt;</code> or something
else uncouth like that. Completely optional.)</p>
<p>…actually, that’s it! We can already get some mileage out of this. For
example, this should be okay:</p>
<pre><code class="lang-c++">Vec3&lt;WorldSpace&gt; p(0, 0, 0);
Vec3&lt;WorldSpace&gt; q(1, 1, 1);
auto r = p + q; // ok!
</code></pre>
<p>On the other hand, this is perhaps <em>not</em> okay:</p>
<pre><code class="lang-c++">Vec3&lt;CameraSpace&gt; s(2, 2, 2);
auto t = r + s; // NOT OK!
</code></pre>
<p>And indeed, the compiler complains <em>and</em> gives a helpful error message!</p>
<pre><code>test.cc:40:14: error: no match for &#39;operator+&#39;
(operand types are &#39;Vec3&lt;WorldSpace&gt;&#39;
and &#39;Vec3&lt;CameraSpace&gt;&#39;)
   40 |   auto t = r + s; // NOT OK!
      |            ~ ^ ~
      |            |   |
      |            |   Vec3&lt;CameraSpace&gt;
      |            Vec3&lt;WorldSpace&gt;
</code></pre><p>Of course! How <em>dare</em> we add a camera-space vector to a world-space vector? We
need to transform <code>s</code> to world-space…</p>
<pre><code class="lang-c++">Vec3&lt;WorldSpace&gt; cameraToWorld(
  Vec3&lt;CameraSpace&gt;&amp; vec
) {
  return Vec3&lt;WorldSpace&gt;(
    vec.x * 2, vec.y - 4, vec.z + 8 // or whatever
  );
}
</code></pre>
<p>…and now <code>g++</code> is happy. :)</p>
<pre><code class="lang-c++">auto u = r + cameraToWorld(s); // ok!
</code></pre>
<p>Perhaps this trick will make its way into PBRT someday!</p>
<hr>
<blockquote>
<p>Update (Dec 23): Rachit Nigam pointed me to this (very recent) SPLASH
<a href="https://2020.splashcon.org/details/splash-2020-oopsla/49/Geometry-Types-for-Graphics-Programming">paper</a>
that considers the same problem. (“See? It’s not just me!”)</p>
</blockquote>
]]></description>
            <link>https://hardmath123.github.io/systems-for-coordinating-coordinate-systems.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/systems-for-coordinating-coordinate-systems.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 15 Dec 2020 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Clogs and Leaks: Why My Tensors Won't Flow]]></title>
            <description><![CDATA[<p>Defining a class of tricky bugs in PyTorch programs</p>


<blockquote>
<p><em>Note:</em> Jekyll and Hyde — is how this blog goes. There are weeks of
rational thought and weeks of irrational ramblings. This fall has been much
of the latter, for good reason, but here is a break(!) in the clouds.</p>
<p>Okay, okay. The truth is that I wrote this essay for my CS class this quarter
(leave it to Pat to assign an essay for a CS class!). But then, I think I
have reached the age where every written assignment in college is to be
treated as an opportunity to say something I otherwise would not have a
chance to say… and so the essay, lightly edited, finds its way to this
blog.</p>
</blockquote>
<hr>
<p>In this essay I want to describe two kinds of tricky bugs that might creep into
your PyTorch programs. I call the bugs “clogs” and “leaks.” In my mind “clogs”
and “leaks” reveal an exciting possible research direction for anyone
interested in designing better APIs for automatic differentiation in machine
learning.</p>
<p><em>Note: The examples presented, though in principle timeless, were tested using
Python 3.8.5 running PyTorch 1.6.0.</em></p>
<h2 id="preliminaries-pytorch-s-pipes">Preliminaries: PyTorch’s Pipes</h2>
<p>If you are familiar with PyTorch internals, you can skip this section. If not,
a brief review of a wonderful topic: how does PyTorch differentiate your code
for gradient descent? The technical term for PyTorch’s approach is <em>tape-based
reverse-mode automatic differentiation</em>. As you perform arithmetic computations
on your variables, PyTorch tracks the intermediate values in a computation
graph. When you want to differentiate a value, you call <code>.backward()</code> on that
value. PyTorch then walks <em>backwards</em> along this computation graph, computing
the derivative at each step and accumulating them according to the chain rule.
Eventually, the leaf nodes of the graph contain the derivatives you asked for.</p>
<p>Let me give a small example. Suppose we wanted to compute ($d2x^2/dx|_{x=3}$).
We might write a program that looks like this:</p>
<pre><code class="lang-python">x = torch.tensor(3., requires_grad=True)
y = 2 * x**2
y.backward()
print(x.grad)
</code></pre>
<p>This program generates the following computation graph.</p>
<p><img src="static/clogs-and-leaks/forward.png" alt="Forward"></p>
<p>When you call <code>.backward()</code>, the PyTorch automatic differentiation walks
backwards along the graph, computing derivatives at <em>each</em> step. By the chain
rule, the product of these gives the overall derivative we sought.</p>
<p><img src="static/clogs-and-leaks/backward.png" alt="Forward"></p>
<h2 id="clogs">Clogs</h2>
<p>Now, consider this simple PyTorch program to compute ($d(\sqrt{x} +
x)/dx|_{x=4}$). What do you expect to be printed?</p>
<pre><code class="lang-python">x = torch.tensor(4., requires_grad=True)
y = sqrt(x) + x
y.backward()
print(x.grad)
</code></pre>
<p>A casual user or AP calculus student would <em>expect</em> to see 1.25 printed, of
course. But what <em>actually</em> gets printed is 1. Why?</p>
<p>Ah! I didn’t show you the full program: I hid the imports. It turns out that
the first line of this program is <code>from math import sqrt</code>, <em>not</em> <code>from torch
import sqrt</code>. Now, the Python standard library’s <code>math.sqrt</code> is not a
PyTorch-differentiable function, and so PyTorch is unable to track the flow of
derivatives through <code>sqrt(x)</code>.</p>
<p>As a result of this bug, backpropagation gets “stuck” on the way back, and only
the derivative of <code>x</code>, i.e. 1, is deposited. This is a clog — the gradients
can’t flow! In the computation graph below, the dotted arrow represents the
clog.</p>
<p><img src="static/clogs-and-leaks/clog.png" alt="Clog graph"></p>
<p>The reason calling <code>math.sqrt()</code> on a PyTorch tensor is not a runtime error is
that PyTorch tensors implicitly convert to “raw” floating-point numbers as
needed. Most of the time this is a useful and indispensable feature. But I
believe this situation should <em>at the very least</em> raise an error or a warning.
While the example I presented was reasonably straightforward, there are <em>many</em>
different ways to “clog” backpropagation, with varying degrees of insidiousness
(for example, what happens when you mutate a variable in place?). It can be a
nightmare to debug such situations when something goes wrong — that is, if you
notice the bug in the first place!</p>
<p><em>By the way:</em> the celebrated “reparametrization trick” that powers variational
autoencoders is really just a workaround for a gradient clog problem. To train
a variational autoencoder, you need to compute the derivative of a sample of a
probability distribution with respect to the distribution’s parameters (e.g.
the mean ($\mu$) and variance ($\sigma^2$) of a Gaussian distribution).
Unfortunately, naïvely sampling from a parametrized distribution abruptly
truncates the computation graph with respect to the parameters, because the
random number generator is not differentiable all the way through — who <em>knows</em>
what <em>it’s</em> doing! The solution, is to sample from a standard unit normal
distribution (where ($\mu=0$) and ($\sigma=1$)), and then re-scale the sample
by multiplying by ($\sigma$) and adding ($\mu$). Of course, multiplication and
addition <em>are</em> easily differentiable, and so the gradients can now flow.
Problem solved!</p>
<h2 id="leaks">Leaks</h2>
<p>Now, consider this slightly more complicated PyTorch program. We are going to
implement a silly reinforcement learning algorithm. Here is the situation:
There is a truck driving on the road with constant velocity, and your goal is
to catch up to it and drive right alongside the truck. At each timestep you are
allowed to choose your velocity, and then you’re told how far you are from the
truck.</p>
<p>The setup:</p>
<pre><code class="lang-python">truck_velocity = torch.tensor(3.142)
truck_position = torch.tensor(2.718)

def get_measurement(car_position):
    global truck_position
    truck_position = truck_position + truck_velocity
    return torch.abs(truck_position - car_position)
</code></pre>
<p>And a simple online gradient-based learning algorithm:</p>
<pre><code class="lang-python">my_velocity = torch.tensor(0.01)
my_position = torch.tensor(0.)
for i in range(500):
    my_velocity.requires_grad_()
    my_position = my_position + my_velocity
    loss = get_measurement(my_position)
    loss.backward()
    my_velocity =\
        my_velocity.detach() - my_velocity.grad * 0.01
</code></pre>
<p>Unlike last time, there’s nothing up my sleeve here — this is all reasonable
PyTorch code. This code actually works just fine.</p>
<p>But, if you run it for long enough (say, 1000 iterations), you’ll notice
something odd: each step starts taking longer and longer. The algorithm is
<a href="https://accidentallyquadratic.tumblr.com">accidentally quadratic</a>! You can see
this behavior quite clearly in this graph, which shows a linear growth in
iteration time from step to step (the spikes are garbage collection pauses).</p>
<p><img src="static/clogs-and-leaks/graph.png" alt="Time graph, essentially linear and
increasing"></p>
<p>How can this be? Isn’t each loop doing the same calculation?</p>
<p>Here is one hypothesis: if you’ve read <a href="https://arxiv.org/abs/1909.13371">this
paper</a> you might look to see if we’re
<code>.detach()</code>-ing <code>my_velocity</code>. The <code>.detach()</code> function snips off all incoming
edges to a node in the computation graph; essentially, creating an artificial
clog. If we forget to do that, the gradients would “leak” back in time across
multiple steps in the graph, all the way back to the first step, and each
iteration would therefore take longer and longer — just as we’re observing.</p>
<p>But, alas, this is not the source of the bug: as you can see, we <em>are</em>
detaching <code>my_velocity</code> when we update it. So, what’s really going on here?</p>
<p>It’s tricky! The leak is in <code>my_position</code>, which subtly depends on <em>all</em>
previous values of <code>my_velocity</code> and therefore makes backpropagation compute
gradients for <em>all</em> previous timesteps. The dataflow diagram below hopefully
clarifies this point. Notice how each <code>velocity</code> has its parent nodes detached
(thanks to the call to <code>.detach()</code>!), but <code>loss</code> still has an indirect
dependence on the chain of <code>positions</code>.</p>
<p><img src="static/clogs-and-leaks/leak.png" alt="Leak graph"></p>
<p>Finding the correct place to insert the line <code>my_position =
my_position.detach()</code> is left as a not-quite-trivial exercise to the reader.
Beware! Putting it in the <em>wrong</em> place will either have no effect <em>or</em> cause
<code>my_velocity</code> to always have gradient 0.</p>
<p>Just like memory leaks, gradient leaks can be extremely sneaky. They pop up
whenever your inference is “stateful” — think of applications like physics
controllers, reinforcement learning, animated graphics, RNNs, and so on. I
would not be surprised if many popular implementations of such algorithms <em>do</em>
have “gradient leak” bugs. However, the bugs usually only manifest themselves
visibly when the inference passes through enough timesteps for the leak to
compound. Just like a dripping tap, you might not notice your losses until you
get the bill at the end of the month… and then, you need to figure out how to
track down the source of the leak and figure out the right way to fix it.</p>
<h2 id="plungers-and-patches-an-appeal-for-plumbing-">Plungers and patches? An appeal for PLumbing…</h2>
<p>In the long term, how can we protect ourselves from this class of bugs? One
potential solution is to embed the API inside a language whose type system
tracks the creation of the computation graph. You might be able to use
well-understood techniques like <em>taint analysis</em> or <em>linear types</em> (pun not
intended) which traditionally track the flow of <em>information</em>, to now track the
flow of <em>differentiability</em> through the program.</p>
<p>Let me be slightly more concrete about this suggestion. In our “clog” example,
a good type system might detect that <code>sqrt</code> cuts off the computation graph,
and, knowing that ($y$) does not directly depend on ($x$) in the expected way
anymore, complain at compile-time when we try to request ($dy/dx$). In our
“leak” example, a good type system might notice that the “old” <code>my_position</code>
effectively goes out of scope when it is re-assigned, and therefore it might
complain that an unreachable reference to it actually does persist through the
new <code>my_position</code>. Such checks seem very reasonable to demand from a modern
type system.</p>
]]></description>
            <link>https://hardmath123.github.io/clogs-and-leaks.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/clogs-and-leaks.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 23 Nov 2020 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Public Time and the Privacy of Time]]></title>
            <description><![CDATA[<p>An old Levi’s ad that ran when I lived in India went like this: “It’s the thread in your seams that’s tied to your dreams. It’s the soles in your feet that keep the beat.”</p>


<p>Over the months I’ve started to miss the little speckles of Stanford life that
I never thought I would get attached to. One of them is the regular bell-toll
of our lovely little clock tower. There is something oddly compelling about
public time — time as a utility — time as something to be broadcast for
consumption. So I have a special fondness for clock towers, and the Network
Time Protocol, and the anchor who says “it’s 8am and this is NPR,” and the GPS
system, and people who set their clocks by the atomic clock in Boulder, and the
website <a href="https://www.time.gov">time.gov</a>, and the pilot who says “we will be
landing shortly in San Francisco, the local time is 6:15am.” At least, in this
divided world, we can agree on <em>something</em>.</p>
<p><img src="static/electric-guitar/edgerton.jpg" alt="edgerton"></p>
<hr>
<p>Well: This evening I was tuning my guitar, and I discovered some more public
time. Here is what happened. First I plucked the low E string, and watched the
string blur into a lenslike pair of arks that envelop the extent of its
vibration. I ran the first few frets to try out the sound: F, F-sharp, G,
G-sharp. Great!  Next, I plucked the A string, and watched the same blur of the
string. But when I tried plucking it at the first fret — that would be a
B-flat — something bizarre happened. Instead of a vibrating blur, the string
slowly wavered back and forth between its two extremes!</p>
<p>What? I rubbed my eyes. A low “Bb” vibrates at around 117 Hz. That is <em>much</em>
faster than my eye can resolve (because of this “persistence of vision,”
animations in video games don’t have much incentive to exceed around 60 frames
per second). In short, I should be seeing a blur. But no! I can track the
string’s leisurely wobble quite easily. I bet you can, too: the GIF below shows
an open-string A on the left and a first-fret Bb on the right.</p>
<p><img src="static/electric-guitar/comparison.gif" alt="gif"></p>
<p>(Sorry for the poor quality — I had to hold my phone between my knees — it
was tricky, okay? You can find the original videos, with sound, on Github.)</p>
<hr>
<p>Here’s what I think is going on. The lights in my room are powered by AC
current from the power grid. In the United States, AC current is standardized
to transmit at 60 hertz (why 60? apparently it’s a <a href="https://en.wikipedia.org/wiki/Utility_frequency#Standardization">long
story</a>, but in
part the number 60 is related to turbine hardware logistics). That means that
the current peaks in (absolute) voltage 120 times per second, and so the lights
in my room are actually all rapidly flickering at 120 Hz. I just don’t realize
it because of the persistence of vision.</p>
<p>Ah! As I mentioned above, a Bb vibrates at around 117 Hz — to be precise,
it’s 116.54 Hz. You can work this out without looking it up by recalling that
concert A is 440 Hz (as printed on every metronome) and the low Bb is two
octaves less one half step below — that’s twice twelve minus one or 23 half
steps below. Indeed, 440/(2^(23/12)) gives 116.54.</p>
<p>Why is this correspondence between 120 and 116.54 so interesting? Well, when
you have nearby frequencies, they inferfere to produce
<a href="https://en.wikipedia.org/wiki/Beat_(acoustics%29">“beat”</a> frequencies.  I’ve
written about beats in the spatial domain before when talking about <a href="moire.html">moiré
patterns</a>. Here, the beats are in the temporal domain, forming a
kind of <a href="https://en.wikipedia.org/wiki/Wagon-wheel_effect">“wagon wheel”</a>
illusion. The superposition of the string’s vibration and the light’s
flickering creates a low-frequency “beat” that gives the illusion of the string
vibrating slowly.</p>
<p>Eyeballing the right GIF, I count around 5 oscillations of the string over the
3-second clip. That gives a beat frequency of 1.67 Hz. The difference in
interfering frequencies is twice the beat frequency (elementary trigonometry
exercise, use the sum-to-product identity). Using that difference, I can
compute that my Bb is tuned to 120 Hz - 3.3 Hz = 116.7 Hz. Not bad! It turns
out that you can buy <a href="https://en.wikipedia.org/wiki/Electronic_tuner#Strobe_tuners">“stroboscopic
tuners”</a> online,
but perhaps if you are good at estimating slow frequencies, you don’t need
one…!</p>
<hr>
<p><img src="static/electric-guitar/muybridge.jpg" alt="muybridge"></p>
<p>Briefly, some thoughts for the future: I’m reminded by my guitar string of
Edgerton’s hummingbirds and Muybridge’s horses, and many more moments frozen in
time. There is a lot more I could say, and that I have said, and perhaps I will
explain this more carefully in a future essay. The confluence of the aural and
visual has also been on my mind lately; though I haven’t “blogged” about it I
have been <a href="https://cs.stanford.edu/~kach/can-one-hear-the-fate-of-a-coin.pdf">thinking a
lot</a> about
clocking coin tosses by recording the sound a coin makes while it rings. In
that story, too, stroboscopic techniques make an appearance. Maybe I will say
more about that as well.</p>
<p>For the moment however I am struck — “struck” — by how strange it is that
there is so much invisible to us even in plain sight, so much hidden in the
folds of time. Public time, and the privacy of time.</p>
<p><img src="static/electric-guitar/duchamp.jpg" alt="duchamp"></p>
]]></description>
            <link>https://hardmath123.github.io/electric-guitar.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/electric-guitar.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 13 Nov 2020 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Believing]]></title>
            <description><![CDATA[<p>Reprinting old thoughts on faith, mathematical and otherwise</p>


<blockquote>
<p>I was talking to a dear friend of mine this week, and she unknowingly
reminded me of a thought I had last year. I wrote down this essay rawly one
November night as an imagined chapter of an imagined book, then by morning
forgot it (and the book) unedited. Today’s moment — personal, political,
historical — seems as good as any to revisit it.</p>
<p>(Note: Reformatted automatically for web by pandoc. I’m sorry if the delicate
math typesetting breaks; then again, for this essay the math is at best
incidental to the message.)</p>
</blockquote>
<hr>
<p>The tricky integrals below are known as the Borwein integrals, and, as
the Mathematician diligently evaluates them one by one, she is treated
to a pattern.</p>
<p>$$\begin{aligned}
B_1 = \int_0^\infty \frac{\sin(x)}{x}dx &amp;= \frac{\pi}{2} \\
B_3 = \int_0^\infty \frac{\sin(x)\sin(x/3)}{x(x/3)}dx &amp;= \frac{\pi}{2} \\
B_5 = \int_0^\infty \frac{\sin(x)\sin(x/3)\sin(x/5)}{x(x/3)(x/5)}dx &amp;=
\frac{\pi}{2} \\
B_7 = \int_0^\infty \frac{\sin(x)\sin(x/3)\sin(x/5)\sin(x/7)}{x(x/3)(x/5)(x/7)}dx &amp;= \frac{\pi}{2}\end{aligned}$$</p>
<p>The aesthetics of mathematics demand at this point for her to formulate
a <em>conjecture</em>, which is that all Borwein integrals ($B_n$) are equal to
($\pi/2$).</p>
<p>Do you believe this conjecture? Perhaps, but our friend the
Mathematician is not so easily satisfied. She presses on and continues
evaluating the integrals, and she is rewarded with further evidence.</p>
<p>$$\begin{aligned}
B_9 = \frac{\pi}{2} \qquad
B_{11} = \frac{\pi}{2} \qquad
B_{13} = \frac{\pi}{2}\end{aligned}$$</p>
<p>But when she gets to ($B_{15}$), she finds unexpectedly (or is it unexpected?)
that it is ever so slightly <em>less</em> than ($\pi/2$). The pattern falls apart, the
conjecture evaporates, and with it our belief.</p>
<p>The previous essay was on seeing; this one is on believing, the act of
believing, the art of believing, the ethics of believing. What does it
mean to believe? We might say that to believe is to have beheld <em>proof</em>
that expels <em>doubt</em>. I turn to Arnold Ross’ Prologue to his eponymous
mathematical summer program held annually at the Ohio State University.
He writes,</p>
<blockquote>
<p>In an effective program of mathematical studies, students are
propelled by eager curiosity to observe and experiment, thus creating
new opportunities for observation. In hopes of unearthing deep
relationships, students search for patterns among those observations,
formulate adventurous conjectures, prune the many conjectures with the
sharp ax of possible counterexamples, and then attempt <em>to endow the
surviving conjectures with the security of a proof.</em></p>
</blockquote>
<p><em>To endow with the security of a proof.</em> Proof is comfort in certainty,
it is relief, it is contentment in the establishment of truth, the ghoul
of doubt having been vanquished. In our so-called “post-truth world,” it
can at times be unfashionable to simply believe, and it is
understandable for our Mathematician to have demanded proof to assuage
her well-placed skepticism.</p>
<hr>
<p><img src="static/believing/caravaggio.png" alt="Image of the painting."></p>
<p>But here matters become complicated. The painting on the right is
Caravaggio’s <em>Incredulity of Saint Thomas</em> (1601). It depicts the
apostle Thomas, who, having missed Jesus’ resurrection, demands proof.
Says Jesus, reach and feel my wounds — be not faithless, but believing
— <em>because thou hast seen me, thou hast believed: blessed are they
that have not seen, and yet have believed</em> (John 20:29). I look at the
furrowed brow of Thomas, his all but prosecutorial examination of the
evidence, and I wonder: What is the ugly side of <em>proof?</em> Is it right
even to use the word <em>belief</em> once you have <em>proof?</em> Or does it then
simply fade into the landscape of fact, or, dare I say, <em>reality?</em> I
think of those haunting words from the American Declaration of
Independence, “We hold these truths to be <em>self-evident</em>, that all men
are created equal.” <em>Self-evident:</em> not “it can be shown that all men
are created equal” or “assume for the sake of argument that all men are
created equal,” no, this truth is <em>self-evident</em>, it denies proof
because proof is unnecessary for an axiom. If even this simple matter of
human equality is up for debate, to be quartered by the Devil’s team of
reasoned and articulate advocates, then how can we proceed at all?</p>
<p>Christianity is not the only religious tradition to find value in the
<em>unproven</em>. I am reminded of Lao Tzu’s <em>Tao Te Ching</em>; I quote now from
Stephen Mitchell’s translation of the final verse:</p>
<blockquote>
<p>True words aren’t eloquent; </p>
<p>eloquent words aren’t true. </p>
<p>Wise men don’t need to prove their point; </p>
<p>men who need to prove their point aren’t wise. </p>
</blockquote>
<p>Or we might turn to an ancient Indian hymn:</p>
<blockquote>
<p>But, after all, who knows, and who can say 
Whence it all came, and how creation happened? 
The gods themselves are later than creation, 
so who knows truly whence it has arisen? 
Whence all creation had its origin, 
the creator, whether he fashioned it or whether he did not, 
the creator, who surveys it all from highest heaven, 
he knows — or maybe even he does not know.</p>
</blockquote>
<p>The unproven, the unknown — the unprovable, the unknowable — that is
where faith <em>begins,</em> where there are stakes to truth.</p>
<p>What, then, do we make of the 17th-century mathematician Blaise Pascal,
who proposed a game-theoretic argument now known as <em>Pascal’s Wager:</em>
that a rational person should live as though God exists because it is
the prudent option, on the off chance that He indeed does? Or shall we
invoke the brilliant 20th-century logician Kurt Gödel, whose seminal
work on “incompleteness theorems” introduced the marvelous notion of the
<em>provably unprovable</em> theorem — what can we make of Gödel’s
“ontological proof” of the existence of God, which has recently been
formalized in a computer system and computer-verified, in some sense
lending proof to the proof itself? “Ah! Like Doubting Thomas, these men
rely too much upon the crutch of rationality,” we might now complain,
“searching for and clinging to proof as if the blueness of the sky, the
sound of a brook, the smell of a peach were not proof enough of a
Creator.”</p>
<hr>
<p>But even this, I think, is too simple, because to the Mathematician
“proof” is more than the expulsion of doubt. Let us play the game of
conjecture again. Pick a prime number, and divide it by four. If the
remainder is one, try to write the prime number as the sum of two
perfect squares:</p>
<p>$$\begin{aligned}
5  = 4\cdot 1 + 1 &amp;= 2^2 + 1^2 \\
13 = 4\cdot 3 + 1 &amp;= 3^2 + 2^2 \\
17 = 4\cdot 4 + 1 &amp;= 4^2 + 1^2 \\
29 = 4\cdot 7 + 1 &amp;= 5^2 + 2^2\end{aligned}$$</p>
<p>Can this be done with all primes?</p>
<p>You might be more cautious to believe this time, so let me just tell
you: it can, the observation is known as Fermat’s theorem, and I can
supply two proofs. The first is an elegant argument that relates sums of
squares to points on a grid, and reasons carefully about the geometry of
that grid to conclude that a point with the desired properties must
exist. The proof is enlightening, it reveals among other things the
importance and origin of that number 4 that smiles so mysteriously in
the theorem’s statement. Knowing the deeper mathematical structure at
work, we can begin to generalize Fermat’s theorem in new directions; the
theorem becomes in a sense just one shadow cast by this structure.</p>
<p><img src="static/believing/zagier.png" alt="Zagier&#39;s proof"></p>
<p>The second proof, due to Zagier, was published in 1990 as a single
sentence, and is reproduced in its entirety on the right. It
hallucinates for us a truly bizarre object, asserts that it has a
certain symmetry that forces there to be an odd number of solutions to
the equation ($x^2 + (2\cdot y)^2 = p$), and, as zero is not odd,
concludes that there must be at least one solution. This is one of the
strangest proofs I have ever seen; it provides absolutely no insight
into <em>why</em> the theorem is true — though it certainly assures me <em>that</em>
it is true. Here it is not the theorem, but rather the proof that is the
shadow.</p>
<p>I show you these two proofs to suggest that not all proofs are created
equal. The great mathematician Paul Erdős spoke of The Book, wherein God
wrote the most elegant, insightful proof for every theorem of
mathematics. “You need not believe in God,” he would say, “but you
should believe in The Book.” A professor of mine once asked me if I
would like a copy of “The <em>Abridged</em> Book,” which for every conceivable
conjecture simply lists whether or not it is true. It is a shortened
form of the Book, so we can agree to treat its word as proof enough —
or perhaps it only provides succinct, convincing but unilluminating
proofs in the style of Zagier’s — but it says no more than “true” or
“false.” This is not an idle fantasy: already we have found fragments of
The Abridged Book in the form of long computer-generated proofs, such as
the proof of the Four-Color Theorem, which no human could ever check by
hand.</p>
<p>Here is the difference between The Book and The Abridged Book: one would
occupy the Mathematician’s mind for decades, the other would be
immediately cast aside as useless. To the Mathematician, the quest for a
proof is not to establish <em>that</em> a theorem is true, but rather <em>why</em>.
The proof is not <em>in</em> the pudding, the proof <em>is</em> the pudding.</p>
<hr>
<p>This winter I spent some time at a planetarium near Stanford, and I was
told by the announcer that the nearest star outside our solar system,
Proxima Centauri, is over twenty trillion miles away. <em>Twenty trillion!</em>
said the announcer, <em>isn’t that amazing?</em> And I thought — well, is it?
Would I be ten times as amazed if Proxima Centauri were two hundred
trillion miles away instead? Surely not. I am not sure what to do with
that number. I suspect that you are not, either, because as humans we
are not equipped to appreciate the celestial scale.</p>
<p>Let us agree then that there is really nothing interesting about that
number “twenty trillion” — ah, except for one thing, which is <em>that we
know it at all;</em> that we, earthbound, can measure the distances to the
stars. On a cloudless night the stars on the horizon appear to follow
you as you drive by the streetlights; really, it is the streetlights
passing you that create the illusion of relative movement, and the rate
of this movement depends on how far away the streetlights are, as a
consequence of elementary geometry. The same effect occurs at the
celestial scale: as the Earth moves around the Sun distant stars appear
to follow us in relation to the nearest stars, a delicate quiver in the
stars, and from this effect we can measure the distance to Proxima
Centauri. The streetlights are Proxima, our vehicle is Spaceship Earth.
To contemplate this, the consistency of geometry, in the small and in
the large, is to me a spiritual experience that assures me that I am
governed by the same laws of parallax that govern the stars I came from;
that is, it is not just all men but <em>all entities</em> that are created
equal before mathematics.</p>
<hr>
<p>There is a poem of Robert Frost’s, “Choose Something Like a Star,” where
the speaker pleads for a star to tell him how it burns</p>
<blockquote>
<p>And it says “I burn.”</p>
</blockquote>
<p>That’s all it says, but then the speaker continues</p>
<blockquote>
<p>But say with what degree of heat.</p>
<p>Talk Fahrenheit, talk Centigrade.</p>
<p>Use language we can comprehend.</p>
<p>Tell us what elements you blend.</p>
</blockquote>
<p>Frost reminds us of the dignity in the silence of the stars, and that
would be all well and good except for one thing: <em>that’s not how stars
are</em>. Stars speak to us when they radiate across time and space, and
they speak openly to anyone with a telescope. It falls on us — that
is, on humanity — to listen, to hear not only the temperature and
elemental composition of the stars, but also their biographies, the
stories of their births and their deaths.</p>
<hr>
<p>In the afterword to <em>Pilgrim at Tinker Creek,</em> Annie Dillard writes</p>
<blockquote>
<p>In October, 1972, camping in Acadia National Park on the Maine coast,
I read a nature book. I had very much admired this writer’s previous
book. The new book was tired. Everything in it was the dear old
familiar this and the dear old familiar that. God save us from
meditations. What on earth had happened to this man? Decades had
happened, that was all. Exhaustedly, he wondered how fireflies made
their light. I knew—at least I happened to know—that two enzymes
called luciferin and luciferase combined to make the light. It seemed
that if the writer did not know, he should have learned. Perhaps, I
thought that night reading in the tent, I might write about the world
before I got tired of it.</p>
</blockquote>
<p><em>To write about the world before you get tired about it.</em> The
Mathematician is not tired of the world, will never be tired of the
world, because she knows the world will never stop speaking to her.</p>
<p>And yes, even though Nature has over the course of this affair revealed
for us some Truth — the numerical distance to a star, its numerical
temperature — it is not that Truth but rather the revelation that was
the true blessing; the number, the Truth, it is just a byproduct, a
projection. I wonder whether in the moment of looking into Christ’s
wounds Thomas thought at all about the little matter of the
resurrection’s authenticity; I wonder if I, seeing Thomas fall to his
knees in understanding, would not myself wish to doubt again so that I,
too, could be shown the light. <em>How</em> we know is the light and <em>what</em> we
know the shadow; <em>proof</em> is the light and <em>theorem</em> is the shadow —
this is the Mathematician’s defense of her art.</p>
]]></description>
            <link>https://hardmath123.github.io/believing.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/believing.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Sun, 08 Nov 2020 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Sol LeWitt and the Soapy Pit]]></title>
            <description><![CDATA[<p>Rendering minimal surfaces of cubical loops</p>


<p>A 2014 <a href="https://www.math.ksu.edu/~rozhkovs/LeWitt_cubes.pdf">paper</a> by
Rozhkovskaya and Reb titled “Is the List of Incomplete Open Cubes Complete?”
complicates Sol LeWitt’s celebrated 1974 artwork, “Incomplete Open Cubes”. The
artwork is a <a href="https://www.metmuseum.org/art/collection/search/691091">gallery</a>
of 122 three-dimensional structures; they are meant to be all distinct
connected subsets of edges of the unit cube that result in a 3D figure — or,
in LeWitt’s words, “all the ways of the cube not being complete”. The
complication, noted by the paper, is that two of the structures are identical
up to rotation; they should instead be mirror reflections of each other.</p>
<p>Have art historians realized yet? I would like to think there is some deeper
philosophical significance to this bit of trivia. The exercise in exhaustive
enumeration of incompleteness is itself incomplete — and, we cannot help but
overlook the incompleteness, just as we cannot help but imagine the missing
edges of any of the 122 almost-cubes.</p>
<p><img src="static/minimal-surface/incomplete-open-cubes.png" alt="Incomplete Open Cubes"></p>
<blockquote>
<p>Image source: <a href="https://cubes-revisited.art">Incomplete Open Cubes Revisited</a></p>
</blockquote>
<p>I have <a href="eucalyptus.html">meditated on the art-historical significance of chirality
before</a>, so today I want to talk about something else. I don’t
know about you, but when I see LeWitt’s almost-cubes, I feel the need to
submerge them in a tank of soapy water. Why? To see what shape the films take,
of course! Is that irrational? Perhaps, but <a href="https://www.moma.org/collection/works/146945">writes
LeWitt</a>, “Irrational thoughts
should be followed absolutely and logically.” It turns out that my thought
isn’t hard to follow — if not absolutely and logically, then at least
computationally.</p>
<p>The shape of a soap film is a <a href="https://en.wikipedia.org/wiki/Minimal_surface">minimal
surface</a>, that is, the surface
with minimal area that obeys the boundary conditions of the “loop” that the
film forms within. The definition doesn’t depend at all on the properties of
soap: any film-forming substance should — in the absence of other forces,
such as gravity — contract to the same shape as a result of surface tension.
These shapes can often be pleasing and unexpected.</p>
<p><img src="static/minimal-surface/helicoid.png" alt="Soap film on a spring"></p>
<blockquote>
<p>Image source: <a href="https://commons.wikimedia.org/wiki/File:Bulle_de_savon_hélicoïde.PNG">Wikipedia</a></p>
</blockquote>
<p>Now, because finding the surface is a question of continuous minimization, we
should be able to easily apply automatic differentiation — at least, to a
discretization of the problem.  Here is the algorithm: we start with a mesh
grid of points and triangulate it. Then, we compute the surface area by
repeated application of Heron’s Theorem to the triangles. Finally, we
differentiate the total area with respect to the positions of the points, and
nudge the points towards less area. The full source code for this adventure is
available <a href="static/minimal-surface/minimal-surface.ipynb">here</a>. (By the way,
I’m using PyTorch out of habit, but perhaps TensorFlow would be more
appropriate because of its oddly LeWittian logo.)</p>
<hr>
<p>Before I give you the results, I want you to take a moment to try and imagine
what the soap films will look like. I think it’s an interesting exercise in
qualitative reasoning, and, in any case, LeWitt’s work all but pleads for us to
see what isn’t there.</p>
<p>For starters, we can reduce the entire problem from 122 cubes into just a
handful of cases. Simply ask: how many distinct-up-to-symmetry “loops” are
there on a cube?  Let’s do casework by face count: there is 1 loop with one
face (the “square”), 1 loop with two faces (the “L”), and 2 loops with three
faces (the “U” and the “corner”). The rest are accounted for by complement: a
four-face loop is also a two-face loop.</p>
<p>This yields a total of 4 cases — much more manageable! And indeed, I’ll
suggest that you start with Case 1, the “square”, since the answer in that case
should be obvious.</p>
<hr>
<p>Okay, now, the results!</p>
<p>Case 1 is easy; we already know the answer. The soap film is just flat, because
its boundary is planar.</p>
<p><img src="static/minimal-surface/case_1.gif" alt="Case 1, the square"></p>
<p>Case 2 is less obvious — I would not have predicted this outcome! It turns
out that the soap film tries to “flatten” itself into the hypotenuse. What are
the implications of this? Well, one implication is that if you want to minimize
the material you use to build a tent (without regard to volume), you can do
better than stretching the material taught into large rectangles. Sagging
actually <em>saves</em> material.</p>
<p><img src="static/minimal-surface/case_2.gif" alt="Case 2, the &quot;L&quot;"></p>
<p>You might expect Case 3a to flatten similarly, but because of the up-down
symmetry that can’t happen. Instead, you get a beautiful saddle shape.
Actually, it turns out that all minimal surfaces are saddle-like; if they had
nonzero curvature at some point then you should be able to “flatten the bulge”
to reduce surface area.</p>
<p><img src="static/minimal-surface/case_3a.gif" alt="Case 3a, the &quot;U&quot;"></p>
<p>And finally, Case 3b — sorry this isn’t quite perfectly modeled — tries to
flatten itself around the center, kind of like a tortilla chip.</p>
<p><img src="static/minimal-surface/case_3b.gif" alt="Case 3b, the &quot;corner&quot;"></p>
]]></description>
            <link>https://hardmath123.github.io/minimal-surface.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/minimal-surface.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Sun, 13 Sep 2020 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Hybrid Hyperoptimization]]></title>
            <description><![CDATA[<p>The most ambitious crossover event in automatic differentiation history</p>


<blockquote>
<p> Summary of this post: It turns out that by carefully mixing forward-mode and
 reverse-mode automatic differentiation, you can greatly simplify certain
 hyperparameter optimization algorithms.</p>
</blockquote>
<p>Here’s a fun recursive idea: just like how we optimize the parameters of
machine learning models by gradient descent, we can also their
<em>hyperparameters</em> by gradient descent. This is by no means a new idea; you can
find 20-year-old papers that discuss (for example) optimizing gradient descent
step sizes by gradient descent itself.</p>
<p>Broadly, there are two ways to do this. The “short-term” way is to take a
<em>single</em>  step of gradient descent, and then based on how that goes, adjust the
hyperparameters. That’s the premise of the 2018 paper <a href="https://arxiv.org/abs/1703.04782">Online Learning Rate
Adaptation with Hypergradient Descent</a>: it
turns out that you can do this in an extremely lightweight way. You would hope
that over time the hyperparameters converge to something optimal alongside the
parameters (and indeed that is the case, though you have to be careful).</p>
<p>The “long-term” way is to train <em>several</em> steps and <em>then</em> backpropagate
through the <em>entire training</em> to adjust the hyperparameters. The hope is that
this provides a stronger “signal” to the hyperparameter-gradient, which is
better for convergence. But, this comes at the (tremendous) expense of having
to store a copy of the entire computation graph for several steps of training,
and then backpropagate through it. If your model has 1GB of parameters, you
need to store ~1GB worth of numbers for <em>each step</em>. You can imagine that adds
up over the course of many epochs, and so the algorithm is severely limited by
how much RAM you have.</p>
<p>The 2015 paper <a href="https://arxiv.org/abs/1502.03492">Gradient-based Hyperparameter Optimization through Reversible
Learning</a> makes this work by throwing away
the intermediate steps of the computation graph and then JIT-re-computing it
“in reverse” during backpropagation. It’s a clever technique, but kind of hairy
and hard to implement (you have to be super careful about numerical precision).</p>
<p>Here’s a graph that visualizes these two techniques (<a href="https://arxiv.org/abs/1909.13371">from this
paper</a>). You’re looking at several parallel
loss curves (($\log(f)$) is the loss plotted on a log scale), pointing towards
you, arranged in order of the “step size” hyperparameter
(that’s ($\log(\alpha)$)). The orange curve represents a “short-term”
hyperparameter optimization, which is allowed to move along the “step size”
axis at each step. The “long-term” hyperparameter optimization instead
optimizes directly on the thick black dashed “U” — that is, after <em>several</em>
steps of training. You can see how the latter is smoother, but also much harder
to compute.</p>
<p><img src="static/hybrid-hyperoptimization/fig-metasurface.png" alt="The preceding paragraph explains this
figure."></p>
<p>To summarize: the short-term way is cheap but noisy. The long-term way is
expensive but less noisy. Can we get the best of both worlds?</p>
<hr>
<p>Well, let’s think more carefully about the source of the expense. The problem
is that we need to store (or be able to reconstruct) the full computation graph
in order to do backpropagation. Okay, but do we really <em>need</em> to do
backpropagation? The only reason we backpropagate is that it’s more efficient
in the case when you want derivatives with respect to <em>many</em> different
variables. If you have millions of model parameters, backpropagation is
millions of times faster than the much simpler <a href="https://en.wikipedia.org/wiki/Automatic_differentiation#Automatic_differentiation_using_dual_numbers">forward-mode (“dual numbers”)
automatic
differentiation</a>.</p>
<p>But we <em>don’t</em> have millions of <em>hyperparameters!</em> Step size, for example, is
just a single number. The Adam optimizer only has a total of 4 hyperparameters.
With this in mind, backpropagation isn’t even the right choice — we <em>should</em>
be using dual numbers for the hyperparameter optimization. On the other hand,
we should still be using backpropagation for the “inner loop” that optimizes
the (non-hyper-) parameters. That is, we want to do something like this:</p>
<pre><code class="lang-python">initialize hyperparameters
# loop to optimize hyperparameters
while True:
  initialize parameters
  # loop to optimize parameters
  for i in range(100):
    run model on data to get loss
    # using reverse-mode!
    compute d(loss) / d(parameters)
    update parameters using hyperparameters
  # using forward-mode
  compute d(loss) / d(hyperparameters)
  update hyperparameters
</code></pre>
<blockquote>
<p>(Update: I discovered that this was suggested in the 2017 paper <a href="https://arxiv.org/abs/1703.01785">Forward and
Reverse Gradient-Based Hyperparameter
Optimization</a>. But keep reading — while
the paper’s
<a href="https://github.com/lucfra/FAR-HO/blob/master/far_ho/hyper_gradients.py#L375">implementation</a>
needs a lot of math to be worked out manually, I’m going to show you how to
implement this in a way that makes all the math in the paper fall out “for
free”…)</p>
</blockquote>
<p>This proposal raises a logistical question: how do we reconcile these two
automatic differentiation algorithms in the same program? <strong>The “trick” is to
“thread” dual numbers through a backpropagation implementation.</strong> In other
words, implement backpropagation as usual, but rather than <code>float</code> type
numbers, exclusively use <code>dual_number</code> type numbers (even when doing derivative
calculations). Initialize the system such that the dual numbers track
derivatives with respect to the hyperparameters you care about. Then, your
final loss value’s attached ($\epsilon$)-value <em>immediately</em> gives you
<code>d(loss)/d(hyperparameters)</code>. No backpropagation needed — and so, it’s safe
to “forget” the computation graph.</p>
<p>That’s it! That’s all I wanted to share in this blog post! :)</p>
<p>Of course, it’s not obvious how to implement this in PyTorch, since you can’t
naïvely do <code>tensor(dtype=dual_number)</code>. Rather than hack in a custom numeric
data type that implemented dual numbers, I wrote my own tiny implementations of
forward- and reverse-mode automatic differentiation. It’s just a couple dozen
(very-recognizable-to-automatic-differentiation-enthusiasts) lines of code. I
was careful to make each implementation generic in the kind of “number” it
accepts. That allowed me to run the reverse-mode algorithms using the
forward-mode data types.</p>
<p>Running it on a simple quadratic optimization problem, we can see that updating
the hyperparameter ($\alpha$) yields better-looking loss curves — in this
GIF, we’re discovering that we should increase the step size. Yay!</p>
<p><img src="static/hybrid-hyperoptimization/loss.gif" alt="GIF of loss curve becoming better over
time"></p>
<p>I think this is a very neat trick, which surely has other cool applications
(for example, differentiating through long-running physics simulations!). If
you’re curious, check out this <a href="static/hybrid-hyperoptimization/hybrid-hyperoptimization.ipynb">IPython
notebook</a> for
the full source code. Feel free to adapt it to your ideas. Then, tell me what
you’ve built!</p>
]]></description>
            <link>https://hardmath123.github.io/hybrid-hyperoptimization.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/hybrid-hyperoptimization.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 11 Sep 2020 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[GPT-3 and Claude Shannon Visit the Tartu-Moscow School]]></title>
            <description><![CDATA[<p>An information-theoretic approach to the question of “AI Literature”</p>


<blockquote>
<p>“I do not know what ‘poetical’ is: is it honest in
deed and word? Is it a true thing?” — Lotman quoting Pushkin quoting Shakespeare in <em>As You like It, 3.3</em>.</p>
</blockquote>
<p>How do I want to say this? Here is one way: Earlier this year, the world was <em>very surprised</em> at a computer doing <em>very unsurprising</em> things. The celebrated computer program, <a href="https://arxiv.org/abs/2005.14165">OpenAI’s GPT-3</a>, is a <em>language model</em>, a machine learning model that (to simplify) outputs the statistically-likeliest completion of a sentence, much like the autocomplete on your iPhone keyboard. If we wrote the prefix</p>
<blockquote>
<p>The weather is warm and…</p>
</blockquote>
<p>GPT-3 might output “sunny” or “humid” — that would be probable and unsurprising. But it would likely not output “turtledove” — that would be improbable and surprising. Of course, you can run this program again and again to sample more and more words, and thus generate long sequences of text.</p>
<ul>
<li>The weather is warm and [sunny]</li>
<li>The weather is warm and sunny [so]</li>
<li>The weather is warm and sunny so [let’s]</li>
<li>The weather is warm and sunny so let’s [go]</li>
<li>The weather is warm and sunny so let’s go [to]</li>
<li>The weather is warm and sunny so let’s go to [the]</li>
<li>The weather is warm and sunny so let’s go to the [beach.]</li>
</ul>
<p>In principle this is nothing new. There was a GPT-1 and a GPT-2, too, as you can imagine. But GPT-3 is bigger and better — it has “read” an <em>enormous</em> corpus of text to build up its database, and the model itself consists of <em>175 billion</em> parameters (GPT-2 was a “mere” 1.5 billion parameters). Just training a model of this size is an exciting engineering accomplishment.</p>
<p>By virtue of its size, GPT-3 is extremely good at hallucinating text in this way. And this is where things begin to get uncanny. If you phrase your prompt as a question, GPT-3 reliably provides answers in complete sentences. If you phrase your prompt as a few coding examples, GPT-3 can generate software to build small user interfaces based on a description of what buttons you want. It can even generate convincing <a href="https://maraoz.com/2020/07/18/openai-gpt3/">hoax blog posts</a>. Don’t worry, what you are reading right now is written by a human — but it is probably not unreasonable to be a little concerned.</p>
<p>I think it is natural to ask whether it can generate “literary” text as well — whether it is a kind of superintelligent monkey that <em>will</em> generate <em>Hamlet</em> if you ask it to. Now, it’s pretty clear that GPT-3 is not quite a Great Automatic Grammatizer (I have written <a href="fabula.html">in the past</a> about what <em>that</em> may look like). But for short fragments, as lots of people have shown, it <a href="https://www.gwern.net/GPT-3">isn’t embarrassing either</a>. And so, as we hurtle towards a future where statistically-sound “literary text” can be mass-produced, I have been thinking a lot about what “literature” is.</p>
<hr>
<p>One set of thoughts comes from an unlikely (so to speak) source, which is the early-1970s literary theory of Yuri Lotman (or Jüri, or Jurij — you choose how to spell it — this becomes relevant soon). In <em>The Structure of the Artistic Text</em> (1971, but I’m referencing Ronald Vroon’s 1977 <a href="https://monoskop.org/images/3/3e/Lotman_Jurij_The_Structure_of_the_Artistic_Text_1977.pdf">translation</a> and <a href="https://www.jstor.org/stable/306702">Ewa Thompson’s guide</a>), Lotman connects Shannon’s and Kolmogorov’s quantitative theories of communication and entropy with the qualitative question <em>what is literature?</em>. At least, that is my understanding — I’m not an expert in Russian literary theory or semiotics, but I’m trying my best.</p>
<p>Let’s think back to one of Shannon’s big ideas, which is that language is <em>redundant</em>, which means there are many ways to convey the same idea. Because of its redundancy, language is resilient to noise in the communication channel (such as loud music at a party). If I stprt intrdducinq mutnt1ons into my spellings, you can still more or less get the message. Similarly, most ideas can be paraphrased. Or, to put it differently, you can convey a concept in many different ways. That is, notions can be materialized into words in multiple fashions. If there were no redundancy in the language — that is, if every combination of letters formed a different valid word, or if there were only one way to express each idea — then this error-correction would be impossible. Imagine if each volume in Borges’ Library of Babel was a meaningful masterpiece! Imagine spilling a teardrop on a page, only to change the meaning dramatically!</p>
<p>This redundancy is the reason why text is so <em>compressible</em>, why a ZIP file can be so small compared to the source it is compressing. You can just strip out the redundancy to get a smaller file: your disk’s I/O channel has negligible noise compared to a loud party, and so the redundancy is not needed. (Any introductory theory-of-computation course touches on <em>Kolmogorov Complexity</em>, a kind of theoretical limit to compressibility in terms of computability: it turns out that Kolmogorov worked with language and literature as well.)</p>
<p>Perhaps this redundancy makes GPT-3’s behavior a little less surprising (I hope that word “surprising” is accruing meaningfulness for you). The more redundancy a language has, the more about the statistical structure of language you can “learn” from data, and therefore the easier it is to predict likely completions. Indeed, <a href="https://bellard.org/textsynth/sms.html">GPT models can be used to perform data compression</a>.</p>
<p>Now, my understanding of Lotman’s thesis is that <em>literature does not behave this way</em>. Literature is <em>more entropic</em>, which is to say, literature is <em>less redundant</em>. It is packed with meaning — in fact, every choice is intentional and significant. Lotman argues that literature lives outside or above “ordinary” natural language; a work of literature creates its own “code,” and each word takes on a heightened meaning in this code. To read literature is to learn to decipher this code. He writes, <em>The artistic text is an intricately constructed thought. All its elements are meaningful elements.</em> Once a text is composed, it is baked to an immutable, brittle crust. “The reader considers the text placed before him as the only possible text.”</p>
<p>For example — and this is my example, not Lotman’s — the repetition in a line from <em>Lear</em></p>
<blockquote>
<p>No, no, no, no! Come, let’s away to prison:</p>
</blockquote>
<p>is exact and precise. Four times, no more, no less — and each utterance of the word “no” is significant, dripping with intention. The raw sound “no” itself becomes a kind of meaningful sign, detached from its usual English-language semantics. You cannot, without losing meaning, compress that text as simply “4 x no.”</p>
<p>Let me give you another example. Consider Roethke’s “The Waking.” It’s a sestina, a poem built on repetition. Every other stanza ends with the same line.</p>
<blockquote>
<p>I wake to sleep, and take my waking slow.<br>I feel my fate in what I cannot fear.<br><strong>I learn by going where I have to go.</strong></p>
</blockquote>
<p>This repeats.</p>
<blockquote>
<p>Of those so close beside me, which are you?<br>God bless the Ground! I shall walk softly there<br><strong>And learn by going where I have to go.</strong></p>
</blockquote>
<p>Until, suddenly and jarringly (in the softest way possible), it doesn’t.</p>
<blockquote>
<p>Great Nature has another thing to do<br>To you and me; so take the lively air,<br><strong>And, <em>lovely</em>, learn by going where to go.</strong></p>
</blockquote>
<p>That word <em>lovely</em> appears so deliberately in that line that its absence in the previous lines is conspicuous. As a result, those repeated lines, which seemed like blind repetitions, become conscious choices. Roethke could have said <em>whatever he wanted</em> but he chose to repeat himself verbatim, without the word “lovely,” in what Lotman might call a kind of “minus-device,” a trembling vacuum, an absence with presence — that in itself is meaning, that in itself holds entropy.</p>
<p>It would not do for Roethke to paraphrase those lines, either. “I educate myself by traveling where needed” has no impact. Because literary text is stripped of redundancy, even small mutations — even semantics-preserving ones — can dramatically affect the text. It’s like compiling for embedded targets with <code>-O4</code>. Like <code>gcc</code>, Lotman does not believe you can paraphrase literature to get equivalent literature. You cannot re-tell a story, because there are no synonyms in the unique code of that particular work of literature. I am reminded of Borges’ “Pierre Menard, Author of the Quixote,” a short story wherein Menard <em>writes</em> (not re-writes) <em>Don Quixote</em>.</p>
<blockquote>
<p>Pierre Menard did not want to compose <em>another</em> Quixote, which surely is easy enough—he wanted to compose <em>the</em> Quixote. Nor, surely, need one be obliged to note that his goal was never a mechanical transcription of the original; he had no intention of <em>copying</em> it. His admirable ambition was to produce a number of pages which coincided—word for word and line for line—with those of Miguel de Cervantes.</p>
</blockquote>
<p>Maybe, in light of the discussion above, these bizarre lines are a little less mystifying, a little more satisfying.</p>
<p>That brings me back to GPT-3, and the literary status of its output. Can we ever call a GPT-3-generated poem a work of literature? Well, following Lotman, let’s instead ask this: how much information, really, does GPT-3 output? How entropic is GPT-3’s output? And of course the answer is that GPT-3 is quite orderly in the grand scheme of things. “Whenever the text does not realize one of at least two possibilities,” writes Lotman, “but automatically follows one, it loses its capacity to convey information.” That is, every time GPT-3 argmaxes its way to a prediction predestined by its statistical model, which is really a synthesis of gigabytes and gigabytes of data, it is outputting — well, nothing. No new information. The occasional choice made by the random sampling is the only “information” being produced, though it is hidden in the fluffy sheep-skin of statistically-unobjectionable prose. If GPT-3 completed the prompt-word “yesterday” with Roosevelt’s “a date which will live in infamy” speech half the time, and Beatles’ “All my troubles seemed so far away” lyrics the other half the time, then GPT-3 has only really communicated one bit of information; the rest is redundant. Indeed, if you were to measure the number of bytes read from <code>/dev/urandom</code> (or whatever RNG) by GPT-3 in the course of sampling, and analyze the relative probabilities predicted by the model, perhaps you would be able to compute a Shannon entropy — or even a convincing literariness score — for the output. I sense it would not be very high.</p>
<hr>
<p>I want to end this piece with two notes.</p>
<p>First: I see a couple of problems with Lotman’s theory — again, I’m not a literary theorist by trade so I do not know whether he (or others) have addressed them.</p>
<ol>
<li>How does Lotman account for the way early (oral) poetry, with its strict rhyme and metrical patterns, was designed to be memorized? Isn’t the poetical form, in a very direct sense, optimizing itself for compression and therefore less entropic — as if the redundancy were to harden the message against the noise of <em>time</em> and <em>amnesia</em>? Or do these qualities themselves encode and exfiltrate information, like the <a href="https://en.wikipedia.org/wiki/Sanskrit_prosody#Chandas_and_mathematics">stress-patterns in Sanskrit verse</a>, or the knots knitted by the women in <em>A Tale of Two Cities</em>? Lotman spends some time at the beginning of Chapter 6 discussing how poetry removed itself from ordinary speech, and then prose removed itself from poetry, and therefore prose is twice-removed from ordinary speech. But that does not quite answer my question.</li>
<li>How does Lotman account for the fuzziness of texts — for example, the many editions of <em>Hamlet</em> that editors <em>to this day</em> Frankenstein together to try to formulate a coherent whole? How does this theory stand in the absence of an original source text?</li>
</ol>
<p>Second: Lotman means all this in a serious technical sense. As far as I can tell, he is not just borrowing scientific buzzwords to give legitimacy to an otherwise fuzzy theory. For example, he cites the quantitative work of Shannon and Kolmogorov directly, even though it was (even for the time) primitive. The preface to his book suggests rhetorically that the only way to test these theories would be to compute empirical metrics on an unthinkably large set of poems. Later Lotman suggests a kind of semantic analysis of poetry, but says</p>
<blockquote>
<p>The operations recounted above give only a general and deliberately rough semantic skeleton, since <strong>a description of all connections arising in the text and of all extra-textual relations which could be ascertained would be an unrealistic task in terms of sheer volume.</strong></p>
</blockquote>
<p>But today, we can do that — take exactly those measurements that Lotman (and Kolmogorov and Shannon) could only have dreamed of! It is as if Lotman predicted the relativistic warping of the stars, and we had to wait decades for the eclipse to make the confirming observations. Practical technology has finally caught up to theory.</p>
<p>And so I think there is potential for unprecedented scholarship here. This is a question where deep learning enthusiasts and literary theorists and semioticians all have something valuable to say. I wonder what the conversation will be like? It is comforting to note that our hero Kolmogorov appeals as early as 1964,</p>
<blockquote>
<p>But real progress in this direction demands that cyberneticians [today’s deep learning researchers?] take a greater interest in the humanities and learn more about them.</p>
</blockquote>
]]></description>
            <link>https://hardmath123.github.io/lotman.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/lotman.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 24 Jul 2020 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[A Differentiable Anti-Anti-Aliasing Adventure]]></title>
            <description><![CDATA[<p>Using PyTorch to produce magical moiré patterns</p>


<p>Another magic trick. Watch these two grids overlap to reveal a secret image!
Can you spot the phantom horse?</p>
<style type="text/css">
.slideme {
  animation: reveal;
  animation-duration: 2s;
  animation-iteration-count: infinite;
  animation-direction: alternate;
  -webkit-animation-delay: 5ms; /*
https://stackoverflow.com/questions/30735571/css-rotate-animation-doesnt-start-properly-in-safari
*/
  margin-bottom: 0;

  width: 50%;
}

.slideup {
  animation: revealb;
  animation-duration: 2s;
  animation-iteration-count: infinite;
  animation-direction: alternate;
  -webkit-animation-delay: 5ms; /*
https://stackoverflow.com/questions/30735571/css-rotate-animation-doesnt-start-properly-in-safari
*/
  margin-top: 0;
  width: 50%;
}

@keyframes reveal {
  from {
    transform: translateY(0);
  }
  50% {
    transform: translateY(0);
  }
  to {
    transform: translateY(100%);
  }
}

@keyframes revealb {
  from {
    transform: translateY(0);
  }
  50% {
    transform: translateY(0);
  }
  to {
    transform: translateY(-100%);
  }
}

</style>

<p><img class="slideme" src="static/moire/warp.png"></img><br/>
<img class="slideup" src="static/moire/screen.png"></img></p>
<p>If my janky CSS animation didn’t work for you, here is another way to see it:
simply shrink the top grid slowly until the horse “pops” out. It’s hard to show
this effect consistently across displays/browsers because of varying pixel
spacings and various browsers’ aggressive anti-aliasing tactics, but here is a
screen recording I captured.</p>
<video controls="controls" name="Moire" style="width: 100%;" src="static/moire/moire.mov"></video>

<p>Magic trick? Not quite. Graphics folks will immediately recognize this effect
as a curious instance of the dreaded <em>moiré pattern,</em> caused by the
interference between two almost-but-not-quite-the-same grids. Researchers work
really hard to escape such “glitchy” artifacts — they are often a result of
sloppily sampling a texture in a grid, leading to jagged edges or moiré streaks
(image below from Wikipedia). </p>
<p><img src="static/moire/bricks.jpg" alt="bricks"></p>
<p>In fact, a <a href="http://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction.html">lot of
effort</a> has
gone into finding “good” sampling patterns that avoid moirés; for example, in
an odd crossover of sampling theory and anatomy, if you look at the
distribution of <a href="https://www.nap.edu/read/1570/chapter/13">photoreceptors in a monkey’s
retina</a> they are neither in a grid
<em>nor</em> uniformly random, but rather arranged in a <em>low-discrepancy</em> pattern to
avoid exactly this kind of artifact.</p>
<p>But for all the hard work gone into avoiding these patterns, you kind of have
to admit that they are beautiful and mysterious. In fact “moiré” gets its name
from a desirable form of silk weave that exhibits this phenomenon owing to
interference from the weave (photo from Wikipedia again). Similarly, <a href="https://en.wikipedia.org/wiki/Guardian_(sculpture">this
sculpture</a> (and a lot more
art) exploits moiré patterns for their hypnotic beauty.</p>
<p><img src="static/moire/silk.jpg" alt="moire silk"></p>
<p>There are lots of real-world ways you can play with this effect, and indeed
soon enough you can’t stop seeing it everywhere you go (this is the joy and
frustration of being interested in visual phenomena):</p>
<ol>
<li>Point a camera at a checkered shirt; the camera’s pixel grid interferes with
the fabric pattern (I’ve been told not to wear checkered shirts when giving
talks at certain venues where everything is recorded).</li>
<li>Align two kitchen strainers and gently bend them to see the patterns change.</li>
<li>Watch a screen door’s shadow interfere with the screen itself, poke the
screen a bit to see the patterns change.</li>
<li>Watch the wire fences on either side of a pedestrian walkway above a road
interfere with each other.</li>
<li>Look at an office chair from behind, watch the mesh of the back interfere
with the mesh of the seat. Depending on their relative curvature you can get
some beautiful effects.</li>
</ol>
<p>These patterns are seemingly random, like marbled paper. But can we control
this randomness? Satisfyingly, an on-demand moiré such as the effect above is
extremely simple to generate using differentiable programming in PyTorch.</p>
<p>What I am optimizing is a <em>small distortion</em> to the image of the grid such that
when the grid and its distortion are superimposed, the resulting image has low
L2 difference from a given target, such as the horse image. What exactly is
this “distortion”? Imagine the image printed on play-dough and deforming it by
stretching and squeezing parts. The result is a warping of the image. To
express this warping, each pixel is assigned a “motion vector” which tells it
where to go. To make sure the motion vectors are smooth and continuous between
adjacent pixels, I actually learn a heavily downsampled set of motion vectors
(even a 12-by-12 array gets good results) and then upsample it to full
resolution with bicubic interpolation.</p>
<p>Here is the wonderful thing: all of these operations are built in to PyTorch
and differentiable, apparently to support <a href="https://arxiv.org/pdf/1506.02025.pdf">Spatial Transformer
Networks</a>. So, a straightforward 30-line
implementation of the above algorithm is all it takes to start “learning” a
moiré grid perturbation. <a href="static/moire/Moire.ipynb">Here</a> is a Jupyter notebook
with all you need to get started.</p>
<p>How much computation does it take to get good results? See this animation: it
converges extremely rapidly, in just 800 iterations or so of plain gradient
descent (a couple of minutes on my laptop).</p>
<p><img src="static/moire/morph.gif" alt="learning"></p>
<hr>
<p>I’m of course not the first person to try this trick; I found references in
2001 and 2013. I think my results look significantly better than those
     presented in the paper <a href="static/moire/2001.pdf"><em>Variational Approach to Moire Pattern Synthesis</em>
(Lebanon and Bruckstein 2001)</a>, even though my technique
is much simpler than their gradient-based technique. On the other hand, I think
the more mathematically-motivated results presented in <a href="static/moire/2013.pdf"><em>Target-Driven Moiré
Pattern Synthesis by Phase Modulation</em> (Tsai &amp; Chuang
2013)</a> are much more impressive, even though they have a
slightly different setup than mine. They also had the resources to print out
their patterns on transparent film and overlay them physically, as well as to
mess with a camera whose Bayer pattern was known! And they suggest using this
trick for cryptography. Very neat.</p>
<p>To understand what they do we need to understand the mechanics of moiré
(instead of leaving it all to SGD!). I’ve always been told that when two
high-frequency signals such as grids are superimposed, they can alias to form
interesting low-frequeny content (famously there is <a href="https://xkcd.com/1814/">this
comic</a>). But I’ve never been given more detail than
that high-level overview, so I looked it up myself. It’s actually quite
wonderful and principled. Here is my understanding as informed by <a href="static/moire/1994.pdf"><em>A
Generalized Fourier-based Method for the Analysis of 2D Moiré Envelope-forms in
Screen Superpositions</em> (Amidror 1994)</a>, specifically by
Figure 4 reproduced below.</p>
<p><img src="static/moire/lattice.png" alt="fig 4"></p>
<p>First, we need to come up with a useful characterization of grids. Recall that
you can take a 2D signal (i.e. image) and perform a 2D Fourier transform, which
is <em>also</em> an image. Each point in frequency-space represents a set of stripes
(i.e. a sine wave) rotated to match the point’s angle with respect to the
origin (see (a) and (b) in the figure for examples of what I mean). The
frequency of the stripes is given by the distance from the point to the origin,
and the amplitude is given by the point’s brightness (complex-valued
brightnesses let you set phase). To get “sharp” stripes instead of a “smooth”
sine wave gradient, you have to add “harmonics” at regular multiples of that
point (recall the Fourier transform of a square wave if that helps). This is
(d) and (e) in the figure from the paper.</p>
<p>Next, we note that superimposing two sets of stripes is like pointwise
multiplication (if you treat zeros as “occlusion”, then multiplying by zero
always sends the result to zero as desired). Because pointwise multiplication
is convolution in frequency space, the superposition looks like a <em>lattice</em>
spanned by the harmonics in frequency space. This lattice is depicted in (f) in
the figure.</p>
<p>The key to the illusion is that this lattice might include points much closer
to the origin than either of the harmonics that span it! Because the human
visual perceptual system acts as a low-pass filter, the result is that you see
a low-frequency “phantom” in the superposition. This is visible in (c) in the
figure.</p>
<p>Tsai and Chuang use this exact insight to reverse the moiré effect
analytically, with excellent results.</p>
<hr>
<p><strong>Open questions:</strong> I think there’s a lot more to do here. In my mind, the holy
grail is a T-shirt you can wear which, <em>only when photographed</em> reveals a
secret message. Another neat application would be a wire mesh rolled into a
cylinder such that as you rotated it, the moiré patterns animated into
something cool (perhaps a galloping horse, as a nod to Muybridge). There is
some precedent for this in “barrier-grid” animation (see, e.g. <a href="https://thevinylfactory.com/features/freaky-formats-moire-effect/">these vinyl
record
covers</a>),
but the benefit of a moiré-based approach would be that the barriers need not
be as thick, allowing for finer detail.</p>
<p>Another direction to go in is audio moiré; that is, could we make two sounds,
both above human aural range (20 kHz) such that when played together you hear a
perfectly clear, audible voice?</p>
<p>As you might have guessed, I think naïve gradient descent should be able to get
us a long way there…</p>
]]></description>
            <link>https://hardmath123.github.io/moire.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/moire.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Thu, 16 Jul 2020 07:00:00 GMT</pubDate>
        </item>
    </channel>
</rss>