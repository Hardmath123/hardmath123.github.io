<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Comfortably Numbered]]></title>
        <description><![CDATA[My blog.]]></description>
        <link>https://hardmath123.github.io</link>
        <image>
            <url>https://hardmath123.github.io/static/avatar.png</url>
            <title>Comfortably Numbered</title>
            <link>https://hardmath123.github.io</link>
        </image>
        <generator>RSS for Node</generator>
        <lastBuildDate>Mon, 26 Sep 2022 16:10:16 GMT</lastBuildDate>
        <atom:link href="https://hardmath123.github.io/feed.xml" rel="self" type="application/rss+xml"/>
        <author><![CDATA[Hardmath123]]></author>
        <language><![CDATA[en]]></language>
        <item>
            <title><![CDATA[Light from shadow, seeing from seeing]]></title>
            <description><![CDATA[<p>Thoughts on a blue and yellow photograph I took this summer</p>


<p>This summer I was briefly in Vancouver, and after dinner on my last day I found
myself walking several blocks back to my hotel. The night was dark but the
sidewalk was brightly lit and it was a lovely journey. Along the way, I took
this picture:</p>
<p><img src="static/vancouver-streetlight.jpg" alt="A streetlight casts two shadows, blue and
yellow"></p>
<p>What struck me is how the streetlight casts two shadows, blue and yellow, and
moreover those shadows appear opposite the yellow and blue lamps, respectively.
What could possibly be going on?</p>
<p>Here is an explanation: the two lamps together create a kind of grayish-white
light that bathes the sidewalk. Where the yellow lamp is occluded, the blue
light is dominant, so the shadow is blue. Similarly, where the blue lamp is
occluded, the yellow light is dominant, so the shadow is yellow.</p>
<p>Looking at this scene I’m reminded of painter Wayne Thiebaud’s rich, saturated
shadows. You could say the perceptual effect here demonstrates that “white
light” is the sum of all wavelengths, a fact we learn in grade school (I think
there is an exhibit at the SF Exploratorium with a similar concept). But to me,
this also demonstrates the range of what we are willing to call “white.” If one
of the lamps were to burn out, our eyes would adjust to the blue or yellow
almost immediately, and we would still see the sidewalk as gray — we
experience a truly remarkable “color constancy” across lighting conditions. In
this way, when he paints a shadow as a saturated, non-gray color, Thiebaud sees
beyond his own seeing.</p>
]]></description>
            <link>https://hardmath123.github.io/thiebaud.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/thiebaud.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 26 Sep 2022 04:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[PeLU: Porcelain-Emulated Linear Unit]]></title>
            <description><![CDATA[<p>A low-power deep learning inference mechanism inspired by flush toilets</p>


<p>The other day my toilet broke and I involuntarily learned a lot about how flushing works. My friend suggested an analogy for me: flushing toilets is like a neuron’s activation: once a critical threshold is met, there’s an “all-at-once” response.</p>
<p>That got me thinking, could we implement deep neural networks in plumbing? It turns out, the answer is yes! A very simplified model of a flush toilet’s nonlinear behavior is as follows: it’s a bucket, into which water can be poured, and there is a hole at height $h \geq 0$. If you pour in volume $v$ of water into the bucket, the output that flows out of the hole is $\text{ReLU}(v - h)$.</p>
<p>The second component we need to build a neural network is a linear map. We can do this by attaching a branching pipe to the hole. This component will have $k$ branches with cross-sectional areas $A_1, A_2, \dots, A_k &gt; 0$. By conservation of mass and a simple pressure argument, the amount of water that pours out of branch $i$ is $A_i / \Sigma_j A_j$.</p>
<p>Together, these components allow us to compute a function from $\mathbb{R}\rightarrow \mathbb{R}^k$, which looks something like $\text{PeLU}(v, \vec{A}, h) = \text{ReLU}(v - h)\cdot \vec{A} / \Sigma_j A_j$. Here, “PeLU” stands for “Porcelain-Emulated Linear Unit.” It is clear how to vectorize this expression over $v$, which effectively creates a new kind of neural network “layer” with trainable parameters $\vec{A}$ and $h$ for each input dimension. To enforce the positivity constraint on $h$ and $A_i$, we will actually work with the following key equation: $\text{PeLU}(v, \vec{A}, h) = \boxed{\text{ReLU}(v - h^2) \cdot \text{softmax}(\vec{A})}$.</p>
<p>All that is left to do at this point is to implement this in PyTorch and train it.</p>
<pre><code>import torch

class PeLU(torch.nn.Module):
    def __init__(self, in_feat, out_feat):
        super().__init__()
        self.heights = torch.nn.Parameter(
            torch.randn(in_feat))
        self.weights = torch.nn.Parameter(
            torch.randn(in_feat, out_feat)
        )

    def forward(self, X):
        X = torch.nn.functional.relu(X - self.heights ** 2)
        X = X.matmul(self.weights.softmax(dim=1))
        return X
</code></pre><p>Here, I built a PeLU layer that can be slipped into any PyTorch model, mapping <code>in_feat</code> inputs to <code>out_feat</code> outputs. Next, let’s stack some PeLU layers together and train the result on the classic “Iris” dataset, which has 4 features and assigns one of 3 labels. We will create a “hidden layer” of size 3, just to keep things interesting.</p>
<pre><code>from iris import feat, labl

m = torch.nn.Sequential(
    PeLU(4, 3),
    PeLU(3, 3)
)
o = torch.optim.Adam(m.parameters(), lr=0.01)
lf = torch.nn.CrossEntropyLoss()

for i in range(10_000):
    o.zero_grad()
    pred = m(feat * 10)
    loss = lf(pred, labl)
    loss.backward()
    o.step()

print(&#39;Loss:&#39;, loss)
print(&#39;Error:&#39;, 1. - ((torch.argmax(pred, dim=1) == labl) * 1.).mean())
</code></pre><p>This trains very quickly, in seconds, and gives an error of 2%. Of course, we haven’t split the dataset into a train/test set, so may be be overfitting.</p>
<p>By the way, you may have noticed that I multiplied <code>feat</code> by 10 before passing it to the model. Because of the conservation of mass, the total amount of water in the system is constant. But each bucket “loses” some water that accumulates below the hole. To make sure there’s enough water to go around, I boosted the total amount.</p>
<p>But that’s all that needs to be done! Once we export the parameters and write a small visualizer…</p>
<p><img src="static/iris-pelu.gif" alt="GIF of PeLU network inferring an Iris example"></p>
<p>Isn’t that a delight to watch? I may even fabricate one to have as a desk toy — it shouldn’t be hard to make a 3D-printable version of this. If we wanted to minimize the number of pipes, we could add an L1 regularization term that enforces sparsity in the $\vec{A}$ terms.</p>
<p>Some final thoughts: this system has a couple of interesting properties. First, there’s a kind of “quasi-superposition” that it allows for: if you pour in more water on top to incrementally refine your input, the output will automatically update. Second, the “conservation of mass” guarantees that the total water output will never exceed the total water input. Finally, it’s of course entirely passive, powered only by gravity.</p>
<p>This has me wondering if we can build extremely low-power neural network inference devices by optimizing analog systems using gradient descent in this way (a labmate pointed me to <a href="https://arstechnica.com/science/2018/07/neural-network-implemented-with-light-instead-of-electrons/">this</a>, for example).</p>
<p>Below is a little widget you can use to enjoy playing with PeLU networks. All inputs must be between 0 and 10. :)</p>
<p>Sepal length (cm): <input id="sl" type="text" value="6.2"></input><br/>
Sepal width (cm):  <input id="sw" type="text" value="3.4"></input><br/>
Petal length (cm): <input id="pl" type="text" value="5.4"></input><br/>
Petal width (cm):  <input id="pw" type="text" value="2.3"></input><br/></p>
<p><input type="button" id="go" value="Predict!"></input><br/></p>
<canvas id="world" width="500" height="500"></canvas>

<script>
var m = [{'h': [2.0516297817230225, 2.18482890761347e-30, 4.2594499588012695, 2.937817096710205], 'w': [[0.08244021981954575, 0.35453349351882935, 0.5630263090133667], [0.783989667892456, 0.0022806653287261724, 0.2137296199798584], [0.0007646767771802843, 0.8042643070220947, 0.19497093558311462], [0.0004950931761413813, 0.9982838034629822, 0.0012211321154609323]]}, {'h': [2.2704419876575757e-26, 33.44374465942383, 12.223723411560059], 'w': [[0.9706816077232361, 0.021526599302887917, 0.007791891228407621], [0.0013629612512886524, 0.022002533078193665, 0.9766345620155334], [0.018276285380125046, 0.9780184626579285, 0.0037053129635751247]]}];

function Chamber(x, y, f, h, w, p) {
  this.x = x;
  this.y = y;
  this.f = f;

  this.h = h;
  this.w = w;
  this.p = p;

  this.l = null;
}

Chamber.prototype.flow = function(dl) {
  if (this.f <= this.h) return;

  dl = Math.max(0.1 * this.f - this.h, dl);
  for (var j = 0; j < this.p.length; j++) {
    this.p[j].f += dl * this.w[j];
  }
  this.f -= dl;
};

Chamber.prototype.draw = function(ctx) {
  var height = 100;
  ctx.save();
  ctx.translate(this.x, this.y);

  if (this.l !== null) {
    ctx.save();
    ctx.font = '8pt Helvetica';
    ctx.translate(0, height);
    ctx.rotate(-Math.PI / 2);
    ctx.fillText(this.l, 0, 5);
    ctx.restore();
  }

  ctx.fillStyle = '#acf';
  ctx.fillRect(10, height - this.f, 30, this.f);

  ctx.beginPath();
  ctx.arc(0, 10, 10, -Math.PI / 2, 0);
  ctx.lineTo(10, height);
  ctx.lineTo(40, height);
  ctx.lineTo(40, 10);
  ctx.arc(50, 10, 10, Math.PI, -Math.PI / 2);
  ctx.stroke();

  if (this.h !== null) {
    ctx.beginPath();
    ctx.arc(35, height - this.h, 5, 0, Math.PI * 2, true);
    ctx.stroke();
  }
  ctx.restore();

  if (this.h !== null) {
    for (var j = 0; j < this.p.length; j++) {
      ctx.save();
      ctx.beginPath();
      ctx.moveTo(this.x + 35, this.y + height - this.h);
      ctx.bezierCurveTo(
        this.x + 35, this.y + height - this.h + 20,
        this.p[j].x + 25, this.p[j].y - 20,
        this.p[j].x + 25, this.p[j].y
      );
      ctx.strokeStyle = 'black';
      ctx.lineWidth = this.w[j] * 10;
      ctx.stroke();
      ctx.lineWidth = ctx.lineWidth * 0.8;
      ctx.strokeStyle = this.f > this.h ? '#acf' : 'white';
      ctx.stroke();

      if (this.f > this.h) {
        ctx.strokeStyle = '#acf';
        ctx.beginPath();
        ctx.moveTo(this.p[j].x + 25, this.p[j].y);
        ctx.lineTo(this.p[j].x + 25, this.p[j].y + 100);
        ctx.stroke();
      }
      ctx.restore();
    }
  }

  if (this.h === null) {
    var best = true;
    for (var j = 0; j < cs[cs.length - 1].length; j++) {
      if (this.f < cs[cs.length - 1][j].f) best = false;
    }
    if (best) {
      ctx.save();
      ctx.fillStyle = 'rgba(255, 255, 0, 0.2)';
      ctx.fillRect(this.x - 10, this.y - 10, 50 + 20, height + 20);
      ctx.restore();
    }
  }
};

var cs = [];
for (var i = 0; i < m.length; i++) {
  cs.push([]);
  for (var j = 0; j < m[i].h.length; j++) {
    var c = new Chamber(30 + 20 * i + 80 * j, 20 + 150 * i, 0, m[i].h[j], m[i].w[j], []);
    cs[cs.length - 1].push(c);
  }
}

cs.push([]);
for (var j = 0; j < cs[cs.length - 2][0].w.length; j++) {
  var c = new Chamber(30 + 20 * i + 80 * j, 20 + 150 * i, 0, null, [], []);
  cs[cs.length - 1].push(c);
}

for (var i = 0; i < m.length; i++) {
  for (var j = 0; j < cs[i].length; j++) {
    cs[i][j].p = cs[i + 1];
  }
}

cs[0][0].l = 'sepal length (cm)';
cs[0][1].l = 'sepal width (cm)';
cs[0][2].l = 'petal length (cm)';
cs[0][3].l = 'petal width (cm)';
cs[2][0].l = 'P(iris setosa)';
cs[2][1].l = 'P(iris versicolour)';
cs[2][2].l = 'P(iris virginica)';

cs[0][0].f = 62;
cs[0][1].f = 34;
cs[0][2].f = 54;
cs[0][3].f = 23;

function frame() {
  var world = document.getElementById('world');
  world.width = world.width;
  var ctx = world.getContext('2d');
  for (var i = 0; i < cs.length; i++) {
    for (var j = 0; j < cs[i].length; j++) {
      cs[i][j].draw(ctx);
    }
  }
  for (var i = 0; i < cs.length - 1; i++) {
    for (var j = 0; j < cs[i].length; j++) {
      cs[i][j].flow(0.1);
    }
  }

  window.requestAnimationFrame(frame);
}

window.addEventListener('load', function() {
  var sl = document.getElementById('sl');
  var sw = document.getElementById('sw');
  var pl = document.getElementById('pl');
  var pw = document.getElementById('pw');
  var go = document.getElementById('go');
  go.addEventListener('click', function() {
    for (var i = 0; i < cs.length; i++) {
      for (var j = 0; j < cs[i].length; j++) {
        cs[i][j].f = 0.;
      }
    }
    cs[0][0].f = Math.max(0., Math.min(100., (parseFloat(sl.value) || 0) * 10));
    cs[0][1].f = Math.max(0., Math.min(100., (parseFloat(sw.value) || 0) * 10));
    cs[0][2].f = Math.max(0., Math.min(100., (parseFloat(pl.value) || 0) * 10));
    cs[0][3].f = Math.max(0., Math.min(100., (parseFloat(pw.value) || 0) * 10));
  });
  frame();
});
</script>]]></description>
            <link>https://hardmath123.github.io/pelu.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/pelu.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Sat, 04 Dec 2021 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Birds and the Representation of Representation]]></title>
            <description><![CDATA[<p>What is it about birds?</p>


<h2 id="toni-morrison-nobel-lecture">Toni Morrison, Nobel Lecture</h2>
<blockquote>
<p>Speculation on what (other than its own frail body) that bird-in-the-hand might signify has always been attractive to me, but especially so now thinking, as I have been, about the work I do that has brought me to this company. <a href="https://www.nobelprize.org/prizes/literature/1993/morrison/lecture/">(full)</a></p>
</blockquote>
<h2 id="richard-siken-the-language-of-the-birds-">Richard Siken, “The Language of the Birds”</h2>
<blockquote>
<p>And just because you want to paint a bird, do actually paint a bird, it doesn’t mean you’ve accomplished anything. <a href="https://poets.org/poem/language-birds">(full)</a></p>
</blockquote>
<h2 id="cross-examination-in-_brancusi-v-united-states_">Cross-examination in <em>Brancusi v. United States</em></h2>
<blockquote>
<p><strong>Waite:</strong> What do you call this?<br><strong>Steichen:</strong> I use the same term the sculptor did, oiseau, a bird.<br><strong>Waite:</strong> What makes you call it a bird, does it look like a bird to you?<br><strong>Steichen:</strong> It does not look like a bird but I feel that it is a bird, it is 
characterized by the artist as a bird.<br><strong>Waite:</strong> Simply because he called it a bird does that make it a bird to you?<br><strong>Steichen:</strong> Yes, your honor.<br><strong>Waite:</strong> If you would see it on the street you never would think of 
calling it a bird, would you?<br>[<strong>Steichen:</strong> Silence]<br><strong>Young:</strong> If you saw it in the forest you would not take a shot at it?<br><strong>Steichen:</strong> No, your honor. <a href="https://www.legalaffairs.org/issues/September-October-2002/story_giry_sepoct2002.msp">(more)</a></p>
</blockquote>
<h2 id="donna-tartt-_the-goldfinch_">Donna Tartt, <em>The Goldfinch</em></h2>
<blockquote>
<p>But who knows what Fabritius intended? There’s not enough of his work left to even make a guess. The bird looks out at us. It’s not idealized or humanized. It’s very much a bird.</p>
</blockquote>
<h2 id="adam-savage-my-obsession-with-objects-and-the-stories-they-tell-">Adam Savage, “My obsession with objects and the stories they tell”</h2>
<blockquote>
<p>And then there is this fourth level, which is a whole new object in the world: the prop made for the movie, the representative of the thing, becomes, in its own right, a whole other thing, a whole new object of desire. . . . There are several people who own originals, and I have been attempting to contact them and reach them, hoping that they will let me spend a few minutes in the presence of one of the real birds, maybe to take a picture, or even to pull out the hand-held laser scanner that I happen to own that fits inside a cereal box, and could maybe, without even touching their bird, I swear, get a perfect 3D scan. And I’m even willing to sign pages saying that I’ll never let anyone else have it, except for me in my office, I promise. I’ll give them one if they want it. And then, maybe, then I’ll achieve the end of this exercise. But really, if we’re all going to be honest with ourselves, I have to admit that achieving the end of the exercise was never the point of the exercise to begin with, was it? <a href="https://www.youtube.com/watch?v=29SopXQfc_s">(full)</a></p>
</blockquote>
<h2 id="michael-shewmaker-the-curlew-">Michael Shewmaker, “The Curlew”</h2>
<blockquote>
<blockquote>
<p>Plate 357 <em>(Numenius Borealis)</em> is the only instance in which the subject appears dead in the work of John James Audubon.</p>
</blockquote>
<p>He waits alone, sketching angels from the shade—<em>a kind of heavenly bird,</em> he reasons with himself—although their wings are broke, faces scarred, each fragile mouth feigning the same sad smile as the one before it. Offered triple his price to paint a likeness of the pastor’s daughter—buried for more than a week—he reluctantly agreed—times being what they are.<br>. . . . .<br>And yet he studies it—from behind the dunes—studies its several postures, grounded and in sudden flight—and not content to praise it from a distance, to sacrifice detail, unpacks his brushes and arranges them before raising his rifle and taking aim.</p>
</blockquote>
]]></description>
            <link>https://hardmath123.github.io/birds-and-the-representation-of-representation.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/birds-and-the-representation-of-representation.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Thu, 21 Oct 2021 04:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Carnival of Mathematics 198]]></title>
            <description><![CDATA[<p>The Carnival revisits an old home</p>


<p>Hello and welcome to the 198th <a href="https://aperiodical.com/carnival-of-mathematics/">Carnival of Mathematics</a>, a roving monthly roundup of mathy blog posts from around the Internet! Longtime Comfortably Numbered readers should be no stranger to the Carnival: I hosted <a href="carnival-of-mathematics-134.html">#134</a>, <a href="carnival-of-mathematics-148.html">#148</a>, and <a href="carnival-of-mathematics-159.html">#159</a> in the past.</p>
<p>As is traditional, I want to start by thinking a bit about the number 198. It is actually a very dear number to me — I even own a “198” t-shirt! Why, you ask? The number 198 is the emblem of the wonderful CS198 program at Stanford, which hires a huge team of undergraduates to help teach the introductory computer science courses every year. (As far back as 1995, the faculty running the program published a <a href="https://cormack.uwaterloo.ca/papers/p48-roberts.pdf">retrospective</a> on its impact on campus.)</p>
<p>Here is another fun fact about the number 198: suppose I start looking at all the integers ($1, 2, 3, 4, 5, \dots$), keeping only the ones that are perfect powers ($1, 4=2^2, 8=2^3, 9=3^2, 16=2^4, \dots$), and then taking their cumulative averages ($1$, $2.5=(1+4)/2$, $4.33=(1+4+8)/3$, $5.5=(1+4+8+9)/4$, $7.6=(1+4+8+9+16)/5, \dots$). You might wonder, are there any other integers in <a href="http://oeis.org/A075457">this sequence</a>? It turns out, <em>yes!</em> And I bet you can guess the first one…</p>
<hr>
<p>Now, time for some links! I’ve provided some “teaser” text from each submission to get you interested.</p>
<p>The BRSR blog asks: <a href="https://brsr.github.io/2021/05/29/curved-triangle.html">can you find a Euclidean triangle on a non-Euclidean surface?</a></p>
<blockquote>
<p>A question that came up in a math chatroom (yes, I’m the kind of nerd who spends time in math chatrooms): find a “Euclidean” triangle on a non-Euclidean surface. More exactly, find a geodesic triangle on a surface with non-constant <a href="https://en.wikipedia.org/wiki/Gaussian_curvature">Gaussian curvature</a> having the same sum of interior angles as an Euclidean triangle.</p>
</blockquote>
<p>On his blog bit-player, Brian Hayes asks: <a href="http://bit-player.org/2021/riding-the-covid-coaster">why do pandemics peak in distinct waves as the months go by?</a></p>
<blockquote>
<p>I’m puzzled by all this structural embellishment. Is it mere noise—a product of random fluctuations—or is there some driving mechanism we ought to know about, some switch or dial that’s turning the infection process on and off every few months?</p>
<p>I have a few ideas about possible explanations, but I’m not so keen on any of them that I would try to persuade you they’re correct. However, I <em>do</em> hope to persuade you there’s something here that needs explaining.</p>
</blockquote>
<p>On Twitter, Thien An asks: <a href="https://twitter.com/thienan496/status/1434522808189521930">would you rather have a bishop and a knight, or two bishops?</a></p>
<blockquote>
<p>To get this figure, it suffices to grab 100k games and only keep the positions with BB+pawns v.s. BN+pawns and see who eventually wins. That’s very simplistic since there are indeed many other important positional features, but I was actually expecting less convincing results…</p>
</blockquote>
<p>Looking at a video of a pomegranate sorting mechanism, Nisar Khan asks: <a href="https://nisarkhanatwork.medium.com/pomegranate-size-differences-in-different-boxes-ff8a627bb82">how much do the pomegranate sizes vary between sorted buckets?</a></p>
<blockquote>
<p>After watching the below video, thought of finding the approximate size differences of pomegranates sorted in different boxes…</p>
</blockquote>
<p>On his math blog, Tony asks: <a href="http://tonysmaths.blogspot.com/2021/09/did-studying-maths-help-emma-raducanu.html?m=1">did studying maths help Emma Raducanu win the US Open?</a></p>
<blockquote>
<p>I’ve discussed toy examples in public lectures on game theory, which (it seems to me) is relevant to choices players make - whether to serve to the forehand or backhand, and where to expect for your opponent to serve, for example. I very much doubt if players ever analyse in these terms, but they are intuitively doing game theory when making their tactical decisions.</p>
<p>But I think in the case of Raducanu there is a more general point…</p>
</blockquote>
<p>On Risen Crypto, Trajesh asks: <a href="https://risencrypto.github.io/QuadraticSieve/">how does the quadratic sieve factoring algorithm really work?</a></p>
<blockquote>
<p>The Quadratic Sieve is the second fastest algorithm for factoring large semiprimes. It’s the fastest for factoring ones which are lesser than 100 digits long.</p>
</blockquote>
<p>On The Universe of Discourse, Mark Dominus asks: <a href="https://blog.plover.com/2021/08/28/#combinator-s">why is the “S” combinator named “S”?</a> He also posted a <a href="https://blog.plover.com/math/dilworth.html">nice puzzle</a> that is easily solved if you happen to know Dilworth’s theorem; separately, Jim Fowler built a <a href="https://twitter.com/kisonecat/status/1435265678525620227">musical PICO-8 game</a> where you race to find the chains guaranteed by Dilworth’s theorem.</p>
<blockquote>
<p>I thought about this for a while but couldn’t make any progress. But OP had said “I know I have to construct a partially ordered set and possibly use Dilworth’s Theorem…” so I looked up Dilworth’s theorem.</p>
</blockquote>
<p>On their blog “Gödel’s Last Letter and P=NP,” Ken Regan and Dick Lipton discuss <a href="https://rjlipton.wpcomstaging.com/2021/09/30/baby-steps/">“baby steps”</a> in math, in the context of some exciting recent results.</p>
<blockquote>
<p>Reckoned against a later <a href="https://annals.math.princeton.edu/2009/170-3/p01">paper</a> by Cohn and Elkies, [Viazovska’s] improvement was 0.0000…000001. The recent post where we discussed the phrase “the proof is in the pudding” involves a number with six more zeroes than that. These are <strong>not</strong> what we mean by “baby steps.”</p>
</blockquote>
<p>And finally, something more about sequences— on her math blog, Tanya Khovanova explores the <a href="https://blog.tanyakhovanova.com/2021/09/the-top-fifty-largest-numbers-that-start-a-sequence-in-the-oeis/">top 50 largest numbers to start OEIS sequences</a>.</p>
<blockquote>
<p>My son, Alexey Radul, wrote a program that finds the largest numbers to start a sequence in the Online Encyclopedia of Integer Sequences (<a href="https://oeis.org/">OEIS</a>). To my surprise, the top ten are all numbers consisting of ones only.</p>
</blockquote>
<hr>
<p>That’s all for this month’s Carnival! What a joy to see — once more — the math blog world <em>overflowing</em> with curiosity. Hosting the Carnival of Mathematics truly is a delight.</p>
<p>Want even more math? The previous Carnival, the 197th, was hosted by <a href="https://jeremykun.com/2021/09/01/carnival-of-mathematics-197/">Jeremy at Math $\cap$ Programming</a>, and the next — the 199th, to which you can contribute <a href="https://aperiodical.com/carnival-of-mathematics/">here</a>, starting now! — will be hosted by <a href="https://doubleroot.in">Vaibhav at DoubleRoot</a>. See you there!</p>
]]></description>
            <link>https://hardmath123.github.io/carnival-of-mathematics-198.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/carnival-of-mathematics-198.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 01 Oct 2021 04:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Fortune in Flux]]></title>
            <description><![CDATA[<p>The fortune of us that are the moon’s men doth ebb and flow like the sea (Henry IV, Part I)</p>


<p>Surely <em>something</em> will happen — which means that if you integrate over all
possible outcomes, the probabilities must add up to 1. This elementary fact
remains true, even if the probabilities of the outcomes change for some reason.
Are you reminded of something? <em>Fluids</em> behave in the same way: no matter how
the particles are distributed, the total quantity of fluid is conserved, which
means that if you integrate over all of space, the masses must add up to 1.
Indeed, when we talk about <em>probability density functions</em> and <em>probability
mass functions</em>, we are borrowing fluidic vocabulary.</p>
<p>I get excited about analogies like this, where the physics of one domain
promises to unexpectedly deliver insight into a different domain. Another
example of this is how spring constants and capacitances play the same
mathematical role in the differential equations that model their respective
mechanical and electrical systems. Harry Olson’s wonderful book <a href="https://archive.org/details/DynamicalAnalogies"><em>Dynamical
Analogies</em></a> shows some more
beautifully-illustrated examples of mechanical, electrical, and acoustic
systems with identical dynamics.</p>
<p>Can we get any mileage out of the probability-fluid analogy? It turns out,
<em>yes!</em> We can apply our physical intuition about fluids to design a clever
statistical machine learning algorithm. That’s the subject of this piece. By
the way, you can undoubtedly find better (and more correct) expositions of this
math in other places; my goal here is to emphasize the interaction between
physical intuition and probabilistic applications, bridged so beautifully by
the fluidic analogy.</p>
<p>A little bit about the problem setup: sometimes, when working with data, you
find yourself in a place where you want to sample from a probability
distribution $p(x)$. This might not be so bad if you can compute $p(x)$ exactly
— for example, you can use rejection sampling or something. Often, however,
you can only compute some scaled quantity $q(x)$ such that $p(x) \propto q(x)$
with some unknown constant of proportionality. Commonly, this happens in
Bayesian inference, where you may want to sample from $\pi(x \mid y) =
\pi(x,y)/\pi(y)$ where $y$ might be data you’ve seen and $x$ might be the
parameters to your model. Here, the problem is that the normalizing constant
$\pi(y)=\int \pi(x,y)dx$ is hard to compute because that integral is
intractable in high dimensions.</p>
<p>One famous way to do this kind of sampling is by the Metropolis-Hastings
algorithm, which approximates $p(x)$ with the stable distribution of a Markov
chain. The “miracle” of the algorithm is that the transition probabilities
between two states only depend on the <em>ratios</em> of the probability densities at
those states — the unknown constant simply cancels out!</p>
<p>But now let’s think about this from a fluids perspective. Have unknown
normalizing constants popped up in physics anywhere? Yes! The partition
function $Z$ in Boltzmann is often unknown. Recall that a system at
thermodynamic equilibrium at temperature $T$ has probability $p_i \propto
e^{-U_i/k_BT}$ of being in state $i$, where $U_i$ is the potential energy of
that state and $k_B$ is a physical constant that you can find, e.g. on
Wikipedia. A curious application of this distribution is working out how thin
the atmosphere is at altitude $h$ by reasoning about $U_h$ being the
gravitational potential energy at altitude $h$: because $U$ is linear in $h$,
we can instantly conclude that atmospheric density decreases exponentially in
altitude.</p>
<p>But returning to machine learning, here is the insight that the fluidic analogy
makes possible: if we set $U(x) = -\log q(x)$ then a particle simulated at some
“temperature” $T=1/k_B$ in potential $U$ would eventually have probability
$e^{-U(x)}=p(x)$ of being at $x$. So, we can use what we know about the
dynamics of particles that are a part of a fluid at thermal equilibrium, to
sample from probability distributions that may have nothing at all to do with
fluids.</p>
<p>What exactly does it mean to “simulate a particle” of this probability fluid?
You can take those words pretty literally, as it turns out. There are two
sources of motion at each timestep: $U$ and $T$. Given potential $U(x)$, we can
derive the force $-\nabla U$ that acts on our particle, and then apply Newton’s
Second Law to infer the velocity. We’ll assume that velocity is roughly
independent between timesteps because the particle is repeatedly being bumped
around by other particles and having its velocity reset — so, no need to
track acceleration. By the way, notice that $\nabla \log q(x)$ doesn’t depend
on the unknown normalization constant, because the logarithm turns it into an
additive constant, which disappears under the derivative.</p>
<p>As for the effect of $T$, we can model the thermal motion of the particle as
Brownian motion: that is, at each time, the particle gets bumped to some random
neighboring position drawn independently from a Gaussian that is somehow scaled
with respect to “temperature” $T$. (How do we translate $T=1/k_B$ to this
un-physical world where “joules” and “kelvins” make no sense? Stay tuned…)</p>
<p>This fluidic analogy leads us to what people call the “Unadjusted Langevin
Algorithm.” The algorithm is quite simple to state and implement:</p>
<ol>
<li>Initialize some $x_0$.</li>
<li>Using some small step size $\Delta t$ integrate $\Delta x = \Delta t \cdot
\nabla\log q(x_t) + \sqrt{2\Delta t} \cdot \xi_t$ where you sample $\xi_t
\sim N(0,1)$; the Greek letter $\xi$ is chosen for maximum squiggliness, in
order to act as a visual metaphor for its function. Importantly, this step only
requires you to evaluate the unnormalized density $q$!</li>
<li>That’s it — under some (not <em>too</em> mild but still reasonable) assumptions,
snapshots of the trajectory ${x_t}$ are distributed accoding to $p(x)$!</li>
</ol>
<p>Are you convinced? Here’s a sanity check: without the stochastic component
$\sqrt{2\Delta t}\cdot\xi_t$ at each step, $x_t$ deterministically walks
towards the maximum likelihood estimator — that is, without the stochastic
component, we’d have invented gradient descent! The stochasticity, however,
makes $x_t$ a random variable instead.</p>
<hr>
<p>The rest of this piece is dedicated to answering the question: where does that
stochastic coefficient $\sqrt{2\Delta t}$ come from? You might have already
worked out that it’s the magnitude of the thermal motion, which depends on $T$.
If that coefficient were too small, then, as we just discussed above, we simply
have gradient descent: our particle is analogous to a marble rolling down a
(syrupy — overdamped!) hill. If the coefficient were too large, then thermal
motion would dominate and the “signal” from the potential $U$ would stop
mattering. So, clearly, the threshold $\sqrt{2\Delta t}$ is “special” in some
way.</p>
<p>First let me convince you that we want this coefficient to indeed be
proportional to $\sqrt{\Delta t}$, even though it looks dimensionally wrong
when placed in a sum right next to the $\Delta t\cdot\nabla \log q$ term.
What’s going on? It turns out that this “lots of little Gaussian bumps” motion
doesn’t have an intuitive integral calculus. In particular, for Gaussians, it’s
the <em>variance</em> that adds, not the standard deviation. So in time $t$ the
variance of the total distance traveled would be $\Sigma\sqrt{\Delta t}^2 =
\Sigma \Delta t = t$, and therefore the standard deviation would be $\sqrt{t}$,
which is exactly what we want for the algorithm to be invariant to how small
$\Delta t$ is. (For more on this, you could read about “Wiener processes.”)</p>
<p>Now for the “2.” To derive this value, let’s return to our fluidic analogy.
Suppose we wanted to compute the change in probability density at a point $x$
over time, that is, ${\partial p(x,t)}/{\partial t}$, with the understanding
that we will set this time derivative to zero later to reason about the
system’s behavior at equilibrium. By following the fluidic analogy, we can
hallucinate a “probability flow” $\vec{J}(x, t)$ and assert, by Gauss’ Law,
that ${\partial p}/{\partial t} = -\nabla \cdot \vec{J}$.</p>
<p>To measure $\vec{J}(x, t)$, we should think about what’s flowing; that is,
about “particles,” whose dynamics is given by the Langevin update rule for
$\Delta x_t$ I wrote out above. The deterministic part is easy to deal with:
from a tiny region of volume $\Delta V$, a mass $p\Delta V$ of particles leaves
with velocity $\nabla\log q$, so the flux is $p\nabla \log q$.</p>
<p>For the stochastic part, notice that the dynamics of the thermal motion are
spatially invariant, i.e. independent of $x$. So, whatever amount diffuses out
of $\Delta V$ is proportional to $p$, and whatever amount diffuses in is
proportional to the density in the neighborhood of $\Delta V$: this means the
net flow is proportional to $-\nabla p$. When we talk about fluids, this
observation is also known as Fick’s Law, and the constant of proportionality is
called the diffusion coefficient $D$.</p>
<p>How can we compute $D$? Looking at only the stochastic part, we have $\partial
p/\partial t = -D\nabla^2 p$, which we recognize as the heat equation. That is,
for this brief aside, we’re going to think of $p$ representing — not
probability over Bayesian model parameters <em>or</em> fluid density in a potential
well, but — heat in a block of metal — a bonus analogy!</p>
<p>Thankfully, the solution to the heat equation turns out to be well-known: it is
a time-varying Gaussian. And it better be! After all, the stochastic diffusion
is just a sum of lots of independent little Gaussian-sampled steps of variance
$\Delta(\sigma^2) = {2\Delta t}$, which together form a big Gaussian of
variance $\sigma^2 = 2t$, as discussed above. But let’s say we didn’t know the
coefficient was 2. Let’s set it to $a$ instead. Now, if we know that $p(x,t) =
e^{-x^2/2(a t)}/\sqrt{2\pi(at)}$, then we can literally take the partial
derivatives on both sides of the heat equation and expand to solve for $D$. A
miracle occurs, and many things cancel, leaving us with simply $-D=a/2$. It’s
not worth writing out the algebra here: I encourage you to try it yourself,
since it is that rare combination of straightforward and satisfying. But
furthermore, it gives some mechanical (if not physical) insight into the “2”
that appears in the denominator of the result: algebraically, the $-1/2$ power
of $at$ in the normalization constant $1/\sqrt{2\pi(at)}$ kicks out some
factors of two that are <em>not</em> balanced by the $-1$ power of $t$ in the
exponential.</p>
<p>In any case, though, we conclude from the above that $\Delta p = -(a/2)\Delta t
\nabla^2 p$, which means the stochastic flux is simply $\nabla p$. Perhaps it’s
all clear to you by now, but let’s take this over the finish line. Putting
these two components together, $U$ and $T$, we have $\partial p/\partial t =
\nabla\cdot (p\nabla\log q-(a/2)\nabla p)$. This nice identity is known as the
Fokker-Planck equation (the devoted reader might note that this is the <em>second</em>
reference to Fokker on this blog).</p>
<p>Now, finally, as promised many paragraphs ago, we will now reason about this
equation at equilibrium. At equilibrium the distribution is static in time, so
we have $0 = \nabla \cdot (p\nabla \log q -(a/2) \nabla p)$. Assuming suitable
boundary conditions, we can assume the argument of the divergence is also zero,
so $\nabla \log q = (a/2)\nabla p / p$. Integrating in all directions, we have
$\log q + c = (a/2)\log p$, or rather $p \propto q^{2/a}$. This makes sense:
just as expected, for large $a$ the effect of $q$ is wiped out, and for small
$a$ all but the maximum value get squashed to zero. Crucially, however, if we
want $p \propto q$, then we must set $a=2$. Aha! So <em>that’s</em> why the thermal
motion needs to be scaled by $\sqrt{2\Delta t}$. (By the way, that constant of
integration above, $c$, oddly enough encodes that pesky normalization constant
that started off this whole discussion…)</p>
<hr>
<p>Okay. I’m glad I got that out of my system, it’s been rattling around for
weeks, now! How wonderful it is, that we can start with the simplest axiom of
probability — “<em>something</em> must happen” — and, reasoning only with physical
intuition, reach a sampling algorithm that solves a real problem.</p>
]]></description>
            <link>https://hardmath123.github.io/langevin.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/langevin.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Wed, 07 Jul 2021 04:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Mr. Bentley's Blizzard-Blossoms]]></title>
            <description><![CDATA[<p>Variations on a theme; or: synthesizing snowf*akes; or: unsupervised learning at Zoom University</p>


<style>
input[type="range"] {
  display: block;
  width: 100%;
}

#imgbox {
  display: inline-block;
  position: relative;
}

#world {
  margin: 0;
  padding: 0;
  max-width: 100%;
}
</style>

<script src="/static/blizzard-blossoms/onnx.min.js"></script>
<script>

window.addEventListener('load', function() {
  var LATENT = 10;
  var fieldset = document.getElementById('sliders');

  sliders = [];
  for (var i = 0; i < LATENT; i++) {
    var slider = document.createElement('input');
    slider.type = 'range';
    slider.min = -4;
    slider.max = +4;
    slider.step = 0.5;
    fieldset.appendChild(slider);
    sliders.push(slider);
  }

  function randomize(suppress_flurry) {
    for (var i = 0; i < LATENT; i++) sliders[i].value = Math.random() * 6 - 3;
    sliders[9].value = +4;
    sliders[5].value = -4;
    update();
    if (!suppress_flurry) flurry();
  }

  document.getElementById('randomize').addEventListener(
    'click', function() {randomize(false)}
  );

  // create a session
  const myOnnxSession = new onnx.InferenceSession();

  function update() {

    // generate model input
    var values = [];
    for (var i = 0; i < LATENT; i++) {
      values.push(sliders[i].value);
    }
    const inferenceInputs = [
      new Tensor(new Float32Array(values), "float32", [1, LATENT])
    ];

    // execute the model
    myOnnxSession.run(inferenceInputs).then((output) => {

      // consume the output
      const outputTensor = output.values().next().value;

      var can = document.createElement('canvas');
      can.height = 32;
      can.width = 32;
      var ctx = can.getContext('2d');
      ctx.scale(can.width / outputTensor.dims[2], can.height / outputTensor.dims[3]);
      for (var x = 0; x < outputTensor.dims[2]; x++) {
        for (var y = 0; y < outputTensor.dims[3]; y++) {
          var value = outputTensor.get(0, 0, x, y);
          ctx.fillStyle = `rgba(100, 100, 255, ${value})`;
          ctx.fillRect(x, y, 1, 1);
        }
      }

      var img = document.getElementById('world');
      img.width = can.width;
      img.height = can.height;
      img.src = can.toDataURL('image/png');
    });

  }

  // load the ONNX model file
  myOnnxSession.loadModel("/static/blizzard-blossoms/model.onnx").then(function() {
    for (var i = 0; i < LATENT; i++) {
      sliders[i].addEventListener('input', update, false);
      sliders[i].addEventListener('change', function() {update(); flurry();}, false);
    }
    document.getElementById('loading').remove();
    randomize(true);
  });


  function flurry() {
    var flake = document.createElement('img');
    var img = document.getElementById('world');
    flake.height = img.height;
    flake.width = img.width;
    flake.src = img.src;
    flake.style.position = 'absolute';
    flake.style.top = img.offsetTop;
    flake.style.left = img.offsetLeft;
    document.getElementById('imgbox').appendChild(flake);

    var dx = 0;
    var xv = Math.random() - 0.5;
    var dy = 0;

    var dtheta = 0;
    var dphi = 0;

    function step_animation() {
      dx += xv;
      xv = xv * 0.99;
      dy += 1;

      dtheta += 1;
      dphi += 1;

      flake.style.transform = `translate(${dx}px, ${dy}px) rotateX(${dtheta}deg) rotateY(${dphi}deg)`;

      if (dy < 1000)
        window.requestAnimationFrame(step_animation);
      else
        flake.remove();
    }

    step_animation();
  }

});
</script>

<fieldset id="sliders">
<span id="loading">(Please be patient as we wait for the snowflake factory to load&hellip;)</span>
<center><span id="imgbox"><img id="world" height=32 width=32></img></span></center><br/>
<center><button id="randomize">Roll the dice</button></center><br/>
</fieldset>









<p>Snow and snowflakes have been on my mind lately. This is not entirely because
of the storm in Texas — rather, I have been thinking since Christmas about
the snowflake’s <em>shape:</em> those small, delicate, spiky circles we all know so
well (err, perhaps there is something of current events here after all…).
This is a blog post about snowflake shapes.</p>
<p>What shape <em>is</em> a snowflake? In principle, we all know what snowflakes look
like. We can all imagine the six-fold symmetry, and then we can imagine the
embellishments.  And yet— not so fast! As a little child I (and I do not
think I am alone in this) wanted to see the shape of a snowflake for myself.
Of course, I never quite could — at least, to the point where I could stuff
and saturate my eyes — because the delicate snowflakes melted away as soon as
my snow-blind eyes could focus on the crystal caught. Think about it, then:
where does your own conception of snowflake-shape arise from? Does it come from
firsthand experience, or from clip-art and wintertime papercrafts?</p>
<blockquote>
<p>(Of course, there <em>is</em> something to be said for indirect inference of
snowflake-shape. Years later, I would marvel at the fact that you could infer
that snowflakes are hexagonal from the existence of <a href="https://github.com/kach/rocket-flame-renderer">22º
haloes</a>, a kind of
crystallography at the heavenly scale, linking the macroscopic and
microscopic. For now however I am concerned with seeing <em>directly</em>, and face
to face, with a presence and visual immediacy that cannot be afforded by
imagination alone.)</p>
</blockquote>
<p>This is no small concern. A year or so ago I visited the California Academy of
Sciences and learned about Wilson Bentley, the first man to photograph a
snowflake. This was on January 15, 1885; he was just shy of 20 years old. To
appreciate this moment, you have to imagine a time <em>before</em> January 15, 1885,
when no such photograph existed. Until Bentley’s innovation, no man had seen a
single snowflake for more than a delicate glimpse. Perhaps spatially the
snowflake could be magnified under a microscope (as <a href="https://en.wikipedia.org/wiki/Timeline_of_snowflake_research">Hooke and his
contemporaries</a>
demonstrated in the 17th century), but how could anyone magnify it
<em>temporally?</em> Bentley’s Edgertonesque miracle — akin to Muybridge’s “horse in
motion,” captured just seven years earlier — is to hold still the moment to
reveal <em>more</em> — to give us the opportunity to gaze without urgency. Dare I
say, Bentley <em>freezes time?</em> The manipulation of time becomes an <a href="electric-guitar.html">act of
unconcealment</a>, and then furthermore an act of creation.
Bentley’s snowflakes outlived him, and they will outlive me.</p>
<hr>
<p>Wilson Bentley was not a scientist by training, and perhaps that served him
well. Over the course of his life he collected over 5,000 images of what he
called “ice flowers.” There is something marvelous about this devotion, which
spanned years and years, crystal after crystal, wonder undiminished. Surely the
5,000th snowflake brought Wilson as much delight as the 4,999th (and surely it
is the case that you know you are in love when you find yourself falling in
love afresh every day).</p>
<p>There is also something to the uniformity of Bentley’s collection. Each
snowflake is framed carefully in the center of a slide, on a black background.
The images are diligently cropped to squares. See for yourself! The University
of Wisconsin-Madison hosts an <a href="https://library.ssec.wisc.edu/bentley/">online
archive</a> of 1,181 of Bentley’s images,
courtesy the Schwerdtfeger Library. Bentley never sought copyright on his
images; they are in the public domain, and so I downloaded them, along with
their labeled categories. The categories have wonderful names, such as “crystal
with broad branches” and “lumped graupel.”</p>
<p>When displayed in a grid, the snowflakes look like a box of delicious holiday
sweets. Surely these delights are made of sugar and not ice.</p>
<p><img src="/static/blizzard-blossoms/snowflakes.png" alt="snowfakes"></p>
<p>What does one <em>do</em> with 1,181 snowflakes? One searches for patterns, the way
Bentley would never have been able to. Bentley’s images demand to be studied as
samples from a distribution, and they are strikingly well-suited to our
favorite statistical computer vision techniques. For example, though they are
scanned at a uniform 1000x1000 resolution, they still make sense when
dramatically downscaled, even to as low as 32x32. Notice also how the
snowflakes are highly amenable to <em>data augmentation</em> on account of their
intrinsic symmetry. Each snowflake can be reflected and rotated arbitrarily to
produce several more high-quality synthetic snowflakes “for free.” So, even
though there are nominally 1,181 unique images, there is much more juice here
than one might expect at first. You can’t always play this game: CIFAR images
can’t really be rotated, and most MNIST digits can’t even be flipped. But
snowflakes are snowflakes.</p>
<p>One February evening I trained — with a lot of help from my friends Gautam
and Jack — a baby variational autoencoder, which embeds the snowflakes in a
small low-dimensional latent space. Here, “low-dimensional” means
10-dimensional: it seems very reasonable to encode 2^10 data points with 10
parameters. In the image below, the top row consists of five randomly-selected
source images, and the bottom row is the VAE’s attempted reconstruction. The
contrastive green-and-purple colorscheme comes from matplotlib’s default, and
reminds me of the delightful pGLO experiments we did in high-school biology.</p>
<p><img src="/static/blizzard-blossoms/vae.png" alt="snowfakes"></p>
<p>Of course, as with all things deep-learning, this was easier said than
done. For a technique that falls broadly under “unsupervised learning,” a VAE
requires an awful lot of babysitting! —but then again, one of the joys of
online school is that <em>nobody can tell</em> if you’re training machine learning
models behind your Zoom tab. So I trained, and I trained. The current model
takes around 6 hours on my dusty old MacBook, though after the first 30 minutes
the results are already non-embarrassing.</p>
<blockquote>
<p><strong>Do you want to build a snowgan?</strong> Separately, I also spent some time trying
to build a model adversarially. The snowgan (so to speak) consists of two
rival sisters: the generator, Elsa, is of the same architecture as the VAE’s
decoder, and the discriminator, Anna, learns to distinguish Elsa’s output
from real snowflakes. Elsa and Anna compete, and as a result Elsa gets better
at camouflaging the fakes among the flakes. Alas, GAN convergence is finicky
enough that I lost interest after a few days… open problem!</p>
</blockquote>
<p>It turns out that the VAE’s decoder can be used to build a small-scale
snowflake factory. Using ONNX.js and only a little bit of guile, I ported the
VAE’s decoder to JavaScript and attached the latent space to some <code>&lt;input
type=&quot;range&quot;&gt;</code> sliders. By using the sliders to choose parameters in the latent
space and feeding these parameters to the decoder, one can interpolate to
hallucinate snowflakes that are <em>not</em> part of Bentley’s training set. With some
experimentation, I found the sliders that correspond to parts of the latent
space that map to outliers; those parameters I clamped to extreme values in the
non-outlier-ey direction. This interface is what you see at the top of this
blog post.</p>
<blockquote>
<p>At time of writing, there is only one Google search result for “this
snowflake does not exist,” which points to a 2017 <a href="http://etd.lib.metu.edu.tr/upload/12621180/index.pdf">master’s
thesis</a> in
elementary-school education by Eli̇f Dedebaş. The context is an activity where
children are making paper snowflakes, and investigating the relationship
between the number of folds and the symmetries. One child creates a
snowflake-like shape with <em>eight</em>-fold symmetry and is asked if she knows why
such a shape is unnatural. Shape!</p>
</blockquote>
<hr>
<p>I imagine this slider infrastructure is on par with the sort of fancy tooling
UI that they have up in the Great Snow Factory In The Sky. I’m reminded of
David Wiesner’s wonderful Caldecott-winning picture book <em>Sector 7</em>, which I
recall reading (“reading”) in my elementary school’s library. It is about a
cloud factory, accessed from the top of the Empire State Building. Notice how
the large sousaphone-bell-shaped openings emanating from the factory roof are
exactly a portrait of an autoencoder’s decoder.</p>
<p><img src="/static/blizzard-blossoms/sector-7.png" alt="snowfakes"></p>
<p>“Wiesner visited the Empire State Building on a zero-visibility day to research
this book,” reads the About-the-Author at the end, “He was the only person
there.” There is something of Wiesner in Bentley and Bentley in Wiesner, I
think. Think of them both, and yourself, too, as Friedrich’s <em>Wanderer</em>.</p>
<p><img src="/static/blizzard-blossoms/wanderer.png" alt="snowfakes"></p>
<hr>
<p>The results interpolated by the VAE are surprisingly convincing.  Here are some
snowflakes that do not exist (snowfakes?). I created these images by sliding
the sliders haphazardly and recording a trace of the changing image.  Often,
people display this kind of interpolation in animated form, but in relation
— and, indeed, deference — to Bentley’s life’s work, I would rather see
them spread spatially than temporally.</p>
<p><img src="/static/blizzard-blossoms/snowfakes.png" alt="snowfakes"></p>
<p>I enjoy trying to work out the exact moment of transition between different
kinds of snowflakes; there is a snowy saltation that occurs at certain
boundaries that I cannot quite point you to. A curious fact is that a t-SNE
plot in the VAE’s latent space shows no clustering with respect to the provided
labels; the VAE seems to have hallucinated its own classification scheme, one
which is perhaps more macroscopically morphological in nature than Magono and
Lee’s 1966 classification scheme.</p>
<hr>
<p>The BentleyBlizzardBlossoms (B3) dataset, along with some PyTorch utilities and
the source code and trained model of the baby autoencoder, is available <a href="https://github.com/kach/bentley-blizzard-blossoms">on
Github</a>. I think it would
make a wonderful alternative to MNIST for small experiments. If you use it for
anything, please let me know!</p>
]]></description>
            <link>https://hardmath123.github.io/bentley-blizzard-blossoms.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/bentley-blizzard-blossoms.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 23 Feb 2021 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Learning to Play the Chaos Game]]></title>
            <description><![CDATA[<p>I, Thomasina Coverly, have found a truly wonderful method whereby all the forms of nature must give up their numerical secrets and draw themselves through number alone…</p>


<p>It’s Christmastime again! Shall we talk about trees? This post is about my new
holiday hobby: hallucinating tree-shaped fractals using our old friend,
gradient descent.</p>
<p><img src="static/chaos-game/tree-building.gif" alt="Christmas tree"></p>
<hr>
<p>But — let me start at the beginning. For a variety of reasons, the cover
image of Douglas Hofstadter’s <a href="https://en.wikipedia.org/wiki/I_Am_a_Strange_Loop"><em>I am a Strange
Loop</em></a> has been on my mind
this month. I have been thinking a lot about this wonderful image created by
pointing a camera at a projector that displays what the camera is seeing. In
modern terms: what happens if you screen-share the screen sharing window? This
happens:</p>
<p><img src="static/chaos-game/strange-loop.png" alt="from the cover of the book"></p>
<p>Such “feedback loop” recursions often rapidly converge to breathtaking
fixed-points. The NYC MoMath even has an <a href="https://momath.org/25-feedback-fractals-1/">art
installation</a> that lets you play
with a camera-and-projector setup to make <a href="https://twitter.com/momath1/status/824740594564075520">wonderful
patterns</a>.</p>
<p>As you can imagine, there is a mathematical theory here. At risk of ruining the
mystery, I will tell you about it, though without writing any equations. :-)
The theory is the theory of <a href="https://en.wikipedia.org/wiki/Iterated_function_system">Iterated Function
Systems</a>, which is
almost exactly what it sounds like. Start with a set of affine functions
(affine because that’s what camera-projector-systems do) and repeatedly apply
them to a set of points, taking the union at each step. Regardless of where you
start, you will soon end up with a fractal structure which is the fixed point
of the system. Why fractal? —because the self-similarity comes from the
infinitely-nested composition of the affine functions.</p>
<p><img src="static/chaos-game/Ifs-construction.png" alt="from Wikipedia"></p>
<p>There are theorems that ensure convergence towards and uniqueness of this fixed
point in the limit, but… it’s believable enough, don’t you think? Following
your intuition, you can build Sierpinski Triangles and <a href="https://en.wikipedia.org/wiki/Barnsley_fern">Barnsley
Ferns</a> and all sorts of other
beautiful structures using an IFS. I refer you to Section 8.2 of <a href="http://algorithmicbotany.org/papers/#abop"><em>The
Algorithmic Beauty of Plants</em> by Prusinkiewicz and Lindenmayer (yes, <em>that</em>
Lindenmayer)</a> for more botanical
connections.</p>
<p><img src="static/chaos-game/Fractal_fern_explained.png" alt="from Wikipedia"></p>
<hr>
<p>Now here is the question that has been on my mind: if I give you a picture of a
fractal-fern, can you give me a set of affine functions whose fixed point is
that fern? This is the <a href="https://en.wikipedia.org/wiki/Iterated_function_system#The_inverse_problem"><em>inverse
problem</em></a>
for IFSes. According to Wikipedia, this problem has real-world applications in
image compression, but it is <em>hard</em> to do in general.</p>
<p>Hmm. Interesting. Can our favorite tool, gradient descent, come to the rescue?
At least in an approximate sense? Recall that a two-dimensional affine function
is really just a 3x3 matrix (with 6 free parameters) that encodes the details
of the transformation. If we could easily compute images of IFS fixed points
from a set of matrices, then perhaps we could try to optimize the parameters of
these matrices to make the fixed point look the way we want it to.</p>
<p>The challenge, of course, is efficiently <em>finding</em> the fixed point —
repeatedly rasterizing and compositing affine transformations on images sounds
expensive — and doing so differentiably sounds like a scalability nightmare!
But there is a wonderful solution to this problem, which is to play the
so-called “<a href="https://en.wikipedia.org/wiki/Chaos_game">chaos game</a>.” Instead of
<em>densely</em> applying all the functions to all the points on the plane, you can
<em>sparsely</em> approximate the fixed point as follows: Start with a point — any
point! — and <em>randomly</em> select one of the affine functions of the IFS and
apply it to the point. Repeat this process with the new point. It turns out
that in the limit, the “trail” left behind by this wandering point converges to
the fixed point of the IFS. (This, too, is a theorem, but again I think it is
believable enough that I will not demand a proof.) This is what the “chaos
game” looks like for the Sierpinski triangle; notice how the salt-and-pepper
spattering of points soon converges boldly into the fractal we know and love
(it’s an animation — stare for a few seconds).</p>
<p><img src="static/chaos-game/Sierpinski_chaos_animated.gif" alt="from Wikipedia"></p>
<p>So here is the plan: we start with a point, and play the chaos game with our
current IFS matrices to obtain a <em>point cloud</em> that stochastically approximates
the fixed-point. Then, we compare this <em>point cloud</em> to our target fern-image
to obtain a “loss.” Finally, we update our IFS matrix parameters to minimize
this loss, until at last the fixed-point converges to the target image. It’s
just “machine learning,” really: we’re learning to play the chaos game!</p>
<p>Here is an outline of the algorithm (with some details elided):</p>
<pre><code class="lang-python"># initialize IFS
F = [random_3x3_affine() for _ in range(4)]
o = torch.optim.Adam(F, lr=0.0001)

for step in range(100_000):
    o.zero_grad()
    # start at origin
    v = torch.tensor([0., 0., 1.])

    # play the chaos game once
    trace = []
    for _ in range(200):
          # applying an affine function is just
        # a matrix multiplication!
        v = torch.matmul(random.choice(F), v)
        trace.append(v)

    # treat the trace as a point cloud
    loss = compare(target_image, trace)

    # update parameters
    loss.backward()
    o.step()
</code></pre>
<p>Ah! Actually, there is <em>one</em> detail that I <em>should</em> explain: how do you compare
a point cloud and an image? My idea is to convert the target image to a
<em>second</em> point cloud by uniformly sampling points from it, for example by
rejection-sampling. Then you can compare the two point clouds by the so-called
“chamfer distance,” which is the mean distance from each point to its nearest
neighbor in the opposite point cloud. This is quadratic-time to compute, and
the most expensive part of the whole operation, but with ~100 points in each
set it is quite doable.</p>
<blockquote>
<p>A pedagogical aside: notice that the “trick” here is really a <em>change in
representation.</em> This optimization problem is easier to solve on point clouds
than on raster images! An important lesson, that applies across the
ML-for-graphics domain… and more broadly to all differentiable programming
enterprises.</p>
</blockquote>
<hr>
<p>Remarkably, this zany scheme works quite well! Let’s try it with a heart. (A
heart, because it’s an easy low-entropy shape, but also because it felt
symbolically appropriate in relation to Hofstadter’s broader philosophical
project of souls-within-souls-within-souls.)</p>
<p>When we initialize the system with 4 random affine transformations, the trail
left behind by the chaos game is very unimpressive. The sparsity is because I
simulate only 200 steps of the chaos game for each step of gradient descent —
there’s a tradeoff between noise and computation time, of course, as there is
with any form of SGD.</p>
<p><img src="static/chaos-game/heart-samples-early.png" alt="chaos game on a heart"></p>
<p>Okay. Time to optimize! Here is what it looks like after 100K steps of
optimization with an Adam optimizer (that’s about 40 minutes of wall-time
computation on my old MacBook). It looks pretty good, don’t you think? The
chamfer distance scheme works!</p>
<p><img src="static/chaos-game/heart-samples.png" alt="chaos game on a heart"></p>
<p>Now we can export the four affine matrices out from PyTorch and work out the
“true” fixed point with a more expensive offline computation (for example, by
simulating a few <em>million</em> steps of the chaos game). It looks like this — not
perfect, and indeed quite crayon-scribble-ey, but clearly <em>something</em>
interesting has happened, and the result is convincingly heartlike.</p>
<p><img src="static/chaos-game/heart-fractal.png" alt="extended chaos game on a heart"></p>
<p>Where is the fractal structure hidden in this heart? This needs some
vizualization tooling to see clearly. After a bit of JS-canvas-hacking: here is
a GIF of the heart that reveals its fixed-point structure.
Hearts-in-hearts-in-hearts!</p>
<p><img src="static/chaos-game/heart.gif" alt="Heart"></p>
<p>I will admit to blinking a few times when I first saw this animation play out
in full — I did <em>not</em> expect it to work, and even now I find myself marveling
at this creation? discovery? of a heart encoded in 24 numbers.</p>
<hr>
<p>But it still doesn’t look <em>quite right</em> — the fractalness isn’t <em>obvious</em>.
How come? It’s because our affine transformations stretch and squash the heart
into almost unrecognizable blobs. A final, natural improvement is to force our
affine transformations to be <em>rigid</em>, so that there isn’t any squishing or
skewing in the fractal. This is actually not too bad to implement: the solution
to problems of constraint are typically reparametrizations, and indeed in this
case we can simply reparametrize the matrices in terms of a rotation, an
(isotropic) scale, and a translation. Now we’re down from 6 parameters per
matrix to just 4 — even more magical! The result is a much more “obviously”
fractal-ey fixed point, simply because your eye can more easily pick out the
structural recursion when it is made of rigid motions.</p>
<p>Let’s take it for a — pardon — <em>spin!</em></p>
<p>Since I promised you trees, here is some fractal foliage. The target image was
a picture of a maple leaf! I’d never thought about this before, but indeed a
maple leaf contains within it an echo of the whole tree. (Full disclosure: it
takes a couple of randomized restarts to get a really impressive result — the
low dimensionality of the fractal’s parametrization leads to the same
stuck-in-a-local-optimum woes that differentiable rendering folks face all the
time…)</p>
<p><img src="static/chaos-game/foliage.gif" alt="Leaf"></p>
<p>This one uses just three matrices — experimentally, three matrices tend to be
enough to get really good results, and more just lead to very busy and crowded
fractals. (I don’t know if there is a word for number-of-maps–in-an-IFS, but
let’s call it the IFS’s <em>arity</em>. In this terminology, I like <em>ternary</em> IFSes.)</p>
<p>And finally, since I promised you Christmas trees…</p>
<p><img src="static/chaos-game/christmas-tree.gif" alt="Christmas tree"></p>
<p>This last animation is actually absolutely <em>baffling</em> to me. In part, this is
because of how <em>treelike</em> the fractal turned out — here it is overlayed
against the silhouette I optimized against. Can you imagine this is encoded by
just 12 parameters? Odd.</p>
<p><img src="static/chaos-game/christmas-tree-match.png" alt="Tree vs. silhouette"></p>
<p>But even more bafflingly: the actual geometry of the recursion is <em>nothing</em>
like the tree recursion geometry you and I are used to from CS106! Compare the
GIF above to Barnsley’s Fern: while Barnsley turns each full leaf into two
smaller leaves with “semantic” affine maps, this Christmas tree does all sorts
of <em>bizarre</em> uninterpretable cartwheels and somehow almost magically works
itself out in the end. It is mesmerizing to me, I can stare at it for minutes
at a time.</p>
<p>When you think about it, it is quite shocking that a Christmas tree is the
<em>unique</em> fixed point of these 3 rigid affine maps. “Beautiful” isn’t the word
for it — neither is “grotesque” (though both words have been used by various
people I have shared this with). There is something just unsettlingly
fascinating about the way something magical <em>pops out</em> from three rectangles.
Maybe I should have demanded a proof of that theorem after all…</p>
<hr>
<p>There is so much more to say about all of this. What shapes are easiest to
IFSify, and why? In higher dimensions, can we treat the IFS as a kind of
SVD-esque dimensionality reduction scheme for data (i.e. literal point clouds)?
What kind of data would it make sense to do this for — where do you find
spatial self-similarity? Do point clouds have an inherent IFS “dimensionality”
(in terms of, say, the number of affine transformations you need to get a good
approximation)? With some more work, could SGD be a legitimate solution to the
open problem of fractal compression?</p>
<p>It’s a lot to think about — but for now… well, there is a Calvin and Hobbes
gag that begins with Calvin saying “I’ve been thinking” and Hobbes interrupting
“On a weekend?” That’s how I’ve been feeling this winter break!  Now it is time
to shutdown the jupyter kernel and get some rest — Comfortably Numbered will
return in 2021!</p>
<p>(Jupyter notebook for this blog post available <a href="https://github.com/kach/chaos-game-fractal-foliage">on
Github</a>.)</p>
<hr>
<p>Update (Dec 27): Check out some
<a href="https://news.ycombinator.com/item?id=25551557">improvements</a> made by a reader!</p>
<p>Update (Dec 29): I was pointed to <a href="http://demo.cs.brandeis.edu/papers/wcci98.pdf">a 1998
paper</a> (Melnik and Pollack) that
(independently!) tells <em>exactly</em> the same story. I’m struck by how closely the
two expositions align. There is something deeply comforting about rediscovering
a place someone else has visited once-upon-a-time, like finding a flag buried
under the snow on a mountaintop — a kind of storybook enchantment that sits
across the table from loneliness.</p>
]]></description>
            <link>https://hardmath123.github.io/chaos-game-fractal-foliage.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/chaos-game-fractal-foliage.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 25 Dec 2020 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Systems for Coordinating Coordinate Systems]]></title>
            <description><![CDATA[<p>Making 3D graphics code safer by telling vectors where home is</p>


<p>When I took Pat and Matt’s wonderful graphics class CS348B (yay PBRT!), one of
the most frustrating debugging journeys I had was related to interpreting
vectors in the wrong coordinate systems. At the time I’d come up with an idea
to catch such bugs statically, and now, 1.5 years later, I finally have found
time to write down the idea.</p>
<p>Here is the problem: A vector only has “meaning” with respect to a coordinate
system. In graphics, a vector could be in all sorts of coordinate systems:
<em>world space</em>, or <em>object space</em>, or <em>camera space</em>, or <em>screen space</em>, or
<em>normalized device coordinate (NDC) space,</em> or <em>raster space</em> (some of these
are self-explanatory, but see <a href="http://www.pbr-book.org/3ed-2018/Camera_Models/Projective_Camera_Models.html">PBR
&sect;6.1.1-6.2</a>
for definitions). This leads to bugs. If you interpret a “camera-space” vector
as if it were in “world space,” your math will be <em>all wrong!</em>  You need to
apply the affine transformation <code>cameraToWorld</code> before doing any world-space
computation on a camera-space vector. But what if you forget to do that? Your
C++ code will happily compile, but your poor bunny might look like chicken soup
instead… (I learned this the hard way!).</p>
<p><img src="static/pbrt-coordinate-system-diagram.png" alt="some pbrt coordinate systems"></p>
<p>This got me thinking: perhaps a safer <code>Vector3f</code> should know <em>at the type
level</em> what coordinate system it is in. Rather than a <code>Vector3f</code>, perhaps what
we really want is a <code>Vector3f&lt;WorldSpace&gt;</code>. You simply can’t add a
<code>Vector3f&lt;CameraSpace&gt;</code> to a <code>Vector3f&lt;WorldSpace&gt;</code> — that’s a <em>type error!</em>
The compiler <em>makes</em> you call <code>cameraToWorld</code> if you want to do anything with
these two vectors. (All this is very much in the spirit of
<a href="https://github.com/kach/torchsaber">torchsaber</a>, by the way.)</p>
<p>I think there are broadly two ways you might want to implement something like
this. One way is to treat <code>CameraSpace</code> as a purely formal symbol, just a
<em>name</em> or an <em>annotation</em>. The compiler’s job is to check these annotations.
You’d have to explicitly break this abstraction inside the implementation of
<code>cameraToWorld</code>, but from a pedagogical perspective I think that is exactly
what you want.</p>
<p>The other way is for the tag <code>CameraSpace</code> to somehow encode the actual
geometry of what <em>camera space</em> “means.” For example, each vector could also
carry around the <code>Transformation</code> that gets you to its host coordinate system
from <code>WorldSpace</code>. Given this information, the compiler could even be able to
<em>infer</em> the implementation of <code>cameraToWorld</code> for <em>free!</em> Of course, this comes
at a cost, which is that the compiler might not be able to statically check
equality of <code>Transformations</code>, because the matrices might not be materialized
at compile-time.</p>
<hr>
<p>In the rest of this post I’ll quickly literate-ly program option (1) as a proof
of concept. It is short and sweet (at least, as “sweet” as any bit of C++
hacking can be).</p>
<p>First, we define our coordinate system “annotations” as dummy classes.</p>
<pre><code class="lang-c++">class Space {};
class WorldSpace : public Space {};
class CameraSpace : public Space {};
</code></pre>
<p>Next, we design a <code>Vec3</code> template class that takes in such a “tag” as a
template parameter (<code>CoordinateSystem</code>), and enforces that the tags are the
same on operations. I’ve implemented the “<code>+</code>“ operator but you can imagine the
rest — there is nothing sneaky going on here! In fact, this is exactly how
one implements a generic <code>Vector3</code> that can be specialized over numeric types
(e.g.  <code>Vector3&lt;float&gt;</code>, <code>Vector3&lt;int&gt;</code>, and so on).</p>
<pre><code class="lang-c++">template &lt;class CoordinateSystem&gt;
class Vec3 {
  static_assert(
    std::is_base_of&lt;Space, CoordinateSystem&gt;::value,
    &quot;Vec3 annotation must derive from Space&quot;
  );

public:
  double x, y, z;
  Vec3(double x, double y, double z)
    : x(x), y(y), z(z) {};

  // the &#39;+&#39; operator!
  Vec3&lt;CoordinateSystem&gt; operator+(
    Vec3&lt;CoordinateSystem&gt;&amp; other
  ) {
    return Vec3(
      this-&gt;x + other.x,
      this-&gt;y + other.y,
      this-&gt;z + other.z
    );
  }
};
</code></pre>
<p>(Note that you need to <code>#include &lt;type_traits&gt;</code> to get <code>std::is_base_of</code>, which
is there just to protect you from trying to make a <code>Vec3&lt;string&gt;</code> or something
else uncouth like that. Completely optional.)</p>
<p>…actually, that’s it! We can already get some mileage out of this. For
example, this should be okay:</p>
<pre><code class="lang-c++">Vec3&lt;WorldSpace&gt; p(0, 0, 0);
Vec3&lt;WorldSpace&gt; q(1, 1, 1);
auto r = p + q; // ok!
</code></pre>
<p>On the other hand, this is perhaps <em>not</em> okay:</p>
<pre><code class="lang-c++">Vec3&lt;CameraSpace&gt; s(2, 2, 2);
auto t = r + s; // NOT OK!
</code></pre>
<p>And indeed, the compiler complains <em>and</em> gives a helpful error message!</p>
<pre><code>test.cc:40:14: error: no match for &#39;operator+&#39;
(operand types are &#39;Vec3&lt;WorldSpace&gt;&#39;
and &#39;Vec3&lt;CameraSpace&gt;&#39;)
   40 |   auto t = r + s; // NOT OK!
      |            ~ ^ ~
      |            |   |
      |            |   Vec3&lt;CameraSpace&gt;
      |            Vec3&lt;WorldSpace&gt;
</code></pre><p>Of course! How <em>dare</em> we add a camera-space vector to a world-space vector? We
need to transform <code>s</code> to world-space…</p>
<pre><code class="lang-c++">Vec3&lt;WorldSpace&gt; cameraToWorld(
  Vec3&lt;CameraSpace&gt;&amp; vec
) {
  return Vec3&lt;WorldSpace&gt;(
    vec.x * 2, vec.y - 4, vec.z + 8 // or whatever
  );
}
</code></pre>
<p>…and now <code>g++</code> is happy. :)</p>
<pre><code class="lang-c++">auto u = r + cameraToWorld(s); // ok!
</code></pre>
<p>Perhaps this trick will make its way into PBRT someday!</p>
<hr>
<blockquote>
<p>Update (Dec 23): Rachit Nigam pointed me to this (very recent) SPLASH
<a href="https://2020.splashcon.org/details/splash-2020-oopsla/49/Geometry-Types-for-Graphics-Programming">paper</a>
that considers the same problem. (“See? It’s not just me!”)</p>
</blockquote>
]]></description>
            <link>https://hardmath123.github.io/systems-for-coordinating-coordinate-systems.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/systems-for-coordinating-coordinate-systems.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 15 Dec 2020 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Clogs and Leaks: Why My Tensors Won't Flow]]></title>
            <description><![CDATA[<p>Defining a class of tricky bugs in PyTorch programs</p>


<blockquote>
<p><em>Note:</em> Jekyll and Hyde — is how this blog goes. There are weeks of
rational thought and weeks of irrational ramblings. This fall has been much
of the latter, for good reason, but here is a break(!) in the clouds.</p>
<p>Okay, okay. The truth is that I wrote this essay for my CS class this quarter
(leave it to Pat to assign an essay for a CS class!). But then, I think I
have reached the age where every written assignment in college is to be
treated as an opportunity to say something I otherwise would not have a
chance to say… and so the essay, lightly edited, finds its way to this
blog.</p>
</blockquote>
<hr>
<p>In this essay I want to describe two kinds of tricky bugs that might creep into
your PyTorch programs. I call the bugs “clogs” and “leaks.” In my mind “clogs”
and “leaks” reveal an exciting possible research direction for anyone
interested in designing better APIs for automatic differentiation in machine
learning.</p>
<p><em>Note: The examples presented, though in principle timeless, were tested using
Python 3.8.5 running PyTorch 1.6.0.</em></p>
<h2 id="preliminaries-pytorch-s-pipes">Preliminaries: PyTorch’s Pipes</h2>
<p>If you are familiar with PyTorch internals, you can skip this section. If not,
a brief review of a wonderful topic: how does PyTorch differentiate your code
for gradient descent? The technical term for PyTorch’s approach is <em>tape-based
reverse-mode automatic differentiation</em>. As you perform arithmetic computations
on your variables, PyTorch tracks the intermediate values in a computation
graph. When you want to differentiate a value, you call <code>.backward()</code> on that
value. PyTorch then walks <em>backwards</em> along this computation graph, computing
the derivative at each step and accumulating them according to the chain rule.
Eventually, the leaf nodes of the graph contain the derivatives you asked for.</p>
<p>Let me give a small example. Suppose we wanted to compute $d2x^2/dx|_{x=3}$.
We might write a program that looks like this:</p>
<pre><code class="lang-python">x = torch.tensor(3., requires_grad=True)
y = 2 * x**2
y.backward()
print(x.grad)
</code></pre>
<p>This program generates the following computation graph.</p>
<p><img src="static/clogs-and-leaks/forward.png" alt="Forward"></p>
<p>When you call <code>.backward()</code>, the PyTorch automatic differentiation walks
backwards along the graph, computing derivatives at <em>each</em> step. By the chain
rule, the product of these gives the overall derivative we sought.</p>
<p><img src="static/clogs-and-leaks/backward.png" alt="Forward"></p>
<h2 id="clogs">Clogs</h2>
<p>Now, consider this simple PyTorch program to compute $d(\sqrt{x} +
x)/dx|_{x=4}$. What do you expect to be printed?</p>
<pre><code class="lang-python">x = torch.tensor(4., requires_grad=True)
y = sqrt(x) + x
y.backward()
print(x.grad)
</code></pre>
<p>A casual user or AP calculus student would <em>expect</em> to see 1.25 printed, of
course. But what <em>actually</em> gets printed is 1. Why?</p>
<p>Ah! I didn’t show you the full program: I hid the imports. It turns out that
the first line of this program is <code>from math import sqrt</code>, <em>not</em> <code>from torch
import sqrt</code>. Now, the Python standard library’s <code>math.sqrt</code> is not a
PyTorch-differentiable function, and so PyTorch is unable to track the flow of
derivatives through <code>sqrt(x)</code>.</p>
<p>As a result of this bug, backpropagation gets “stuck” on the way back, and only
the derivative of <code>x</code>, i.e. 1, is deposited. This is a clog — the gradients
can’t flow! In the computation graph below, the dotted arrow represents the
clog.</p>
<p><img src="static/clogs-and-leaks/clog.png" alt="Clog graph"></p>
<p>The reason calling <code>math.sqrt()</code> on a PyTorch tensor is not a runtime error is
that PyTorch tensors implicitly convert to “raw” floating-point numbers as
needed. Most of the time this is a useful and indispensable feature. But I
believe this situation should <em>at the very least</em> raise an error or a warning.
While the example I presented was reasonably straightforward, there are <em>many</em>
different ways to “clog” backpropagation, with varying degrees of insidiousness
(for example, what happens when you mutate a variable in place?). It can be a
nightmare to debug such situations when something goes wrong — that is, if you
notice the bug in the first place!</p>
<p><em>By the way:</em> the celebrated “reparametrization trick” that powers variational
autoencoders is really just a workaround for a gradient clog problem. To train
a variational autoencoder, you need to compute the derivative of a sample of a
probability distribution with respect to the distribution’s parameters (e.g.
the mean $\mu$ and variance $\sigma^2$ of a Gaussian distribution).
Unfortunately, naïvely sampling from a parametrized distribution abruptly
truncates the computation graph with respect to the parameters, because the
random number generator is not differentiable all the way through — who <em>knows</em>
what <em>it’s</em> doing! The solution, is to sample from a standard unit normal
distribution (where $\mu=0$ and $\sigma=1$, and then re-scale the sample
by multiplying by $\sigma$ and adding $\mu$. Of course, multiplication and
addition <em>are</em> easily differentiable, and so the gradients can now flow.
Problem solved!</p>
<h2 id="leaks">Leaks</h2>
<p>Now, consider this slightly more complicated PyTorch program. We are going to
implement a silly reinforcement learning algorithm. Here is the situation:
There is a truck driving on the road with constant velocity, and your goal is
to catch up to it and drive right alongside the truck. At each timestep you are
allowed to choose your velocity, and then you’re told how far you are from the
truck.</p>
<p>The setup:</p>
<pre><code class="lang-python">truck_velocity = torch.tensor(3.142)
truck_position = torch.tensor(2.718)

def get_measurement(car_position):
    global truck_position
    truck_position = truck_position + truck_velocity
    return torch.abs(truck_position - car_position)
</code></pre>
<p>And a simple online gradient-based learning algorithm:</p>
<pre><code class="lang-python">my_velocity = torch.tensor(0.01)
my_position = torch.tensor(0.)
for i in range(500):
    my_velocity.requires_grad_()
    my_position = my_position + my_velocity
    loss = get_measurement(my_position)
    loss.backward()
    my_velocity =\
        my_velocity.detach() - my_velocity.grad * 0.01
</code></pre>
<p>Unlike last time, there’s nothing up my sleeve here — this is all reasonable
PyTorch code. This code actually works just fine.</p>
<p>But, if you run it for long enough (say, 1000 iterations), you’ll notice
something odd: each step starts taking longer and longer. The algorithm is
<a href="https://accidentallyquadratic.tumblr.com">accidentally quadratic</a>! You can see
this behavior quite clearly in this graph, which shows a linear growth in
iteration time from step to step (the spikes are garbage collection pauses).</p>
<p><img src="static/clogs-and-leaks/graph.png" alt="Time graph, essentially linear and
increasing"></p>
<p>How can this be? Isn’t each loop doing the same calculation?</p>
<p>Here is one hypothesis: if you’ve read <a href="https://arxiv.org/abs/1909.13371">this
paper</a> you might look to see if we’re
<code>.detach()</code>-ing <code>my_velocity</code>. The <code>.detach()</code> function snips off all incoming
edges to a node in the computation graph; essentially, creating an artificial
clog. If we forget to do that, the gradients would “leak” back in time across
multiple steps in the graph, all the way back to the first step, and each
iteration would therefore take longer and longer — just as we’re observing.</p>
<p>But, alas, this is not the source of the bug: as you can see, we <em>are</em>
detaching <code>my_velocity</code> when we update it. So, what’s really going on here?</p>
<p>It’s tricky! The leak is in <code>my_position</code>, which subtly depends on <em>all</em>
previous values of <code>my_velocity</code> and therefore makes backpropagation compute
gradients for <em>all</em> previous timesteps. The dataflow diagram below hopefully
clarifies this point. Notice how each <code>velocity</code> has its parent nodes detached
(thanks to the call to <code>.detach()</code>!), but <code>loss</code> still has an indirect
dependence on the chain of <code>positions</code>.</p>
<p><img src="static/clogs-and-leaks/leak.png" alt="Leak graph"></p>
<p>Finding the correct place to insert the line <code>my_position =
my_position.detach()</code> is left as a not-quite-trivial exercise to the reader.
Beware! Putting it in the <em>wrong</em> place will either have no effect <em>or</em> cause
<code>my_velocity</code> to always have gradient 0.</p>
<p>Just like memory leaks, gradient leaks can be extremely sneaky. They pop up
whenever your inference is “stateful” — think of applications like physics
controllers, reinforcement learning, animated graphics, RNNs, and so on. I
would not be surprised if many popular implementations of such algorithms <em>do</em>
have “gradient leak” bugs. However, the bugs usually only manifest themselves
visibly when the inference passes through enough timesteps for the leak to
compound. Just like a dripping tap, you might not notice your losses until you
get the bill at the end of the month… and then, you need to figure out how to
track down the source of the leak and figure out the right way to fix it.</p>
<h2 id="plungers-and-patches-an-appeal-for-plumbing-">Plungers and patches? An appeal for PLumbing…</h2>
<p>In the long term, how can we protect ourselves from this class of bugs? One
potential solution is to embed the API inside a language whose type system
tracks the creation of the computation graph. You might be able to use
well-understood techniques like <em>taint analysis</em> or <em>linear types</em> (pun not
intended) which traditionally track the flow of <em>information</em>, to now track the
flow of <em>differentiability</em> through the program.</p>
<p>Let me be slightly more concrete about this suggestion. In our “clog” example,
a good type system might detect that <code>sqrt</code> cuts off the computation graph,
and, knowing that $y$ does not directly depend on $x$ in the expected way
anymore, complain at compile-time when we try to request $dy/dx$. In our
“leak” example, a good type system might notice that the “old” <code>my_position</code>
effectively goes out of scope when it is re-assigned, and therefore it might
complain that an unreachable reference to it actually does persist through the
new <code>my_position</code>. Such checks seem very reasonable to demand from a modern
type system.</p>
]]></description>
            <link>https://hardmath123.github.io/clogs-and-leaks.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/clogs-and-leaks.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 23 Nov 2020 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Public Time and the Privacy of Time]]></title>
            <description><![CDATA[<p>An old Levi’s ad that ran when I lived in India went like this: “It’s the thread in your seams that’s tied to your dreams. It’s the soles in your feet that keep the beat.”</p>


<p>Over the months I’ve started to miss the little speckles of Stanford life that
I never thought I would get attached to. One of them is the regular bell-toll
of our lovely little clock tower. There is something oddly compelling about
public time — time as a utility — time as something to be broadcast for
consumption. So I have a special fondness for clock towers, and the Network
Time Protocol, and the anchor who says “it’s 8am and this is NPR,” and the GPS
system, and people who set their clocks by the atomic clock in Boulder, and the
website <a href="https://www.time.gov">time.gov</a>, and the pilot who says “we will be
landing shortly in San Francisco, the local time is 6:15am.” At least, in this
divided world, we can agree on <em>something</em>.</p>
<p><img src="static/electric-guitar/edgerton.jpg" alt="edgerton"></p>
<hr>
<p>Well: This evening I was tuning my guitar, and I discovered some more public
time. Here is what happened. First I plucked the low E string, and watched the
string blur into a lenslike pair of arks that envelop the extent of its
vibration. I ran the first few frets to try out the sound: F, F-sharp, G,
G-sharp. Great!  Next, I plucked the A string, and watched the same blur of the
string. But when I tried plucking it at the first fret — that would be a
B-flat — something bizarre happened. Instead of a vibrating blur, the string
slowly wavered back and forth between its two extremes!</p>
<p>What? I rubbed my eyes. A low “Bb” vibrates at around 117 Hz. That is <em>much</em>
faster than my eye can resolve (because of this “persistence of vision,”
animations in video games don’t have much incentive to exceed around 60 frames
per second). In short, I should be seeing a blur. But no! I can track the
string’s leisurely wobble quite easily. I bet you can, too: the GIF below shows
an open-string A on the left and a first-fret Bb on the right.</p>
<p><img src="static/electric-guitar/comparison.gif" alt="gif"></p>
<p>(Sorry for the poor quality — I had to hold my phone between my knees — it
was tricky, okay? You can find the original videos, with sound, on Github.)</p>
<hr>
<p>Here’s what I think is going on. The lights in my room are powered by AC
current from the power grid. In the United States, AC current is standardized
to transmit at 60 hertz (why 60? apparently it’s a <a href="https://en.wikipedia.org/wiki/Utility_frequency#Standardization">long
story</a>, but in
part the number 60 is related to turbine hardware logistics). That means that
the current peaks in (absolute) voltage 120 times per second, and so the lights
in my room are actually all rapidly flickering at 120 Hz. I just don’t realize
it because of the persistence of vision.</p>
<p>Ah! As I mentioned above, a Bb vibrates at around 117 Hz — to be precise,
it’s 116.54 Hz. You can work this out without looking it up by recalling that
concert A is 440 Hz (as printed on every metronome) and the low Bb is two
octaves less one half step below — that’s twice twelve minus one or 23 half
steps below. Indeed, 440/(2^(23/12)) gives 116.54.</p>
<p>Why is this correspondence between 120 and 116.54 so interesting? Well, when
you have nearby frequencies, they inferfere to produce
<a href="https://en.wikipedia.org/wiki/Beat_(acoustics%29">“beat”</a> frequencies.  I’ve
written about beats in the spatial domain before when talking about <a href="moire.html">moiré
patterns</a>. Here, the beats are in the temporal domain, forming a
kind of <a href="https://en.wikipedia.org/wiki/Wagon-wheel_effect">“wagon wheel”</a>
illusion. The superposition of the string’s vibration and the light’s
flickering creates a low-frequency “beat” that gives the illusion of the string
vibrating slowly.</p>
<p>Eyeballing the right GIF, I count around 5 oscillations of the string over the
3-second clip. That gives a beat frequency of 1.67 Hz. The difference in
interfering frequencies is twice the beat frequency (elementary trigonometry
exercise, use the sum-to-product identity). Using that difference, I can
compute that my Bb is tuned to 120 Hz - 3.3 Hz = 116.7 Hz. Not bad! It turns
out that you can buy <a href="https://en.wikipedia.org/wiki/Electronic_tuner#Strobe_tuners">“stroboscopic
tuners”</a> online,
but perhaps if you are good at estimating slow frequencies, you don’t need
one…!</p>
<hr>
<p><img src="static/electric-guitar/muybridge.jpg" alt="muybridge"></p>
<p>Briefly, some thoughts for the future: I’m reminded by my guitar string of
Edgerton’s hummingbirds and Muybridge’s horses, and many more moments frozen in
time. There is a lot more I could say, and that I have said, and perhaps I will
explain this more carefully in a future essay. The confluence of the aural and
visual has also been on my mind lately; though I haven’t “blogged” about it I
have been <a href="https://cs.stanford.edu/~kach/can-one-hear-the-fate-of-a-coin.pdf">thinking a
lot</a> about
clocking coin tosses by recording the sound a coin makes while it rings. In
that story, too, stroboscopic techniques make an appearance. Maybe I will say
more about that as well.</p>
<p>For the moment however I am struck — “struck” — by how strange it is that
there is so much invisible to us even in plain sight, so much hidden in the
folds of time. Public time, and the privacy of time.</p>
<p><img src="static/electric-guitar/duchamp.jpg" alt="duchamp"></p>
]]></description>
            <link>https://hardmath123.github.io/electric-guitar.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/electric-guitar.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 13 Nov 2020 05:00:00 GMT</pubDate>
        </item>
    </channel>
</rss>