<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Comfortably Numbered]]></title>
        <description><![CDATA[My blog.]]></description>
        <link>https://hardmath123.github.io</link>
        <image>
            <url>https://hardmath123.github.io/static/avatar.png</url>
            <title>Comfortably Numbered</title>
            <link>https://hardmath123.github.io</link>
        </image>
        <generator>RSS for Node</generator>
        <lastBuildDate>Mon, 04 Nov 2024 15:39:06 GMT</lastBuildDate>
        <atom:link href="https://hardmath123.github.io/feed.xml" rel="self" type="application/rss+xml"/>
        <author><![CDATA[Hardmath123]]></author>
        <language><![CDATA[en]]></language>
        <item>
            <title><![CDATA[Play Me a High C]]></title>
            <description><![CDATA[<p>The world is like an apple whirring silently through space</p>


<p>After a chance conversation at our lab retreat, I got curious about what tides
“sound” like. I downloaded the NOAA’s <a href="https://tidesandcurrents.noaa.gov/stations.html">water level
data</a> for Boston Harbor from
the start of my PhD until today and sped it up by a factor of about 200
million.</p>
<p>You can clearly hear the tone representing the daily tides (pleasingly, it’s
tuned to “middle C”… get it?). The vibrato or “beating” is caused by the
difference between the solar and lunar constituents, and corresponds to the
semimonthly spring/neap tides. Finally, if you speed it up even more, you can
hear three annual “scrapes” corresponding to yearly variation (caused by the
Earth’s axial tilt?).</p>
<ul>
<li>200 million times faster:  <audio controls src="static/harbor.wav"></audio></li>
<li>800 million times faster:  <audio controls src="static/harbor-4x.wav"></audio>
</li>
</ul>
]]></description>
            <link>https://hardmath123.github.io/tides.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/tides.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Peripheral pareidolia]]></title>
            <description><![CDATA[<p>Faces that you can only see without looking</p>


<p>I work in a dense urban campus, and the view from my building looks into the windows of the next building over.</p>
<p><img src="../static/pareidolia-window.png" alt="The view from my building faces the windows of the next building over"></p>
<p>Sometimes, when I meet with my advisor, I sit at a table facing that window. On these days, a strange thing happens. When I look up at my advisor, I get the eerie sensation in the corner of my eye that there is a face in that window watching me. Of course, when I look at the window, the face vanishes—there is no one there.</p>
<p>I have been chalking this up to the paranoid hallucinations of a tired brain. But a chance email from a professor this week made me realize what is actually going on here.</p>
<p>When I look up at my advisor, my eyes bring them into focus. To do this, they converge (“cross”) slightly, so that the image is aligned on my retinas. What happens to the window in the background? Because it is behind my advisor, it <em>diverges</em> slightly: I actually see two copies of it superimposed with a slight horizontal displacement, kind of like a Magic Eye stereogram. Of course, the window also blurs out of focus—in part because it is far behind my eyes’ current focal plane, and in part because it is in my peripheral vision, which has significantly lower acuity.</p>
<p>Here is a video of what happens when you simulate diverging and blurring the window. I’ve tiled the image many times so that you always have a few copies in your peripheral vision.</p>
<video src="../static/pareidolia-video.mp4" style="width: 100%;" controls></video>

<p>Aha! At the end of the simulation, the window-images align perfectly to form a pair of little faces side by side. The light fixtures form the eyes and the nose, and the frame provides the lips and the mouth. The faces are not the clearest, but they are certainly face-like enough to evoke pareidolia.</p>
<p>Of course, if you were to look directly at the window, your eyes would unconverge, the windows would fuse, and the face would seem to disappear. So, <em>it’s not paranoia, it’s peripheral pareidolia!</em></p>
<p>(Food for thought: how can we automatically find/create more examples of “peripheral pareidolia”? Does this effect say something interesting about the organization of our visual system?)</p>
]]></description>
            <link>https://hardmath123.github.io/peripheral-pareidolia.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/peripheral-pareidolia.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 30 Apr 2024 04:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Undetectable Bayesian Improv Theater]]></title>
            <description><![CDATA[<p>How to pretend like you have a biased coin</p>


<p>Suppose two Bayesians, Alice and Bob, put on a variety show where they take
turns tossing a biased coin and announcing outcomes to a live studio audience.
(Bayesians love this kind of thing—it keeps them entertained for hours…)</p>
<p>Unfortunately, just as Alice goes on stage, she realizes with dread that she
forgot to bring the coin. Thinking on her feet, she mimes pulling a tiny
imaginary coin out of her pocket, and says “This is a biased coin!” It
works—the audience buys it and the crowd goes wild.</p>
<p>She mimes tossing the pretend coin and randomly announces “heads” or “tails.”
Then, she hands the coin to Bob, who (catching on) also mimes a toss. This has
just turned into a Bayesian improv show.</p>
<p>But now Bob has a problem. Should he announce “heads” or “tails”? He could
choose uniformly at random, but after many rounds the audience might get
suspicious if the coin’s bias is too close to 50%. How can he keep up the
charade of a <em>biased</em> coin?</p>
<p>Here’s what Bob does. In the spirit of “yes, and…,” he infers the coin’s bias
based on Alice’s reported outcome (say, with a uniform prior) and samples a
fresh outcome with that bias. So if Alice said “heads,” Bob would be a bit
likelier to say “heads” as well.</p>
<p>Then Alice takes the coin back and does the same, freshly inferring the coin’s
bias from the past <em>two</em> tosses. In this way, the two actors take turns
announcing simulated outcomes to the oblivious audience, while building a
shared understanding of the coin’s bias.</p>
<p>What happens? How can we characterize the sequence of outcomes? Intuitively, we
might expect either a “rich-get-richer” effect where they end up repeating
heads or tails. Or we might expect a “regression-to-the-mean” where they
converge to simulating a fair coin.</p>
<p>The surprising answer is that this process is indistinguishable from Alice and
Bob tossing a <em>real</em> coin with fixed bias (chosen uniformly). A critic lurking
in the audience would never suspect something afoot!</p>
<p>This result is a consequence of the correspondence between the Pólya
distribution and the Beta-binomial distribution.</p>
<p>I have a hunch that this observation could be useful: perhaps in designing a
new kind of cryptographic protocol, or perhaps in explaining something about
human cognition. If you have ideas, let me know!</p>
<hr>
<p>Proof sketch: Model the actors’ belief with a Beta distribution with parameters
$(h, t)$ initialized to $(1, 1)$, i.e. uniform. At each toss the probability of
heads is given by $h/(h+t)$, and the outcome increments $h$ or $t$ by 1. You
can think of this as a Pólya urn with h black and t white balls: each time you
draw a ball, you put it back and add a “bonus” ball of the same color. It is
well-known (look
<a href="https://djalil.chafai.net/blog/2015/11/30/back-to-basics-polya-urns/">here</a> or
<a href="https://math.uchicago.edu/~may/REU2013/REUPapers/Helfand.pdf">here</a> or
<a href="https://www.randomservices.org/random/bernoulli/BetaBernoulli.html">here</a>)
that this is the same as the Beta-binomial process.</p>
<blockquote>
<p>See also: <a href="https://a.exozy.me/posts/asian-bayesian-2/">cool new blog post that riffs on these
ideas</a></p>
</blockquote>
]]></description>
            <link>https://hardmath123.github.io/bayesian-improv.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/bayesian-improv.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Sun, 21 Jan 2024 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[The light tries to enter the long black branches]]></title>
            <description><![CDATA[<p>Thinking about a streetlight’s wooden halo</p>


<p>Have you noticed the way streetlights shine through the bare branches of trees
on cold winter nights? Walking home from work tonight I was struck by the
almost overwhelming beauty of the scene. The light creates a perfect,
glittering halo around itself, like the moon in a Van Gogh painting. The tree
in turn all but reaches its fingers out to grasp the light. If you move your
head from side to side, it feels as if a wormhole has opened in the tree,
sucking the branches into the light’s force field.</p>
<p>I think this happens because of the Fresnel effect. If the light hits a branch
at just the exact angle, the wood (like many other materials in the world)
becomes unexpectedly shiny. In the brambles that hang off of a tree’s bare
branch, the only twigs that catch the light’s glint are the ones that make a
specific shallow angle with your eye. By radial symmetry, it’s not hard to see
that this should result in a glowing circular halo. It’s similar to how divers
see a <a href="https://en.wikipedia.org/wiki/Snell%27s_window">circular window</a> on the
surface of the water when they look up, or the <a href="https://en.wikipedia.org/wiki/22°_halo">22º
halo</a> you sometimes see around the sun
— but here the medium is wood, not water.</p>
<p>If you live in a place that has seasons, I encourage you to go and see this
effect for yourself. It’s a hard phenomenon to capture on camera — it
requires high dynamic range, high-resolution video, and patience outdoors when
it’s cold and dark. For now, here is the best I could do with my old iPhone.
(I stabilized the light with a little Python script.)</p>
<video style="width: 100%;" controls>
  <source src="./static/tree-fresnel.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>For comparison, here are two frames from this video:</p>
<p><img src="./static/tree-fresnel-000.png" alt="Light outside tree"></p>
<p><img src="./static/tree-fresnel-196.png" alt="Light inside tree"></p>
]]></description>
            <link>https://hardmath123.github.io/tree-fresnel.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/tree-fresnel.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 27 Nov 2023 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Half a Jar of Honey]]></title>
            <description><![CDATA[<p>Investigating the stability of honey containers</p>


<p>I knocked over a tall jar of honey. Clumsy, clumsy! But the jar was nearly empty; jars tend to fall when they are nearly empty.</p>
<p>But jars also tend to fall when they are full, which got me thinking: for both full and empty jars, we can argue by symmetry that their center of mass is halfway up. By Rolle’s Theorem, then, at some point when the jar is partially filled it should have a minimally- or maximally-high center of mass. It’s easy to see that the center of mass can never get higher than halfway up. This suggests that the jar’s stability increases and then decreases as the honey is consumed.</p>
<p>When is the jar most stable? Working in units where the empty jar’s height and mass are 1, its radius is $r$, and honey’s density is $\alpha$, the height of the center of mass is given by:
$$
c = \frac{\alpha r^2h(h/2) + 1/2}{\alpha r^2h + 1}
$$</p>
<p>Let’s assume honey is extremely viscous (i.e. changes shape slowly), and that the jar tips over if its center of mass is over its rim. Then the maximum angle you can tip the jar before it falls over is:
$$
\theta^\star = \tan^{-1}(r/c)
$$
The question is then, for a given $r$ and $\alpha$, what $h$ maximizes $\theta^\star$? Intuitively, it seems like it should be somewhere halfway between empty and full.</p>
<p>We can compute the optimal $h$ by differentiating $\theta^\star$ with respect to $h$ and setting the derivative to zero:
$$
\frac{d\theta^\star}{dh}=-\frac{r^2}{r^2+c^2}\frac{dc}{dh} = 0
$$
The positive solution to this, thanks to WolframAlpha, is given by:
$$
h^\star=\frac{1}{1+\sqrt{\alpha r^2+1}}
$$
Does this make sense? As $\alpha r^2$ increases (heavier honey), the optimal height gets lower, which makes sense because the honey begins to have a ballasting effect.</p>
<p>Now we can plug in some numbers. A standard 8-oz honey bottle is about 6 inches tall, has a base radius of about 1 inch, and weighs about 1oz. The density of honey is 0.8oz per cubic inch. In our units, this means the jar’s radius is $1/6$ and the density of honey is $0.8/(1/6)^3$. Plugging this in, we find remarkably that $h^\star \approx 0.3$. So a jar of honey is most stable when only about 1/3 full!</p>
]]></description>
            <link>https://hardmath123.github.io/honey.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/honey.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Sun, 12 Nov 2023 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[How much should you stir your tofu?]]></title>
            <description><![CDATA[<p>Can stirring more ever lead to worse outcomes?</p>


<p>Suppose you are cooking tofu, cut into tasty 1-inch cubes. On your stove it takes each cube-face 10 minutes to cook. You could cook each of the 6 faces of each cube in one hour (60 minutes) by rotating the cube to an uncooked face every 10 minutes–but this is clearly tedious and suboptimal.</p>
<p>A better idea is stirring: instead of carefully rotating each cube to an uncooked face, you can vigorously stir your pan and reorient each cube uniformly at random (like rolling dice).</p>
<p>The question I want to consider today is, “how often should you stir your tofu cubes”? For example, you might choose to stir them every 10 minutes for an hour, for a total of 6 stirs including right at the beginning. In that case, you might get lucky and have every face exposed to heat once. However, because each stir is an independent random roll, the more likely scenario is that some face gets cooked twice or more (burnt!) and other faces don’t get any heat (raw!). <strong>Intuitively, it seems like we should stir more frequently to “even out” the heat.</strong> Let’s check this intuition.</p>
<hr>
<p>Suppose we stir $n$ times over the course of the hour, and we are willing to tolerate being off by up to $\delta=5\%$ of optimal cookedness–that is, consider the tofu raw before 9 minutes and 30 seconds, and burnt after 10 minutes and 30 seconds. Let $f(n)$ be the proportion of burnt or raw faces, in expectation. We want to see how $f(n)$ depends on $n$.</p>
<p>We can compute $f(n)$ many ways. Most obviously, we can simulate the tofu system with some large number of cubes and see what happens.</p>
<pre><code class="lang-python">import numpy as np
from matplotlib import pyplot as plt

mean = 1/6
tol = 0.05

def experiment(n):
    N = 1_000
    rolls = np.random.multinomial(n, np.ones(6)/6, size=N)
    toasts = rolls / n
    burns = ((np.abs(toasts - mean) &gt;= tol * mean) * 1.0).mean()
    return burns
</code></pre>
<p>It is also not hard to work this out analytically. First, by linearity of expectation we can just look at one face, which acts like a binomial random variable: for each of the $n$ stirs, there is a $1/6$ probability of getting heat. We can subtract the CDFs at the tolerance limits $\mu\cdot(1\pm\delta)$ to compute the proportion of times the face is well-cooked, and then subtract from 1 to get the proportion of bad faces.</p>
<pre><code class="lang-python">import scipy.stats

def analytical(n):
    return 1 - (
        scipy.stats.binom.cdf( (1 + tol) * (n / 6), n, 1/6 ) -
        scipy.stats.binom.cdf( (1 - tol) * (n / 6), n, 1/6 )
    )
</code></pre>
<p>We can also use statistics tricks to approximate the answer in useful closed forms. For example, we can take a normal approximation to the binomial as $n \rightarrow \infty$. The mean is $\mu=1/6$, of course, and the variance is given by $\sigma^2 = p(1-p)/n = 5/36n$. We can then plug those parameters into the normal CDF, along with the tolerance limits as above.</p>
<pre><code class="lang-python">def normal(n):
    vari = 1/6 * (1 - 1/6) / n
    z = (tol * mean) / np.sqrt(vari)
    return 1 - (scipy.stats.norm.cdf(z) - scipy.stats.norm.cdf(-z))
</code></pre>
<p>Finally, we can apply Chebyshev’s inequality or a Chernoff bound. These formulas only provide an upper bound on the proportion of bad faces, but they don’t depend on a normal approximation and are thus guaranteed to be true. I won’t work through the derivations here.</p>
<pre><code class="lang-python">def chebyshev(n):
    vari = 1/6 * (1 - 1/6) / n
    z = (tol * mean) / np.sqrt(vari)
    return np.minimum(1., 1 / z ** 2)

def chernoff(n):
    return np.minimum(1., 2 * np.exp(-(tol ** 2) / 3 * (1/6) * n))
</code></pre>
<p>Plotting all of these metrics out to asymptotic $n \rightarrow \infty$ (see the graph below), we see several expected patterns:</p>
<ol>
<li>As $n$ increases, the proportion of burnt faces goes down with exponential falloff.</li>
<li>The analytical solution closely tracks the simulation, at least until the variance of the simulation gets high enough to make it unreliable.</li>
<li>The normal approximation is indistinguishable from the analytical solution.</li>
<li>The bounds are quite loose. Chebyshev “wins” for a little while, but ultimately Chernoff’s exponentiality kicks in and dominates.</li>
</ol>
<p>So far, so good: it seems like you can get quite well-cooked tofu with only a logarithmic amount of stirring effort!</p>
<pre><code class="lang-python">ns = np.arange(1, 30000, 100)
plt.plot(ns, [experiment(int(n)) for n in ns], &#39;x&#39;, label=&#39;Simulation&#39;)
plt.plot(ns, [analytical(int(n)) for n in ns], &#39;-&#39;, label=&#39;Analytical&#39;)
plt.plot(ns, [normal(int(n)) for n in ns], &#39;--&#39;, label=&#39;Normal approximation&#39;)
plt.plot(ns, [chebyshev(int(n)) for n in ns], &#39;-&#39;, label=&#39;Chebyshev\&#39;s inequality&#39;)
plt.plot(ns, [chernoff(int(n)) for n in ns], &#39;-&#39;, label=&#39;Chernoff bound&#39;)
plt.yscale(&#39;log&#39;)

plt.ylabel(&#39;P(burnt or raw face)&#39;)
plt.xlabel(&#39;n, the number of tosses per hour&#39;)
plt.legend()
</code></pre>
<pre><code>&lt;matplotlib.legend.Legend at 0x7fbf0b32ae60&gt;
</code></pre><p><img src="static/tofubes/output_9_1.png" alt="png"  /></p>
<p>But asymptotic $n$ is unreasonable in this setting: for example, 6000 flips per hour (6 kfph) is more than one flip per second. Let’s zoom in on small $n$ to see what is happening at the scale of the real world. In this plot, I’m hiding the two bounds (which are all maxed out at 1) and only showing up to $n=600$, which is a stir every 6 seconds.</p>
<pre><code class="lang-python">ns = np.arange(1, 600)
plt.plot(ns, [experiment(int(n)) for n in ns], &#39;x&#39;, label=&#39;Simulation&#39;)
plt.plot(ns, [analytical(int(n)) for n in ns], &#39;.&#39;, label=&#39;Analytical&#39;)
plt.plot(ns, [normal(int(n)) for n in ns], &#39;--&#39;, label=&#39;Normal approximation&#39;)

plt.ylabel(&#39;P(burnt or raw face)&#39;)
plt.xlabel(&#39;n, the number of tosses per hour&#39;)
plt.legend()
</code></pre>
<pre><code>&lt;matplotlib.legend.Legend at 0x7fbf09037460&gt;
</code></pre><p><img src="static/tofubes/output_11_1.png" alt="png"  /></p>
<p>A curious pattern emerges! Here is what I notice:</p>
<ol>
<li>The analytical solution (orange dots) continues to track the simulation (blue crosses).</li>
<li>However, they both diverge from the normal approximation (which is perhaps as we expect at low $n$).</li>
<li>This divergence is systematic. The analytical probabilities form an interesting “banded” structure.</li>
<li>Most surprisingly, sometimes increasing $n$ <em>increases</em> the number of bad faces!</li>
<li>To do better than $n=6$, you have to go all the way out to $n\approx 500$.</li>
</ol>
<p>The puzzle is, <strong>Why does more stirring cause worse cooking?</strong></p>
<p>Let’s consider the cases $n=6$ and $n=12$. For $n=6$, we already reasoned that a face is well-cooked if it is exposed to heat for only one of the 6 turns. This has probability $\binom{6}{1}(1/6)^1(6/5)^4 \approx 0.402$, and indeed the graph above shows that analytically, we expect a $0.6$ probability of a face getting burnt or being raw at $n=6$.</p>
<p>Now consider $n=12$. Now, a face is definitely well-cooked if it gets 2 turns on the heat, because $2/12 = 1/6 = \mu$. But what about 1 turn or 3 turns? In each case, it is off by a factor of $(1/12)/(1/6) = 0.5 \geq \delta$, so the face would either be raw or get burnt. Hence, the probability of a well-cooked face is given by $\binom{12}{2}(1/6)^2(5/6)^{10} \approx 0.296$, which yields a probability of $0.704$ that the face is raw or burnt. Higher than $0.6$ at $n=6$!</p>
<p>Another way to see what’s going on is to consider 2-faced pancakes instead of 6-faced tofu cubes. If you cook a pancake with $n=2$ random flips, you have a good chance that you will cook both sides (one on each flip), though of course there is some chance you burn one side by cooking it twice, and leave one side uncooked. But if you cook a pancake with $n=3$ flips, you will necessarily always burn one side and leave the other side uncooked, because at best you will get a 1/3-2/3 ratio of cooking time.</p>
<p>Returning to cubes now, let’s see what happens more generally. Say a face gets $X$ turns on the heat where $X \sim \text{Binom}(n, 1/6)$. The probability of being well-cooked is $\Pr[|X/n-\mu| \leq (1+\delta)\mu]$ where $\mu=1/6$. Breaking this up, we can sum over possible outcomes of $X$, to have $\sum_{(1-\delta)(1/6) \leq x/n \leq (1+\delta)(1/6)} \Pr[X=x]$. In other words, we are summing over integer multiples of $1/n$ in the range $[(1-\delta)(1/6), (1+\delta)(1/6))]$. How many integer multiples are there in that range? This is easily given by $\lceil (1/6)(1+\delta)/(1/n) \rceil - \lfloor (1/6)(1-\delta)/(1/n) \rfloor - 1$. We can plot this against the analytical probability to get some insight.</p>
<pre><code class="lang-python">ns = np.arange(1, 100)
plt.plot(ns, [analytical(int(n)) for n in ns], &#39;.&#39;, label=&#39;Analytical&#39;)
plt.plot(
    ns,
    (np.ceil(ns * 1/6 * (1 + tol)) -
     np.floor(ns * 1/6 * (1 - tol)) - 1),
    label=&#39;Number of integer multiples of 1/n in range&#39;
)
plt.xlabel(&#39;n, the number of tosses per hour&#39;)
plt.legend()
</code></pre>
<pre><code>&lt;matplotlib.legend.Legend at 0x7fc0105f1150&gt;
</code></pre><p><img src="static/tofubes/output_13_1.png" alt="png"  /></p>
<p>The spikes in the number-of-integers graph correspond directly to the jumps between bands. This suggests the following explanation of what’s going on: that there are two “forces” at play.</p>
<ol>
<li>The number of integer multiples of $1/n$ that could be matched within the range allowed by the tolerance, which jumps around in a quantized manner according to number theory.</li>
<li>The sum of probabilities of matching each of those integer multiples, which decreases as we add more flips, because there are more possible other-outcomes.</li>
</ol>
<p>When the first “force” spikes, it causes the probability of a bad face to drop suddenly because there are more ways to cook a good face. However, the second “force” causes the probability of a bad face to rise gradually because each way to make a good face becomes less likely. This explains the banding structure in the graph above.</p>
<p>To summarize, in the limit, continuous stirring indeed helps cook your tofu more evenly. However, if you are stirring only occasionally, then sometimes more stirring can actually harm your tofu!</p>
]]></description>
            <link>https://hardmath123.github.io/tofubes.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/tofubes.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 06 Jun 2023 04:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Light from shadow, seeing from seeing]]></title>
            <description><![CDATA[<p>Thoughts on a blue and yellow photograph I took this summer</p>


<p>This summer I was briefly in Vancouver, and after dinner on my last day I found
myself walking several blocks back to my hotel. The night was dark but the
sidewalk was brightly lit and it was a lovely journey. Along the way, I took
this picture:</p>
<p><img src="static/vancouver-streetlight.jpg" alt="A streetlight casts two shadows, blue and
yellow"></p>
<p>What struck me is how the streetlight casts two shadows, blue and yellow, and
moreover those shadows appear opposite the yellow and blue lamps, respectively.
What could possibly be going on?</p>
<p>Here is an explanation: the two lamps together create a kind of grayish-white
light that bathes the sidewalk. Where the yellow lamp is occluded, the blue
light is dominant, so the shadow is blue. Similarly, where the blue lamp is
occluded, the yellow light is dominant, so the shadow is yellow.</p>
<p>Looking at this scene I’m reminded of painter Wayne Thiebaud’s rich, saturated
shadows. You could say the perceptual effect here demonstrates that “white
light” is the sum of all wavelengths, a fact we learn in grade school (I think
there is an exhibit at the SF Exploratorium with a similar concept). But to me,
this also demonstrates the range of what we are willing to call “white.” If one
of the lamps were to burn out, our eyes would adjust to the blue or yellow
almost immediately, and we would still see the sidewalk as gray — we
experience a truly remarkable “color constancy” across lighting conditions. In
this way, when he paints a shadow as a saturated, non-gray color, Thiebaud sees
beyond his own seeing.</p>
]]></description>
            <link>https://hardmath123.github.io/thiebaud.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/thiebaud.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 26 Sep 2022 04:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[PeLU: Porcelain-Emulated Linear Unit]]></title>
            <description><![CDATA[<p>A low-power deep learning inference mechanism inspired by flush toilets</p>


<p>The other day my toilet broke and I involuntarily learned a lot about how flushing works. My friend suggested an analogy for me: flushing toilets is like a neuron’s activation: once a critical threshold is met, there’s an “all-at-once” response.</p>
<p>That got me thinking, could we implement deep neural networks in plumbing? It turns out, the answer is yes! A very simplified model of a flush toilet’s nonlinear behavior is as follows: it’s a bucket, into which water can be poured, and there is a hole at height $h \geq 0$. If you pour in volume $v$ of water into the bucket, the output that flows out of the hole is $\text{ReLU}(v - h)$.</p>
<p>The second component we need to build a neural network is a linear map. We can do this by attaching a branching pipe to the hole. This component will have $k$ branches with cross-sectional areas $A_1, A_2, \dots, A_k &gt; 0$. By conservation of mass and a simple pressure argument, the amount of water that pours out of branch $i$ is $A_i / \Sigma_j A_j$.</p>
<p>Together, these components allow us to compute a function from $\mathbb{R}\rightarrow \mathbb{R}^k$, which looks something like $\text{PeLU}(v, \vec{A}, h) = \text{ReLU}(v - h)\cdot \vec{A} / \Sigma_j A_j$. Here, “PeLU” stands for “Porcelain-Emulated Linear Unit.” It is clear how to vectorize this expression over $v$, which effectively creates a new kind of neural network “layer” with trainable parameters $\vec{A}$ and $h$ for each input dimension. To enforce the positivity constraint on $h$ and $A_i$, we will actually work with the following key equation: $\text{PeLU}(v, \vec{A}, h) = \boxed{\text{ReLU}(v - h^2) \cdot \text{softmax}(\vec{A})}$.</p>
<p>All that is left to do at this point is to implement this in PyTorch and train it.</p>
<pre><code>import torch

class PeLU(torch.nn.Module):
    def __init__(self, in_feat, out_feat):
        super().__init__()
        self.heights = torch.nn.Parameter(
            torch.randn(in_feat))
        self.weights = torch.nn.Parameter(
            torch.randn(in_feat, out_feat)
        )

    def forward(self, X):
        X = torch.nn.functional.relu(X - self.heights ** 2)
        X = X.matmul(self.weights.softmax(dim=1))
        return X
</code></pre><p>Here, I built a PeLU layer that can be slipped into any PyTorch model, mapping <code>in_feat</code> inputs to <code>out_feat</code> outputs. Next, let’s stack some PeLU layers together and train the result on the classic “Iris” dataset, which has 4 features and assigns one of 3 labels. We will create a “hidden layer” of size 3, just to keep things interesting.</p>
<pre><code>from iris import feat, labl

m = torch.nn.Sequential(
    PeLU(4, 3),
    PeLU(3, 3)
)
o = torch.optim.Adam(m.parameters(), lr=0.01)
lf = torch.nn.CrossEntropyLoss()

for i in range(10_000):
    o.zero_grad()
    pred = m(feat * 10)
    loss = lf(pred, labl)
    loss.backward()
    o.step()

print(&#39;Loss:&#39;, loss)
print(&#39;Error:&#39;, 1. - ((torch.argmax(pred, dim=1) == labl) * 1.).mean())
</code></pre><p>This trains very quickly, in seconds, and gives an error of 2%. Of course, we haven’t split the dataset into a train/test set, so may be be overfitting.</p>
<p>By the way, you may have noticed that I multiplied <code>feat</code> by 10 before passing it to the model. Because of the conservation of mass, the total amount of water in the system is constant. But each bucket “loses” some water that accumulates below the hole. To make sure there’s enough water to go around, I boosted the total amount.</p>
<p>But that’s all that needs to be done! Once we export the parameters and write a small visualizer…</p>
<p><img src="static/iris-pelu.gif" alt="GIF of PeLU network inferring an Iris example"></p>
<p>Isn’t that a delight to watch? I may even fabricate one to have as a desk toy — it shouldn’t be hard to make a 3D-printable version of this. If we wanted to minimize the number of pipes, we could add an L1 regularization term that enforces sparsity in the $\vec{A}$ terms.</p>
<p>Some final thoughts: this system has a couple of interesting properties. First, there’s a kind of “quasi-superposition” that it allows for: if you pour in more water on top to incrementally refine your input, the output will automatically update. Second, the “conservation of mass” guarantees that the total water output will never exceed the total water input. Finally, it’s of course entirely passive, powered only by gravity.</p>
<p>This has me wondering if we can build extremely low-power neural network inference devices by optimizing analog systems using gradient descent in this way (a labmate pointed me to <a href="https://arstechnica.com/science/2018/07/neural-network-implemented-with-light-instead-of-electrons/">this</a>, for example).</p>
<p>Below is a little widget you can use to enjoy playing with PeLU networks. All inputs must be between 0 and 10. :)</p>
<p>Sepal length (cm): <input id="sl" type="text" value="6.2"></input><br/>
Sepal width (cm):  <input id="sw" type="text" value="3.4"></input><br/>
Petal length (cm): <input id="pl" type="text" value="5.4"></input><br/>
Petal width (cm):  <input id="pw" type="text" value="2.3"></input><br/></p>
<p><input type="button" id="go" value="Predict!"></input><br/></p>
<canvas id="world" width="500" height="500"></canvas>

<script>
var m = [{'h': [2.0516297817230225, 2.18482890761347e-30, 4.2594499588012695, 2.937817096710205], 'w': [[0.08244021981954575, 0.35453349351882935, 0.5630263090133667], [0.783989667892456, 0.0022806653287261724, 0.2137296199798584], [0.0007646767771802843, 0.8042643070220947, 0.19497093558311462], [0.0004950931761413813, 0.9982838034629822, 0.0012211321154609323]]}, {'h': [2.2704419876575757e-26, 33.44374465942383, 12.223723411560059], 'w': [[0.9706816077232361, 0.021526599302887917, 0.007791891228407621], [0.0013629612512886524, 0.022002533078193665, 0.9766345620155334], [0.018276285380125046, 0.9780184626579285, 0.0037053129635751247]]}];

function Chamber(x, y, f, h, w, p) {
  this.x = x;
  this.y = y;
  this.f = f;

  this.h = h;
  this.w = w;
  this.p = p;

  this.l = null;
}

Chamber.prototype.flow = function(dl) {
  if (this.f <= this.h) return;

  dl = Math.max(0.1 * this.f - this.h, dl);
  for (var j = 0; j < this.p.length; j++) {
    this.p[j].f += dl * this.w[j];
  }
  this.f -= dl;
};

Chamber.prototype.draw = function(ctx) {
  var height = 100;
  ctx.save();
  ctx.translate(this.x, this.y);

  if (this.l !== null) {
    ctx.save();
    ctx.font = '8pt Helvetica';
    ctx.translate(0, height);
    ctx.rotate(-Math.PI / 2);
    ctx.fillText(this.l, 0, 5);
    ctx.restore();
  }

  ctx.fillStyle = '#acf';
  ctx.fillRect(10, height - this.f, 30, this.f);

  ctx.beginPath();
  ctx.arc(0, 10, 10, -Math.PI / 2, 0);
  ctx.lineTo(10, height);
  ctx.lineTo(40, height);
  ctx.lineTo(40, 10);
  ctx.arc(50, 10, 10, Math.PI, -Math.PI / 2);
  ctx.stroke();

  if (this.h !== null) {
    ctx.beginPath();
    ctx.arc(35, height - this.h, 5, 0, Math.PI * 2, true);
    ctx.stroke();
  }
  ctx.restore();

  if (this.h !== null) {
    for (var j = 0; j < this.p.length; j++) {
      ctx.save();
      ctx.beginPath();
      ctx.moveTo(this.x + 35, this.y + height - this.h);
      ctx.bezierCurveTo(
        this.x + 35, this.y + height - this.h + 20,
        this.p[j].x + 25, this.p[j].y - 20,
        this.p[j].x + 25, this.p[j].y
      );
      ctx.strokeStyle = 'black';
      ctx.lineWidth = this.w[j] * 10;
      ctx.stroke();
      ctx.lineWidth = ctx.lineWidth * 0.8;
      ctx.strokeStyle = this.f > this.h ? '#acf' : 'white';
      ctx.stroke();

      if (this.f > this.h) {
        ctx.strokeStyle = '#acf';
        ctx.beginPath();
        ctx.moveTo(this.p[j].x + 25, this.p[j].y);
        ctx.lineTo(this.p[j].x + 25, this.p[j].y + 100);
        ctx.stroke();
      }
      ctx.restore();
    }
  }

  if (this.h === null) {
    var best = true;
    for (var j = 0; j < cs[cs.length - 1].length; j++) {
      if (this.f < cs[cs.length - 1][j].f) best = false;
    }
    if (best) {
      ctx.save();
      ctx.fillStyle = 'rgba(255, 255, 0, 0.2)';
      ctx.fillRect(this.x - 10, this.y - 10, 50 + 20, height + 20);
      ctx.restore();
    }
  }
};

var cs = [];
for (var i = 0; i < m.length; i++) {
  cs.push([]);
  for (var j = 0; j < m[i].h.length; j++) {
    var c = new Chamber(30 + 20 * i + 80 * j, 20 + 150 * i, 0, m[i].h[j], m[i].w[j], []);
    cs[cs.length - 1].push(c);
  }
}

cs.push([]);
for (var j = 0; j < cs[cs.length - 2][0].w.length; j++) {
  var c = new Chamber(30 + 20 * i + 80 * j, 20 + 150 * i, 0, null, [], []);
  cs[cs.length - 1].push(c);
}

for (var i = 0; i < m.length; i++) {
  for (var j = 0; j < cs[i].length; j++) {
    cs[i][j].p = cs[i + 1];
  }
}

cs[0][0].l = 'sepal length (cm)';
cs[0][1].l = 'sepal width (cm)';
cs[0][2].l = 'petal length (cm)';
cs[0][3].l = 'petal width (cm)';
cs[2][0].l = 'P(iris setosa)';
cs[2][1].l = 'P(iris versicolour)';
cs[2][2].l = 'P(iris virginica)';

cs[0][0].f = 62;
cs[0][1].f = 34;
cs[0][2].f = 54;
cs[0][3].f = 23;

function frame() {
  var world = document.getElementById('world');
  world.width = world.width;
  var ctx = world.getContext('2d');
  for (var i = 0; i < cs.length; i++) {
    for (var j = 0; j < cs[i].length; j++) {
      cs[i][j].draw(ctx);
    }
  }
  for (var i = 0; i < cs.length - 1; i++) {
    for (var j = 0; j < cs[i].length; j++) {
      cs[i][j].flow(0.1);
    }
  }

  window.requestAnimationFrame(frame);
}

window.addEventListener('load', function() {
  var sl = document.getElementById('sl');
  var sw = document.getElementById('sw');
  var pl = document.getElementById('pl');
  var pw = document.getElementById('pw');
  var go = document.getElementById('go');
  go.addEventListener('click', function() {
    for (var i = 0; i < cs.length; i++) {
      for (var j = 0; j < cs[i].length; j++) {
        cs[i][j].f = 0.;
      }
    }
    cs[0][0].f = Math.max(0., Math.min(100., (parseFloat(sl.value) || 0) * 10));
    cs[0][1].f = Math.max(0., Math.min(100., (parseFloat(sw.value) || 0) * 10));
    cs[0][2].f = Math.max(0., Math.min(100., (parseFloat(pl.value) || 0) * 10));
    cs[0][3].f = Math.max(0., Math.min(100., (parseFloat(pw.value) || 0) * 10));
  });
  frame();
});
</script>]]></description>
            <link>https://hardmath123.github.io/pelu.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/pelu.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Sat, 04 Dec 2021 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Birds and the Representation of Representation]]></title>
            <description><![CDATA[<p>What is it about birds?</p>


<h2 id="toni-morrison-nobel-lecture">Toni Morrison, Nobel Lecture</h2>
<blockquote>
<p>Speculation on what (other than its own frail body) that bird-in-the-hand might signify has always been attractive to me, but especially so now thinking, as I have been, about the work I do that has brought me to this company. <a href="https://www.nobelprize.org/prizes/literature/1993/morrison/lecture/">(full)</a></p>
</blockquote>
<h2 id="richard-siken-the-language-of-the-birds-">Richard Siken, “The Language of the Birds”</h2>
<blockquote>
<p>And just because you want to paint a bird, do actually paint a bird, it doesn’t mean you’ve accomplished anything. <a href="https://poets.org/poem/language-birds">(full)</a></p>
</blockquote>
<h2 id="cross-examination-in-_brancusi-v-united-states_">Cross-examination in <em>Brancusi v. United States</em></h2>
<blockquote>
<p><strong>Waite:</strong> What do you call this?<br><strong>Steichen:</strong> I use the same term the sculptor did, oiseau, a bird.<br><strong>Waite:</strong> What makes you call it a bird, does it look like a bird to you?<br><strong>Steichen:</strong> It does not look like a bird but I feel that it is a bird, it is 
characterized by the artist as a bird.<br><strong>Waite:</strong> Simply because he called it a bird does that make it a bird to you?<br><strong>Steichen:</strong> Yes, your honor.<br><strong>Waite:</strong> If you would see it on the street you never would think of 
calling it a bird, would you?<br>[<strong>Steichen:</strong> Silence]<br><strong>Young:</strong> If you saw it in the forest you would not take a shot at it?<br><strong>Steichen:</strong> No, your honor. <a href="https://www.legalaffairs.org/issues/September-October-2002/story_giry_sepoct2002.msp">(more)</a></p>
</blockquote>
<h2 id="donna-tartt-_the-goldfinch_">Donna Tartt, <em>The Goldfinch</em></h2>
<blockquote>
<p>But who knows what Fabritius intended? There’s not enough of his work left to even make a guess. The bird looks out at us. It’s not idealized or humanized. It’s very much a bird.</p>
</blockquote>
<h2 id="adam-savage-my-obsession-with-objects-and-the-stories-they-tell-">Adam Savage, “My obsession with objects and the stories they tell”</h2>
<blockquote>
<p>And then there is this fourth level, which is a whole new object in the world: the prop made for the movie, the representative of the thing, becomes, in its own right, a whole other thing, a whole new object of desire. . . . There are several people who own originals, and I have been attempting to contact them and reach them, hoping that they will let me spend a few minutes in the presence of one of the real birds, maybe to take a picture, or even to pull out the hand-held laser scanner that I happen to own that fits inside a cereal box, and could maybe, without even touching their bird, I swear, get a perfect 3D scan. And I’m even willing to sign pages saying that I’ll never let anyone else have it, except for me in my office, I promise. I’ll give them one if they want it. And then, maybe, then I’ll achieve the end of this exercise. But really, if we’re all going to be honest with ourselves, I have to admit that achieving the end of the exercise was never the point of the exercise to begin with, was it? <a href="https://www.youtube.com/watch?v=29SopXQfc_s">(full)</a></p>
</blockquote>
<h2 id="michael-shewmaker-the-curlew-">Michael Shewmaker, “The Curlew”</h2>
<blockquote>
<blockquote>
<p>Plate 357 <em>(Numenius Borealis)</em> is the only instance in which the subject appears dead in the work of John James Audubon.</p>
</blockquote>
<p>He waits alone, sketching angels from the shade—<em>a kind of heavenly bird,</em> he reasons with himself—although their wings are broke, faces scarred, each fragile mouth feigning the same sad smile as the one before it. Offered triple his price to paint a likeness of the pastor’s daughter—buried for more than a week—he reluctantly agreed—times being what they are.<br>. . . . .<br>And yet he studies it—from behind the dunes—studies its several postures, grounded and in sudden flight—and not content to praise it from a distance, to sacrifice detail, unpacks his brushes and arranges them before raising his rifle and taking aim.</p>
</blockquote>
<hr>
<h2 id="richard-hunt-s-quest-for-notation-for-birdsong">Richard Hunt’s quest for notation for birdsong</h2>
<p>(Added April 18, 2023)</p>
<blockquote>
<p>Zizzy, uncanny, pebble-tapping, lusty, pule. Ventriloquial, tantara,
feminine, crepitate. Tintinnabulation, sough, devil’s tattoo. Sparrowy.</p>
</blockquote>
<p>(<a href="https://daily.jstor.org/what-it-sounds-like-when-doves-cry/">See the article for
more…</a>)</p>
<h2 id="children-imitating-cormorants">Children imitating cormorants</h2>
<p>(Added June 2023)</p>
<p>By <a href="https://hellopoetry.com/poem/15275/children-imitating-cormorants/">Kobayashi
Issa</a>;
mentioned by a professor on a walk by the Charles River.</p>
<blockquote>
<p>Children imitating cormorants<br>are even more wonderful<br>than cormorants.</p>
</blockquote>
<h2 id="fake-birds-in-disneyland">Fake birds in Disneyland</h2>
<p>(Added June 2023)</p>
<p>From Philip K. Dick’s speech, <a href="https://urbigenous.net/library/how_to_build.html">“How to build a universe.”</a></p>
<blockquote>
<p>In my writing I got so interested in fakes that I finally came up with the
concept of fake fakes. For example, in Disneyland there are fake birds worked
by electric motors which emit caws and shrieks as you pass by them. Suppose
some night all of us sneaked into the park with real birds and substituted
them for the artificial ones. Imagine the horror the Disneyland officials
would feel when they discovered the cruel hoax. Real birds! And perhaps
someday even real hippos and lions. Consternation.</p>
</blockquote>
<h2 id="a-representation-of-an-idea">A representation of an idea</h2>
<p>(Added June 2023)</p>
<p>From Eric Kraft’s <em>Where do we stop?</em></p>
<blockquote>
<p>The idea that seemed so bright when it was leaping and darting and fluttering through my mind looked dull and dead when I’d caught it and pinned it to my paper … It wasn’t an idea now, but the representation of an idea. It didn’t fly, didn’t flutter by, didn’t catch the eye as I thought it would. (154)</p>
</blockquote>
<h2 id="symbolic-of-my-entire-existence">Symbolic of my entire existence</h2>
<p>(Added June 2023)</p>
<p>From CAKE’s Mr. Mastodon Farm</p>
<blockquote>
<p>Now due to a construct in my mind<br>That makes their falling and their flight<br>Symbolic of my entire existence<br>It becomes important for me<br>To get up and see<br>Their last-second curves toward flight</p>
<p>It’s almost as if my life would fall<br>Unless I see their ascent.</p>
</blockquote>
<p>See also: the poem “<a href="https://poets.org/poem/because-you-asked-about-line-between-prose-and-poetry">Because You Asked about the Line Between Prose and
Poetry</a>“
by Howard Nemerov.</p>
<blockquote>
<p>Sparrows were feeding in a freezing drizzle<br>That while you watched turned to pieces of snow<br>Riding a gradient invisible<br>From silver aslant to random, white, and slow.</p>
<p>There came a moment that you couldn’t tell.<br>And then they clearly flew instead of fell.</p>
</blockquote>
<h2 id="why-illustrate-">Why illustrate?</h2>
<p>From an
<a href="https://www.nytimes.com/2023/07/06/science/birding-illustration-sibley.html">interview</a>
with illustrator David Sibley.</p>
<blockquote>
<p>An illustration provides so much more than a photograph. In an illustration,
I can create a typical bird, an average bird of a species in the exact pose
that I want, and create an image of a similar species in exactly the same
pose so that all the differences are apparent… Your drawing becomes a
record of your understanding of that bird in that moment.</p>
</blockquote>
<h2 id="unseeable-birds">Unseeable birds</h2>
<p>(See Edgerton’s photograph of Mrs. May Rogers Webster with her hummingbirds.)</p>
<h2 id="my-crow">My Crow</h2>
<p>A poem by Raymond Carver</p>
<blockquote>
<p>A crow flew into the tree outside my window.<br>It was not Ted Hughes’s crow, or Galway’s crow.<br>Or Frost’s, Pasternak’s, or Lorca’s crow.<br>Or one of Homer’s crows, stuffed with gore,<br>after the battle. This was just a crow.<br>That never fit in anywhere in its life,<br>or did anything worth mentioning.<br>It sat there on the branch for a few minutes.<br>Then picked up and flew beautifully<br>out of my life.  </p>
</blockquote>
]]></description>
            <link>https://hardmath123.github.io/birds-and-the-representation-of-representation.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/birds-and-the-representation-of-representation.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Thu, 21 Oct 2021 04:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Carnival of Mathematics 198]]></title>
            <description><![CDATA[<p>The Carnival revisits an old home</p>


<p>Hello and welcome to the 198th <a href="https://aperiodical.com/carnival-of-mathematics/">Carnival of Mathematics</a>, a roving monthly roundup of mathy blog posts from around the Internet! Longtime Comfortably Numbered readers should be no stranger to the Carnival: I hosted <a href="carnival-of-mathematics-134.html">#134</a>, <a href="carnival-of-mathematics-148.html">#148</a>, and <a href="carnival-of-mathematics-159.html">#159</a> in the past.</p>
<p>As is traditional, I want to start by thinking a bit about the number 198. It is actually a very dear number to me — I even own a “198” t-shirt! Why, you ask? The number 198 is the emblem of the wonderful CS198 program at Stanford, which hires a huge team of undergraduates to help teach the introductory computer science courses every year. (As far back as 1995, the faculty running the program published a <a href="https://cormack.uwaterloo.ca/papers/p48-roberts.pdf">retrospective</a> on its impact on campus.)</p>
<p>Here is another fun fact about the number 198: suppose I start looking at all the integers ($1, 2, 3, 4, 5, \dots$), keeping only the ones that are perfect powers ($1, 4=2^2, 8=2^3, 9=3^2, 16=2^4, \dots$), and then taking their cumulative averages ($1$, $2.5=(1+4)/2$, $4.33=(1+4+8)/3$, $5.5=(1+4+8+9)/4$, $7.6=(1+4+8+9+16)/5, \dots$). You might wonder, are there any other integers in <a href="http://oeis.org/A075457">this sequence</a>? It turns out, <em>yes!</em> And I bet you can guess the first one…</p>
<hr>
<p>Now, time for some links! I’ve provided some “teaser” text from each submission to get you interested.</p>
<p>The BRSR blog asks: <a href="https://brsr.github.io/2021/05/29/curved-triangle.html">can you find a Euclidean triangle on a non-Euclidean surface?</a></p>
<blockquote>
<p>A question that came up in a math chatroom (yes, I’m the kind of nerd who spends time in math chatrooms): find a “Euclidean” triangle on a non-Euclidean surface. More exactly, find a geodesic triangle on a surface with non-constant <a href="https://en.wikipedia.org/wiki/Gaussian_curvature">Gaussian curvature</a> having the same sum of interior angles as an Euclidean triangle.</p>
</blockquote>
<p>On his blog bit-player, Brian Hayes asks: <a href="http://bit-player.org/2021/riding-the-covid-coaster">why do pandemics peak in distinct waves as the months go by?</a></p>
<blockquote>
<p>I’m puzzled by all this structural embellishment. Is it mere noise—a product of random fluctuations—or is there some driving mechanism we ought to know about, some switch or dial that’s turning the infection process on and off every few months?</p>
<p>I have a few ideas about possible explanations, but I’m not so keen on any of them that I would try to persuade you they’re correct. However, I <em>do</em> hope to persuade you there’s something here that needs explaining.</p>
</blockquote>
<p>On Twitter, Thien An asks: <a href="https://twitter.com/thienan496/status/1434522808189521930">would you rather have a bishop and a knight, or two bishops?</a></p>
<blockquote>
<p>To get this figure, it suffices to grab 100k games and only keep the positions with BB+pawns v.s. BN+pawns and see who eventually wins. That’s very simplistic since there are indeed many other important positional features, but I was actually expecting less convincing results…</p>
</blockquote>
<p>Looking at a video of a pomegranate sorting mechanism, Nisar Khan asks: <a href="https://nisarkhanatwork.medium.com/pomegranate-size-differences-in-different-boxes-ff8a627bb82">how much do the pomegranate sizes vary between sorted buckets?</a></p>
<blockquote>
<p>After watching the below video, thought of finding the approximate size differences of pomegranates sorted in different boxes…</p>
</blockquote>
<p>On his math blog, Tony asks: <a href="http://tonysmaths.blogspot.com/2021/09/did-studying-maths-help-emma-raducanu.html?m=1">did studying maths help Emma Raducanu win the US Open?</a></p>
<blockquote>
<p>I’ve discussed toy examples in public lectures on game theory, which (it seems to me) is relevant to choices players make - whether to serve to the forehand or backhand, and where to expect for your opponent to serve, for example. I very much doubt if players ever analyse in these terms, but they are intuitively doing game theory when making their tactical decisions.</p>
<p>But I think in the case of Raducanu there is a more general point…</p>
</blockquote>
<p>On Risen Crypto, Trajesh asks: <a href="https://risencrypto.github.io/QuadraticSieve/">how does the quadratic sieve factoring algorithm really work?</a></p>
<blockquote>
<p>The Quadratic Sieve is the second fastest algorithm for factoring large semiprimes. It’s the fastest for factoring ones which are lesser than 100 digits long.</p>
</blockquote>
<p>On The Universe of Discourse, Mark Dominus asks: <a href="https://blog.plover.com/2021/08/28/#combinator-s">why is the “S” combinator named “S”?</a> He also posted a <a href="https://blog.plover.com/math/dilworth.html">nice puzzle</a> that is easily solved if you happen to know Dilworth’s theorem; separately, Jim Fowler built a <a href="https://twitter.com/kisonecat/status/1435265678525620227">musical PICO-8 game</a> where you race to find the chains guaranteed by Dilworth’s theorem.</p>
<blockquote>
<p>I thought about this for a while but couldn’t make any progress. But OP had said “I know I have to construct a partially ordered set and possibly use Dilworth’s Theorem…” so I looked up Dilworth’s theorem.</p>
</blockquote>
<p>On their blog “Gödel’s Last Letter and P=NP,” Ken Regan and Dick Lipton discuss <a href="https://rjlipton.wpcomstaging.com/2021/09/30/baby-steps/">“baby steps”</a> in math, in the context of some exciting recent results.</p>
<blockquote>
<p>Reckoned against a later <a href="https://annals.math.princeton.edu/2009/170-3/p01">paper</a> by Cohn and Elkies, [Viazovska’s] improvement was 0.0000…000001. The recent post where we discussed the phrase “the proof is in the pudding” involves a number with six more zeroes than that. These are <strong>not</strong> what we mean by “baby steps.”</p>
</blockquote>
<p>And finally, something more about sequences— on her math blog, Tanya Khovanova explores the <a href="https://blog.tanyakhovanova.com/2021/09/the-top-fifty-largest-numbers-that-start-a-sequence-in-the-oeis/">top 50 largest numbers to start OEIS sequences</a>.</p>
<blockquote>
<p>My son, Alexey Radul, wrote a program that finds the largest numbers to start a sequence in the Online Encyclopedia of Integer Sequences (<a href="https://oeis.org/">OEIS</a>). To my surprise, the top ten are all numbers consisting of ones only.</p>
</blockquote>
<hr>
<p>That’s all for this month’s Carnival! What a joy to see — once more — the math blog world <em>overflowing</em> with curiosity. Hosting the Carnival of Mathematics truly is a delight.</p>
<p>Want even more math? The previous Carnival, the 197th, was hosted by <a href="https://jeremykun.com/2021/09/01/carnival-of-mathematics-197/">Jeremy at Math $\cap$ Programming</a>, and the next — the 199th, to which you can contribute <a href="https://aperiodical.com/carnival-of-mathematics/">here</a>, starting now! — will be hosted by <a href="https://doubleroot.in">Vaibhav at DoubleRoot</a>. See you there!</p>
]]></description>
            <link>https://hardmath123.github.io/carnival-of-mathematics-198.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/carnival-of-mathematics-198.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 01 Oct 2021 04:00:00 GMT</pubDate>
        </item>
    </channel>
</rss>