<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Comfortably Numbered]]></title>
        <description><![CDATA[My blog.]]></description>
        <link>http://hardmath123.github.io</link>
        <image>
            <url>http://hardmath123.github.io/static/avatar.png</url>
            <title>Comfortably Numbered</title>
            <link>http://hardmath123.github.io</link>
        </image>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sat, 07 Sep 2019 19:28:59 GMT</lastBuildDate>
        <atom:link href="http://hardmath123.github.io/feed.xml" rel="self" type="application/rss+xml"/>
        <author><![CDATA[Hardmath123]]></author>
        <language><![CDATA[en]]></language>
        <item>
            <title><![CDATA[Unraveling the Un-unravelable]]></title>
            <description><![CDATA[<p>And some observations about the associativity of English affixes</p>


<p>Say what you will about <em>The Big Bang Theory</em>, it has its moments, and this is
one of my favorites (watch it to the end of the clip!):</p>
<iframe width="560" height="315"
src="https://www.youtube-nocookie.com/embed/MsawieizDeE?start=210"
frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope;
picture-in-picture" allowfullscreen></iframe>

<p>Putting aside the wonderful meta-dramatic irony of this moment, the word
“un-unravelable” is the kind of thing you can sit and think about for hours. I
know because I have.</p>
<p>See, you might think that the “un-“ prefixes cancel, such that “un-unravelable”
means the same thing as “ravelable.” This is kind of like how
“un-unforgettable” probably means “ordinary.” And indeed, <a href="https://en.wiktionary.org/wiki/un-unravelable">Wiktionary
agrees</a> with this assessment,
suggesting instead “ravelable” as the “simpler and more immediately logical
choice.”</p>
<p>But clearly that’s not what Dr. Cooper meant in this case! “Ravelable” means
<em>can be raveled</em> whereas Dr. Cooper meant <em>cannot be unraveled</em>, which are at
least intuitively separate concepts. So what’s going on?</p>
<p>First of all, we need to figure out what “ravel” means: it
<a href="https://www.websters1913.com/words/Ravel">means</a> to entangle. Something that
is “raveled” is entangled and knotted like the HDMI cords behind your cable
box. The sleep that Macbeth murders is the “sleep that knits up the raveled
sleeve of care” (2.2.37). Actually, that’s not the full story. “Ravel” <em>also</em>
means to <em>disentangle</em>, making the word a so-called “Janus word” (read more at
<a href="https://www.merriam-webster.com/words-at-play/words-own-opposites">Merriam-Webster</a>).
But let’s stick with the former definition for the moment.</p>
<p>Okay, so now we can start tacking on prefixes and suffixes. But we have to be
careful!  The dynamics of affixes are tricky — the order in which we tack
them on matters.</p>
<p>For example, “ravel-able” would mean <em>can be entangled,</em> so spaghetti is
ravelable. Then “un-ravelable” could mean <em>cannot be entangled,</em> so ravioli is
unravelable.</p>
<p>On the other hand, “un-ravel” would mean <em>to disentangle,</em> so Sherlock unravels
mysteries. Then “unravel-able” could mean <em>disentanglable,</em> so the mysteries
Sherlock solves are unravelable (by him, at least).</p>
<p>To summarize:</p>
<ul>
<li>un+(ravel+able): there is <em>no</em> way to ravel it</li>
<li>(un+ravel)+able: there <em>is</em> a way to unravel it</li>
</ul>
<p>Alas! The associative property has broken down! This means we need to examine
each of the five possible paranthesisifications of “un+un+ravel+able”
separately (<a href="https://en.wikipedia.org/wiki/Catalan_number">why five?</a>).</p>
<p>Here they are:</p>
<ol>
<li><strong>(un+un)+(ravel+able)</strong> and</li>
<li><strong>((un+un)+ravel)+able</strong> are easy: neither of them typecheck, because
“un+un” is meaningless.</li>
<li><strong>un+(un+(ravel+able)):</strong> “ravelable” means <em>can be entangled,</em> so
“un-ravelable” means <em>cannot be entangled,</em> and thus “un-unravelable”
means <em>is not such that it cannot be entangled,</em> i.e. <em>can be
entangled.</em> For example, your new favorite organizational scheme might seem
perfect at first, but after a few weeks you will find that no scheme can
fend off the entropic tendencies of the universe, and that your files are by
nature un-un-ravelable.</li>
<li><strong>(un+(un+ravel))+able:</strong> “unravel” means <em>disentangle,</em> so “un-unravel”
means <em>re-entangle</em> (by analogy to “un-undo” meaning “redo”) and thus
“ununravel-able” means <em>re-entanglable.</em> For example, a jigsaw puzzle that
you mix up again after solving is ununravel-able.</li>
<li><strong>un+((un+ravel)+able):</strong> “unravel” means <em>disentangle,</em> so “unravel-able”
means <em>can be disentangled,</em> and thus “un-unravelable” means <em>cannot be
disentangled.</em> For example, Dr. Cooper’s web of lies is un-unravelable.</li>
</ol>
<p>So what’s really going on here is that the prefix <em>un-</em> has many faces (a Janus
prefix?); it reverses verbs and negates adjectives. Perhaps this is clearer in
the language of first-order logic, where <em>-able</em> introduces an existential
quantifier over <em>a way to do the thing</em> (and thus do(w, x) does w to x).</p>
<ol>
<li>N/A</li>
<li>N/A</li>
<li><strong>(un+(un+(ravel+able)))(x)</strong> means &not;&not;&exist;w. raveled(do(w, x))</li>
<li><strong>((un+(un+ravel))+able)(x)</strong> means &exist;w. &not;&not;raveled(do(w, x))</li>
<li><strong>(un+((un+ravel)+able))(x)</strong> means &not;&exist;w. &not;raveled(do(w, x))</li>
</ol>
<p>Only (5) puts an odd number of negations outside the existential quantifier,
flipping it to a universal quantifier, and expressing that you <em>cannot</em> unravel
whatever needs unraveling.</p>
<p>Why doesn’t this effect happen with “un-unforgettable”? It’s because “unforget”
isn’t a common word, and so we reach for interpretation (3) rather than (5).
Though now that I think about it, “unforget” is an excellent replacement for
“remember.”</p>
]]></description>
            <link>http://hardmath123.github.io/un-unravelable.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/un-unravelable.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 27 Aug 2019 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Words that do Handstands]]></title>
            <description><![CDATA[<p>Dreaming ambigrams by gradient descent</p>


<p>The other day I ordered a book on art history from Amazon… which reminded me
of the movie <em>Angels and Demons</em>, which reminded me of ambigrams. Ambigrams are
those stylized words that look the same when you turn them upside down. Here is
the classic, <a href="http://www.johnlangdon.net/works/angels-demons/">by John
Langdon</a>.</p>
<p><img src="static/ambigrams/langdon.png" alt="earth air fire water"></p>
<p>I’ve been thinking a lot about the absurd power of gradient descent these days,
and so almost immediately I felt the need to try and synthesize ambigrams
on-demand by gradient descent.</p>
<p>It turns out that this isn’t too hard to implement. Let’s start with just
single-character ambigrams. Here’s the plan: first, we train some sort of
neural net to recognize characters. This training is by gradient descent on the
weights / convolution filter / whatnot; specifically, we imagine a function of
the weights that outputs some loss and take its derivative <em>with respect to the
weights</em> to do gradient descent. Once the neural net is trained, however, we
can just treat it as a single function from tensors (input image) to tensors (a
one-hot encoding of the classification… or some softmaxed approximation).
Taking that one-hot output and converting it into a cross-entropy loss against
some desired output, we can now do gradient descent to minimize this loss <em>with
respect to the input image</em> (keeping the weights constant). This process lets
us “dream” of images representing whichever characters we want.</p>
<p>Similarly, we can do the same thing but with the tensor turned upside-down, to
dream images of upside-down characters. Okay, so now we can just combine these
two processes — literally, by adding the loss functions — to dream an image
that matches one character right-side-up and another character upside-down.
That’s it! Now we let it run all day to generate an image for each alphanumeric
character pair (36^2 = 1296) and we start to get a nice little “ambigram font”:</p>
<p><img src="static/ambigrams/grid.png" alt="grid"></p>
<p>Combining letters from this “font” lets us make arbitrary ambigrams.</p>
<p>This was a low-effort operation, hacked together over an evening and a morning,
plus a full day of “training.” I’m sure it’s possible to do better. Here are
some technical notes for anyone who wants to have a go at it. My code is all
online, <a href="https://github.com/kach/neural-ambigrams">here</a>.</p>
<ul>
<li>It’s all written in PyTorch. The neural net itself is literally PyTorch’s
built-in <a href="https://github.com/pytorch/examples/tree/master/mnist">MNIST
example</a>.</li>
<li>I used the <a href="https://www.nist.gov/node/1298471/emnist-dataset">EMNIST</a>
dataset, which is MNIST that also adds in the alphabets from the NIST
database (and comes with PyTorch). I used the “balanced” split.</li>
<li>I added an L1 regularizer to force the output images to be sparse, i.e.
mostly “blank paper” with some “ink.” Without this, you get very dark images.</li>
</ul>
<p>Okay, now time for some pictures. Some turned out better than others, and there
is plenty of scope for improvement…</p>
<p>P.S. Depending on your browser, images in this post may or may not flip
upside-down automatically if you hover over them.</p>
<style>
img {
  transform: rotate(0deg);
  transition: transform 0.5s;
}

img:hover {
  transform: rotate(180deg);
  transition: transform 0.5s;
}
</style>

<h2 id="angel-and-demon">ANGEL AND DEMON</h2>
<p><img src="static/ambigrams/angel-and-demon.png" alt="ANGEL AND DEMON"></p>
<h2 id="banana">BANANA</h2>
<p><img src="static/ambigrams/banana.png" alt="BANANA"></p>
<h2 id="year-2016">YEAR, 2016</h2>
<p><img src="static/ambigrams/year-2016.png" alt="YEAR, 2016"></p>
<h2 id="good-evil">GOOD, EVIL</h2>
<p><img src="static/ambigrams/good-evil.png" alt="GOOD, EVIL"></p>
<h2 id="teamwork">TEAMWORK</h2>
<p><img src="static/ambigrams/teamwork.png" alt="TEAMWORK"></p>
<h2 id="blues-skate">BLUES, SKATE</h2>
<p><img src="static/ambigrams/blues-skate.png" alt="BLUES, SKATE"></p>
<h2 id="dream-world">DREAM, WORLD</h2>
<p><img src="static/ambigrams/dream-world.png" alt="DREAM, WORLD"></p>
]]></description>
            <link>http://hardmath123.github.io/ambigrams.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/ambigrams.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 26 Aug 2019 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Buffering the Sock Stream]]></title>
            <description><![CDATA[<p>How much space do I need to do my laundry?</p>


<p>It’s the end of week 4 of the quarter and you’ve decided it’s finally time to
do some laundry. You somehow transport the massive mound of clothing in your
closet to the laundry room, occupy all three machines and also the dishwasher,
and finally transport the now-slightly-less-smelly mound of clothing back up
the stairs to your room. Then you turn on your favorite Pink Floyd album and
begin folding…</p>
<p>You pull out items of clothing from the heap one by one. Shirts and pants you
can fold immediately, but socks pose some difficulty: you can’t put them away
until they have been paired. So you put them aside on your bedside table to be
processed later. Soon, however, the pile of socks on your bedside table grows
too large, and the table can only fit so much sock on it.</p>
<p>So you decide to try and manage the situation by eagerly pairing up socks. If
you pull a sock out of the heap of clothes and see its partner on the bedside
table, you pair them up and put them away. Otherwise, you put the unpaired sock
on the bedside table.</p>
<p>Suppose you own an infinite number of socks, each of which is uniformly one of
($ k $) colors. How big does your bedside table need to be? Let’s call the pile
of clean clothes the “stream,” and the bedside table the “buffer.” Then the
question is, what is the distribution of the buffer size over time as a
function of ($ k $)?</p>
<p>Here is one way to think about this: if ($ k = 1 $) then the buffer size is 0
half the time and 1 half the time; the distribution is normal at mean 0.5 and
variance 0.25. For larger ($ k $), we can think of this as just a superposition
of multiple buffers, one for each ($ k $). The means and variances add, so we
expect the distribution to be normal with mean ($ k/2 $) and standard deviation
($ \sqrt{k}/2 $).</p>
<p>Here is another way to think about this: we have a Markov process where the
transition from state ($ i $) to ($ i + 1 $) is with probability ($ (k - i)/k
$) and to ($ i - 1 $) is with probability ($ i/k $) for ($ 0 \leq i \leq k $).
We can create a matrix representing this transition system; then, eigenvectors
of this matrix (with eigenvalue 1, and normalized to sum 1) represent
equilibrium probability distributions.</p>
<p>So, incredibly, we’ve shown that the coordinates of the ($ \lambda=1 $)
eigenvector for this matrix can be computed using the normal CDF. What a
fascinating correspondence!</p>
<p>If you don’t believe it, the graph below shows the normal-CDF predictions
(orange line) and the elements of the eigenvector (blue dots) for ($ k = 100
$). The IPython notebook that generated this graph is
<a href="static/sock-graph.ipynb">here</a>.</p>
<p><img src="static/sock-graph.png" alt="sock graph"></p>
]]></description>
            <link>http://hardmath123.github.io/sock-buffer.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/sock-buffer.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Sat, 13 Apr 2019 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Windmills on my Mind]]></title>
            <description><![CDATA[<p>Some fast recreational physics</p>


<p>Quick back-of-the-envelope analysis of a windmill, just for fun.</p>
<p>According to <a href="https://www.wind-watch.org/faq-size.php">National Wind Watch</a> a
typical windmill is a GE 1.5-megawatt model which has 116-ft blades.</p>
<p>Let’s say the wind speed is ($ v $), which is typically 10-15 m/s on a good
day. Then in time ($ t $) the volume of air that passes across the windmill is
a cylinder of height ($ tv $) and area ($ \pi r^2$). Air’s density, ($ \rho $),
is around 1.225 kilograms per cubic meter. This gives a total mass of ($ M =
\rho tv\pi r^2 $). Taking ($ v $) to be a modest 10 m/s, this works out to 48
metric <em>tons</em> of air per <em>second</em>.</p>
<p>Now let’s apply conservation of energy on this mass of air. Initial energy is
($ Mv^2 / 2 $) and final energy is ($ Mw^2/2 $), and the difference is on the
order of the energy output, which is a function of the power output and ($ t $)
assuming reasonable efficiency in the turbine.</p>
<p>So, we have ($ \rho tv\pi r^2 v^2 / 2 - \rho tv\pi r^2 w^2 /2 = Pt $). Time
cancels, which is comforting since this is a continuous process. The equation
that’s left suggests that power output of a windmill is proportional to the
<em>cube</em> of the wind speed!</p>
<p>Another thing to think about is how much the wind slows down by. Solving for ($
w $) and taking ($ P $) to be 1.5 MW, we have ($ w = \sqrt{ \frac{2P/\rho \pi
r^2 - v^3 }{-v}} $). For 15 m/s winds, this means wind slows down by around 10%
because of the windmill.</p>
<p>Actually this wasn’t “just for fun,” it was to show off initial progress on my
side-project. :-)</p>
<p><img src="static/windmill-math.png" alt="screenshot"></p>
<p>Oh, and an unresolved question: because air is coming into the windmill faster
than it’s going out, the momentum flux across the plane of the windmill is net
negative.  Applying Gauss’ law, we would expect a buildup of air in the
windmill, which obviously doesn’t happen. What’s wrong?</p>
]]></description>
            <link>http://hardmath123.github.io/windmills.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/windmills.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Wed, 27 Mar 2019 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Caustics and Casinos on the I-5]]></title>
            <description><![CDATA[<p>An analytical analysis of an anomalous advertisement</p>


<p>I was in Los Angeles over winter break, and on the long drive back home I began
thinking about a billboard I saw just at the edge of the city advertising the
<strong>“closest casino to anywhere in LA.”</strong></p>
<p>This is a fascinating claim. Let me rephrase it, at least the way I interpret
it: the claim is that <em>wherever</em> you are in LA, the closest casino is the one
advertised on the billboard. (This is confusingly distinct from the claim that
the casino is closest to <em>anywhere</em> in LA, in the sense that the placement of
the casino minimizes the distance to the nearest bit of LA soil. Of course,
practically speaking this latter claim is useless because any casino <em>within</em>
LA trivially has the minimal distance of zero to “anywhere in LA.”)</p>
<p>The question is this: what region does the billboard’s claim imply is devoid of
casinos?</p>
<p>Let’s start with a simple case to get some intuition. Suppose LA is a
15-mile-radius disk, and the casino is at the center of the disk. Then,
according to the billboard, there must be <em>no</em> other casinos within LA’s
15-mile radius (otherwise, if you were in LA, you <em>might</em> be closer to that
other casino!). But actually, the claim is quite a bit stronger: there cannot
be any casinos within <em>thirty</em> miles of the center. Why? Well, suppose there
was a casino 20 miles from the center of LA. Then, someone just within the city
borders would be 5 miles away from that <em>other</em> casino, but 15 miles from the
city-center casino.</p>
<p>One more simple case: suppose LA is a line segment of length 15 miles, and the
casino is located at one endpoint. Can you imagine what the region in question
must be? It is a disk of radius 15 miles, centered at the <em>other</em> endpoint!
This is not entirely obvious, and you might need to draw a picture to convince
yourself that this is true.</p>
<p>Now let’s consider a much trickier case: suppose LA is a circle again, but the
casino is located along the <em>circumference</em>. Suddenly, it’s <em>much</em> harder to
picture what’s going on — sitting in a car without pencil or paper, I had no
idea what the region might look like. My instinct was “circle centered at the
diametrically-opposite point on the circumference,” but it turns out that this
is <em>wrong!</em></p>
<p>In the rest of this post, we’ll build up some mathematical machinery to answer
this question correctly. If you don’t want to work through the math, however,
then feel free to just scroll to the red-bordered squares and enjoy the
interactive demos.</p>
<hr>
<p><strong>Definition.</strong> Given some region ($ R \subset \mathbb{R}^2 $), the <em>casino
closure</em> with respect to some point ($ p \in \mathbb{R}^2 $) is defined as</p>
<p>\[
R^p = \left\{
  c \in \mathbb{R}^2 ~\middle|~
  \exists z \in R, || p - z || &gt; || c - z ||
\right\}
\]</p>
<p>Or, informally: the casino closure is the set of <em>possible casino locations</em> ($
c $) such that there exists a person ($ z $) in the region ($ R $) who is
closer to ($ c $) than to ($ p $).</p>
<p>From here on out, I’m only going to worry about “nice” regions, i.e. closed,
connected regions with smooth boundaries. I’m also going to use ($ x, p, z, c
$) to range over points in ($ \mathbb{R}^2 $), but really most math that
follows is equally applicable in ($ \mathbb{R}^n $). It’s just harder to
visualize.</p>
<p><strong>Lemma.</strong> Let the disk ($ D(z, r) $) be the set of all points in ($
\mathbb{R}^2 $) within ($ r $) of point ($ z $). Then,
\[
R^p = \bigcup_{z~\in~R} D(z, ||p - z||)
\]</p>
<p>That is, we can construct ($ R^p $) by combining all the disks at all the
points in ($ R $) that have ($ p $) on their circumference.</p>
<p><strong>Proof sketch.</strong> This should make sense “by construction.” If point ($ c $) is
in this constructed ($ R^p $), then it lies in some disk ($ d $), and thus the
point ($ z \in R $) that created disk ($ d $) fulfills the existence criterion
of our definition.</p>
<p><strong>Lemma.</strong> If region ($ R $) has a boundary ($ B \subset R $), then ($
B^p = R^p $).</p>
<p><strong>Proof sketch.</strong> Clearly, ($ B^p \subset R^p $) if you believe the first lemma
— adding more points to ($ B $) should only increase the union of disks.</p>
<p>The harder direction to show is ($ R^p \subset B^p $). Consider some point ($ z
\in R$). Extend the ray ($ \overrightarrow{pz} $) until it intersects with ($ B
$) at point ($ z^\prime $). Then, we can relate the disks created by ($ z $)
and ($ z^\prime $): ($ D(z, ||p-z||) \subset D(z^\prime, ||p-z^\prime||) $).
Why? Because the disk from ($ z $) is smaller and internally tangent to the
disk from ($ z^\prime $).</p>
<p>Mapping this argument over the entire union, it makes sense that ($ R^p \subset
B^p $).</p>
<p>Okay, time for some empirical verification of all this theory. Try drawing the
boundary of a region (in black) here! The disks will show up in red. Notice how
filling in your region with black dots doesn’t change the red blob at all.
(Click to clear.)</p>
<canvas id="world-freestyle" width=300 height=300 style="border: 1px solid red;"></canvas>

<hr>
<p>The boundary-circle lemma gives us a nicer characterization of ($ R^p $): it is
the region bounded by the curve that is tangent to all of the disks created by
the points on ($ B $). It turns out that there is a very nice mathematical
theory of “the curve that is tangent to all curves in a given family of
curves,” and that is the theory of <em>envelopes</em>. The account below is
paraphrased from <a href="https://www.jstor.org/stable/3617131?seq=1#metadata_info_tab_contents">“What is an
Envelope?”</a>,
a lovely 1981 paper.</p>
<p>Let the function ($ F(x, t) : \mathbb{R}^2 \times \mathbb{R} \rightarrow
\mathbb{R} $) define a family of curves parameterized by ($ t $), in the sense
that ($ F(x, 0) = 0 $) defines a curve and ($ F(x, 1) = 0 $) defines another
curve, and so on. Then, we seek to characterize the envelope curve which is
tangent to every curve in ($ F $).</p>
<p><strong>Lemma.</strong> If the boundary of ($ R $) can be (periodically) parameterized as ($
B(t), t \in \mathbb{R} $) then the boundary of ($ R^p $) is the <em>envelope</em> with
respect to ($ t $) of
\[
F(x, t) = || x - B(t) ||^2 - || p - B(t) ||^2
\]</p>
<p><strong>Proof sketch.</strong> Again, this should make sense “by construction”: ($ F $) is
chosen to correspond to circles centered at ($ B(t) $) passing through ($ p $).</p>
<hr>
<p>Ah, but how do we find the envelope? Here we need a tiny bit of multivariable
calculus.</p>
<p>Let ($ X(t) $) be the parameterization of the envelope of ($ F $). Then for all
($ t $), we have that ($ F(X(t), t) = 0 $) because the envelope must lie on the
respective curve in the family (“tangent” means “touch”!). We also have that
the curve ($ X(t) $) must be parallel to the member of family ($ F $) at ($ t
$). We can then express this condition by saying that the gradient (with
respect to ($ X $)) of ($ F $) at ($ t $) is perpendicular to the derivative of
($ X $) at ($ t $). Or:</p>
<p>\[
d X(t) / dt    \cdot    \nabla_X F(X(t), t) = 0
\]</p>
<p>In two dimensions, with ($ X(t) = (x(t), y(t)) $), this equation manifests
itself as ($ x^\prime(t)\partial F(x, y, t) / \partial x + y^\prime(t)\partial
F(x, y, t) / \partial y $).</p>
<p>The left hand side is oddly reminiscent of the multivariable chain rule.
Indeed, if we took the partial derivative of our equation ($ F(X(t), t) = 0 $)
with respect to ($ t $), we would get:</p>
<p>\[
dF(X, t)/dt = dX/dt\cdot\nabla_X F(X, t) + dt/dt\cdot \partial F(X, t)/\partial t = 0
\]</p>
<p>So we must have ($ \partial F(X, t) / \partial t = 0$).</p>
<p>There is a simpler but less rigorous derivation if you believe that the
envelope is exactly the points of intersection of infinitesimally close curves
in the family ($ F $). Then we want every point ($ X $) on the envelope to
satisfy both ($ F(X, t) $) and ($ F(X, t + \delta) $) for some ($ t $) and some
infinitesimal ($ \delta $). Taking the limit as ($ \delta $) approaches zero
gives the same condition that ($ \partial F(X, t) / \partial t = 0 $). The
paper above discusses how this notion is subtly different in some strange
cases, but it suffices to say that for all “nice” ($ R $), we’re fine.</p>
<p><strong>Almost-a-theorem.</strong> The boundary of ($ R^p $) is given by the parameterized
vectors ($ X $) that satisfy ($ F(X, t) = 0 $) and ($ \partial F (X, t) /
\partial t = 0 $) for the ($ F $) defined above.</p>
<p><strong>Almost-a-proof-sketch.</strong> Almost! In general, the solution for ($ X $) might
self-intersect, so we want to take only the “outermost” part of ($ X $). But
this is easy to work out on a case-by-case basis.</p>
<p><strong>Example.</strong> Suppose ($ R $) is the unit disk centered at ($ (a, 0) $), and ($
p $) is located at the origin. Then ($ B(t) = (\cos t + a, \sin t) $) and ($ p
= (0, 0) $). We have</p>
<p>\[
F((x, y), t) =
  ((x - (\cos t + a))^2 + (y - \sin t)^2) - ((\cos t + a)^2 + \sin^2 t)
\]</p>
<p>\[
F((x, y), t) = -2ax + x^2 - 2x\cos t + y^2 - 2y \sin t = 0
\]</p>
<p>We also have</p>
<p>\[
\partial F((x, y), t) / \partial t = 2x\sin t - 2y\cos t = 0
\]</p>
<p>Solving these by eliminating ($ t $) is a simple exercise in polar coordinates.
Discover from the second equation that ($ t = \theta $), then recall that ($
r^2 = x^2 + y^2 $). The resulting boundary of ($ R^p $) is (almost!) the curve
($ r = 2(1 + a\cos\theta) $). In other words, it’s (almost!) a limaçon! As ($ a
$) varies, the character of the limaçon varies, and at the critical points ($ a
= \pm 1 $), we get a cardioid with a cusp. Beyond those critical points, the
curve has an inner loop that we have to ignore; hence, “almost!”</p>
<p>Okay, time for more empiricism. Move your mouse around in the square below to
see how the relative placement of LA and the casino affects the envelope.
Notice also the inner loop predicted by the envelope, which we should of course
ignore for the purposes of bounding ($ R^p $).</p>
<canvas id="world-cardioid" width=300 height=300 style="border: 1px solid red;"></canvas>

<p>An amazing fact is that a cardioid is the same shape you get on the surface of
your coffee mug when you put it under a light! Well, not quite — it depends
subtly on where the light source is. Read more about caustics at
<a href="http://chalkdustmagazine.com/features/cardioids-coffee-cups/">Chalkdust</a>, from
whom I also borrowed the image below:</p>
<p><img src="https://i0.wp.com/chalkdustmagazine.com/wp-content/uploads/2017/10/photo-1.jpg?resize=768%2C576" alt="coffee
cardioid"></p>
<hr>
<p>Further reading: Wikipedia has <a href="https://en.wikipedia.org/wiki/Cardioid#Cardioid_as_envelope_of_a_pencil_of_circles">great
diagrams</a>
to accompany.  Dan Kalman’s article <a href="http://dankalman.net/AUhome/pdffiles/ladder_paper_MM.pdf">“Solving the Ladder Problem on the Back of
an Envelope.”</a>
includes a nice pedagogically-oriented discussion of envelope subtleties. It
cites Courant’s <a href="https://archive.org/details/DifferentialIntegralCalculusVol2/page/n183">Differential and Integral Calculus (vol
2)</a>,
which is freely available on Archive.org.</p>
<script>
  var worldFreestyle = document.getElementById('world-freestyle');
  var worldCardioid = document.getElementById('world-cardioid');

  function mark(world, ctx, X, Y) {
    var originX = world.width / 2;
    var originY = world.height / 2;

    ctx.strokeStyle = 'rgba(255, 0, 0, 0.3)'
    ctx.beginPath();
    ctx.arc(
      X, Y,
      Math.sqrt(
          (X - originX) * (X - originX) +
        (Y - originY) * (Y - originY)),
      0, Math.PI * 2
    );
    ctx.stroke();
    ctx.fillRect(X, Y, 2, 2);
  }

  function clear(world, ctx) {
    var originX = world.width / 2;
    var originY = world.height / 2;

    // Clear
    world.height = world.height;
    ctx.textAlign = 'center';
    ctx.textBaseline = 'middle';

    // Casino
    ctx.strokeRect(originX - 2, originY - 2, 4, 4);
    ctx.fillText("Casino", originX, originY - 12);
  }

  worldFreestyle.addEventListener('click', function(event) {
    var ctx = this.getContext('2d');
    clear(this, ctx);
  }, false);
  clear(worldFreestyle, worldFreestyle.getContext('2d'));
  worldFreestyle.addEventListener('mousemove', function(event) {
    var ctx = this.getContext('2d');
    var X = event.clientX - this.offsetLeft + window.scrollX;
    var Y = event.clientY - this.offsetTop  + window.scrollY;
    mark(this, ctx, X, Y);
  }, false);

  worldCardioid.addEventListener('mousemove', function(event) {
    var ctx = this.getContext('2d');
    var X = event.clientX - this.offsetLeft + window.scrollX;
    var Y = event.clientY - this.offsetTop  + window.scrollY;
    var originX = this.width / 2;
    var originY = this.height / 2;
    clear(this, ctx);

    // Region
    var R = this.width / 6;
    var N = 18;
    for (var i = 0; i < N; i++) {
      mark(
        this, ctx,
        X + Math.sin(i / N * Math.PI * 2) * R,
        Y + Math.cos(i / N * Math.PI * 2) * R
      );
    }

    // Circle
    ctx.beginPath();
    ctx.arc(X, Y, R, 0, Math.PI * 2);
    ctx.stroke();
    ctx.fillStyle = 'black';
    ctx.fillText("Los Angeles", X, Y - 12);


    // Limacon
    ctx.save();
    ctx.lineWidth = 10;
    ctx.beginPath();
    for (var theta = 0; theta < 2 * Math.PI; theta += 0.01) {
      var phi = Math.atan2(originX - X, originY - Y);
      var a = Math.sqrt(Math.pow(originX - X, 2) + Math.pow(originY - Y, 2)) / R;
      var r = 2 * (1 + a * Math.cos(theta)) * R;
      ctx.lineTo(
        originX + r * Math.cos(theta - phi - Math.PI / 2),
        originY + r * Math.sin(theta - phi - Math.PI / 2)
      );
    }
    ctx.stroke();
    ctx.restore();
  }, false);
</script>
]]></description>
            <link>http://hardmath123.github.io/envelope.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/envelope.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 04 Jan 2019 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Tuning a Typewriter]]></title>
            <description><![CDATA[<p>A Markovian adventure in optimizing historical typewriter layouts</p>


<p>I recently read this <a href="https://www.theatlantic.com/technology/archive/2016/11/chinese-computers/504851/">Atlantic
piece</a>
on input methods for computers, and it reminded me of a mathematical adventure
I had this summer that I should have blogged about at the time.</p>
<p>Back in July, I was exploring the City Museum of New York when I came across
this lovely Crown typewriter from the late 1800s. By our modern standards, it
uses a rather clunky input mechanism: you manually shift the pointer to the
character you want to type, press the button, rinse, repeat.</p>
<p><img src="static/crown-typewriter/crown-typewriter.png" alt="Crown Typewriter"></p>
<p>“What a waste of time,” you say. Ah, but even in the late 19th centure time was
the essential ingredient, and in the modern world there was no time. Notice,
then, that the designers of the Crown typewriter did not place the letters in
alphabetical order. Instead, the they placed commonly-together letters near
each other. You can type “AND” and “THE” rather quickly with just 2 shifts
apiece; “GIG,” on the other hand, takes 15 shifts from the G to the I, and 15
shifts back to the G.</p>
<p>The question, then, begs to be asked: is this the <em>most efficient</em> permutation
of letters? For example, Q and U are on opposite ends of the typewriter; surely
it would be better to put them together?</p>
<p>I have a doubly disappointing answer to this question: first, this is <em>not</em> the
most efficient permutation of letters (based on my digraph frequency map), but
I also don’t <em>know</em> what the most efficient permutation is! That is, I’ve found
permutations that are more efficient, but cannot prove that they are optimal
— after all, exploring all 26-factorial permutations is not an option.</p>
<p>My “cost” metric here is defined as “Ignoring non-alphabetic characters, how
many shifts does it take to type all of <em>Pride and Prejudice</em> followed by all
of <em>The Time Traveller</em> followed by all of <em>The Adventures of Sherlock
Holmes</em>?” (All three are classic 19th-century novels available from Project
Gutenberg.)</p>
<p>By this metric, the naive alphabetical order (<code>ABCDEFGHIJKLMNOPQRSTUVWXYZ</code>)
requires a whopping 9,630,941 shifts. In comparison, the Crown Typewriter
(<code>XQKGBPMCOFLANDTHERISUWYJVZ</code>) requires only 6,283,692 shifts. But the
ComfortablyNumbered typewriter (<code>ZKVGWCDNIAHTESROLUMFYBPXJQ</code>) requires a mere
5,499,341 shifts. This means we can save nearly one in eight shifts from the
Crown typewriter!</p>
<p>I found this permutation with the following algorithm: start with a random
permutation, then “optimize” it by repeatedly swapping characters such that the
post-swap permutation has a lower cost than the pre-swap permutation. Continue
“optimizing” until no further swaps can be made. Now, do this procedure for a
large number of random starting permutations, and pick the lowest-cost
optimized permutation.</p>
<p>Encouragingly, it turns out that many different random starts get optimized to
my solution. What do I mean by that? Well, the graph below shows two datasets:
the costs of 1,000 random permutations and the costs of those permutations once
optimized. Clearly, optimization is doing great good.</p>
<p><img src="static/crown-typewriter/typewriter-graph-1.png" alt="First graph"></p>
<p>But let’s zoom in on the optimized permutations. Notice that the two
lowest-cost bars are almost an order of magnitude more frequent than the
remaining bars. In fact, the lower-cost bar’s “bucket” in the histogram is
populated <em>only</em> with permutations of cost 5499341! This suggests that 5499341
is a “hard ceiling” for my optimizer, rather than the furthest point on the tip
of a long tail that my optimizer samples from.</p>
<p><img src="static/crown-typewriter/typewriter-graph-2.png" alt="Second graph"></p>
<p>Of course, this is no proof: there might be a single highly-efficient
permutation that is hard to reach by optimizing a random permutation. But that
<em>feels</em> unlikely!</p>
<p>So: I leave this as an open problem for readers to explore.</p>
<blockquote>
<p>Update (Dec 15): I found this <a href="https://yurichev.com/blog/cabling_Z3/">blog
post</a> by Dennis Yurichev that tackles
the same problem, but restated as “in what order do I mount these devices on
a rack if I want to minimize the total length of cables between them”? Dennis
finds an optimal solution for 8 devices with Z3… perhaps the solution
scales to 26 “devices”? Intriguingly, his post was published just weeks
before my visit to the City Museum!</p>
<p>Update (Jan 28): @rjp on Github has posted a <a href="http://rjp.is/blogging/posts/2019/01/linear-typewriters/">blog
post</a> with
<em>several</em> new approaches to this problem!</p>
</blockquote>
]]></description>
            <link>http://hardmath123.github.io/crown-typewriter.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/crown-typewriter.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 20 Nov 2018 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Carnival of Mathematics 159]]></title>
            <description><![CDATA[<p>Another fine math I’ve gotten myself into.</p>


<blockquote>
<p>Welcome to this month’s edition of the <a href="https://aperiodical.com/carnival-of-mathematics/">Aperiodical Carnival of
Mathematics</a>!
The Carnival is a monthly roundup of exciting mathematical blog posts. Last
month, it was hosted by <a href="http://www.aperiodical.com/">Paul at the
Aperiodical</a>. This month, it is my honor
to host it here at Comfortably Numbered. But first…</p>
</blockquote>
<p><strong>Let’s play a game</strong>, shall we?</p>
<p>Pick a number. Not too large, though! You’re about to do some quick math on it.
(I’ll play along with 4.)</p>
<p>Okay. Ready? Good.</p>
<p>Now take your number and square it. (4 squared is 16.) Then, add your original
number to the square. (16 plus 4 is 20.) Finally, add forty-one. (20 plus 41 is
61.)</p>
<p>And now – your result – is it prime? Ha! I thought so. (61 certainly is.)</p>
<p>This little trick is due to Euler, who pointed out in 1772 that the polynomial
($ f(x) = x^2 + x + 41$) returns prime numbers for small integers — indeed,
all nonnegative integers up to and including 39. Since then, the quest for
other such “prime-generating” polynomials has fascinated number theorists from
around the world. As a little exercise, you may try convincing yourself that
there is no <em>perfect</em> prime-generating polynomial; that is, that there will
always be at least one integer input that gives a composite output.</p>
<p>But I digress. Here is what matters: The integers ($ x $) for which ($ f(x) $)
is composite are the deviants, the rebels, the ones who refuse to play along
with Euler’s little game.</p>
<p>Forty is the first such integer.</p>
<p>One hundred fifty-nine is another.</p>
<p>Welcome to the 159th Carnival of Mathematics.</p>
<hr>
<p>The Carnival always has a special place in its heart for clever ways to teach
children various math concepts. And this one’s no exception. In <a href="http://mathmisery.com/wp/2018/06/14/set-theory-for-second-grade/">Set Theory for
Second
Grade</a>, Manan
talks about how he designed an engaging lesson on set theory (and common
multiples!) for second graders. A quote from his students: “Can you hang this
in the hall so that everyone can see the college math we did?” </p>
<blockquote>
<p>What an amazing moment — new symbols, new concepts, no problems! At this
point, I made sure to remind them that what they are learning right now is no
different from what I would teach in college. And that if today, here in
second grade they could do college math, then in third grade they can do
third grade math, in fourth grade they can do fourth grade math, and that
they can always do math! More than a few students’ faces lit up.</p>
</blockquote>
<p>In <a href="https://cameroncounts.wordpress.com/2018/06/14/british-mathematical-colloquium-days-3-and-4/">British Mathematical Colloquium, days 3 and
4</a>,
Peter Cameron recounts in excellent detail the last couple days of the
Colloquium (it reads like a mini-Carnival!). Days 1 and 2 are linked within.</p>
<blockquote>
<p>An induced subgraph of a graph is obtained by throwing away some vertices and
the edges incident with them; you are not allowed to throw away an edge
within the set of vertices you are keeping. Paul began with the general
problem: given a graph H, can you determine the structure of graphs G
containing no induced copy of H? … The answer is known in embarrassingly
few cases … Not even for a 4-cycle is the answer known!</p>
</blockquote>
<p>In <a href="https://pballew.blogspot.com/2018/06/sum-of-cubes-is-square-of-sum-and-more.html">Sum of Cubes is Square of Sum… And
More!</a>,
Pat Ballew begins with a fact that most high-schoolers are taught, and then
rather suddenly finds himself deep in a fascinating rabbit hole. (Editor’s
note: I encourage you to read the author’s <em>On This Day in Math</em> series; I
would list all thirty of the past month if I could…)</p>
<blockquote>
<p>Like many teachers at the upper level high school math classes, over the
years I’ve presented the sum of the Cubes of the natural numbers formula
above many dozens of times.  Then, perhaps like many others, I would point
out how nice it is that it turns out to be the square of the nth triangular
number, a happy coincidence that would make it easier to remember.  Usually
then, we would challenge them to extend the idea to fourth powers and see if
they could do the induction proof, even though there was no really nice
simplification (to my knowledge) of the sums of fourth powers.</p>
<p>But then I reread a book that has been in my library for about six years, and
realized that many of those teachers may have known a different approach to
sums of cubes equaling square of sums that I had been completely unaware of.
In case there are other teachers who somehow also didn’t know, I share my
newfound ancient knowledge.</p>
</blockquote>
<p>In <a href="http://voices.norwich.edu/daniel-mcquillan/2018/06/29/thinking-about-the-law-of-quadratic-reciprocity/">Thinking about the Law of Quadratic
Reciprocity</a>,
Dan McQuillan gives a fast-paced overview of one of my personal favorite
theorems in number theory.</p>
<blockquote>
<p>Mathematics, the way it is currently written, can be difficult to read.
Sometimes it helps to see how people think about a topic or theorem before
(or after, or during) the reading of a proper treatment or rigorous proof.
The purpose of this post is to provide such a view regarding the proof of the
famous law of quadratic reciprocity. There are many details missing, on
purpose, and the hope is that it reads like a good story that’s both
interesting, believable and easily verifiable.</p>
</blockquote>
<hr>
<p><em>Just for fun!</em></p>
<p>In <a href="https://www.johndcook.com/blog/2018/06/16/magical-learning/">Magical
Learning</a>, John
Cook reports the results from an informal Twitter poll he conducted: “If a
genie offered to give you a thorough understanding of one theorem, what theorem
would you choose?”</p>
<p>In case you missed it, Christian at the Aperiodical is running <a href="http://aperiodical.com/2018/06/announcing-the-big-internet-math-off/">The Big
Internet
Math-Off</a>.
It may not have the intense moment-by-moment drama of the World Cup, but the
daily tidbits of math are definitely worth subscribing for.</p>
<p>In <a href="https://mathwithbaddrawings.com/2018/06/20/math-explained-through-anagrams-2/">Math Explained through
Anagrams</a>,
Ben Orlin constructs a frankly impressive amount of anagrams for various parts
of math. And they’re all illustrated! (Editor’s note: I also enjoyed <a href="https://mathwithbaddrawings.com/2018/06/27/powers-great-and-small/">this
piece</a> by
the same author. I don’t want to spoil it, but here is a wonderful quotation:
“I sometimes think that there are no puddles in math;” says Orlin, “there are
only oceans in disguise.”)</p>
<hr>
<p>That’s all I have for you this month! Come back next time when Robin at
<a href="http://www.theoremoftheday.org/">Theorem of the Day</a> will host the 160th
Carnival of Mathematics!</p>
]]></description>
            <link>http://hardmath123.github.io/carnival-of-mathematics-159.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/carnival-of-mathematics-159.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 06 Jul 2018 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Ladders Go Both Ways]]></title>
            <description><![CDATA[<p>A guest post for White Group Mathematics</p>


<p>Earlier this month, Frederick Koh, a Singaporean math tutor, invited me to
write a guest post for his <a href="http://www.whitegroupmaths.com">website</a>. Of course
I accepted — blogging is, after all, a conversation.</p>
<p>But what could I write about?</p>
<p>I spent the month of November on the lookout for an idea worth sharing, and
last week I found one. I follow a lot of math-educator blogs (blogging is,
after all, a conversation!), and one of my favorites is that of Dan Meyers.
Recently, Dan posted a rather provocatively-titled piece, “<a href="http://blog.mrmeyer.com/2017/dismantling-the-privilege-of-the-mathematical-1/">Dismantling the
Privilege of the Mathematical
1%</a>“.
He makes the case that those with a mathematical education — those who are
mathematically <em>privileged</em> — those who make up the mathematical 1% — they
are the ones with the responsibility to define what mathematics <em>is</em>. Dan says,
eloquently as always, that “through our action or inaction we create systems
that preserve our status as the knowers and doers of mathematics.”</p>
<p>Dan’s post made me think deeply about where I myself fit into his spectrum: I’m
hardly one of the 1% — and certainly won’t graduate with a math degree —
but at the same time, I love math, both as an activity and as a set of truths.</p>
<p>And I realized that this gives <em>me</em> a privileged position, one where I can
comment on mathematical topics from a somewhat neutral perspective. I am
neither the person who barely scraped through high school calculus, nor the
person who skipped high school calculus because it was too easy. So, trapped
between “math people” and “not a math person people”, I wish to use this post
to explore what exactly creates this divide — or, rather, explore it in a way
slightly different from what you have probably heard a dozen times already.</p>
<hr>
<p>It is almost universally acknowledged that “education is good.” More education,
says society, will lead to a happier, more prosperous world. Educate everyone,
says society.</p>
<p>I agree, of course.</p>
<p>And yet, paradoxically, we are so attached to the notion that education —
college, in my case, grad school or high school for others — is a means to
distinguish oneself from one’s peers. A degree, we argue, makes us stand out in
both the workplace and in society. The more competitive the institution, the
more valuable the degree.</p>
<p>Less enthusiastically, I agree with this as well: reality forces me to concede
the point. Why else would college admissions be so competitive? Why else would
classes be graded on curves? Why else would we even have grades in the first
place? As disturbed as I am by it, society cares very much about my academic
performance, <em>especially</em> in relation to others.</p>
<p>And therein lies the rub. Reader, are these not contradictory notions? If the
purpose of an education for an individual is to separate him or her from the
general populace, then what follows is the absurd notion that universal
education is self-defeating: that the more people we educate, the less an
education is worth to the individual.</p>
<p>How does one reconcile this? Can education in the limit benefit both the
individual and the society? As an optimist, I wish it could. And in fact I
believe it <em>can</em>, but only if we rethink what education means to the
individual.</p>
<hr>
<p>Here is what I think. I believe that too often, we conceive education to be a
ladder that lifts us — above others, if we’re fast enough — rung by rung.
The more you climb, the higher you get.</p>
<p>But too often, we forget that you can also climb <em>down</em> a ladder. That an
education can also lower the arrogant to humility and place them alongside the
less privileged, on common ground. That education builds capacity for empathy
and communication, empowering the individual but also society at large to have
a dialogue. That this view resolves the paradox of the previous section,
because both individuals <em>and</em> society benefit from the capacity for having
that dialogue.</p>
<p>What do I mean by this? Let’s return to mathematics for a moment.</p>
<p>If you are reading this blog, you most likely have been at a gathering of
mathematicians at some point in your lives. It is quite a marvellous thing to
behold: a congregation of brilliant minds sharing ideas. Mathematics as a
community has its own folklore, its own in-jokes, and its own language. You
need only to glance at sites like <a href="https://artofproblemsolving.com/community">The Art of Problem
Solving</a> or
<a href="http://www.mathmo.org/test/mathmotest.html">mathmo.org</a> to see this community
in all its glory.</p>
<p>I love it. There is charm to the way mathematicians celebrate their field,
unlike any other profession I have come across.</p>
<p>And yet, imagine being an outsider for a moment. Imagine being part of the
mathematical 99%. How would you feel if someone responded to a question of
yours with a grin, saying “left as an exercise to the reader”? Or if someone
made you use this <a href="http://www.danielallington.net/2016/09/the-latex-fetish/">weird software
package</a> with lots of
backslashes to write up your homework? Or if someone went off on a tangent
about why their coffee mug said “donut” on it? Or if someone makes an
arithmetic mistake and then says “it’s true in base 12” before you even notice
the error? Or if someone claimed a very unobvious-to-you solution was
“trivial”?</p>
<p>These phrases aren’t meant to be exclusionary. Some are common inside jokes.
Others are part of the mathematical vocabulary. The word “trivial,” after all,
has a very specific mathematical meaning — think of the “trivial group” with
one element, for example. I
<a href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=trivial+site:hardmath123.github.io&amp;ie=UTF-8&amp;oe=UTF-8">use</a>
it all the time.</p>
<p>Yet to an outsider, they make the mathematical community seem simply
impenetrable. How am I ever going to understand all this? I can’t think that
fast!</p>
<p>“Math people” tend to be remarkably self-selecting, and I believe this is one
of the reasons why: there is a divide between the initiated and the
uninitiated, and far too few resources for the latter.</p>
<hr>
<p>Education, I believe, should be tasked with <em>bridging</em> this divide — rather
than exacerbating it as it does now. Education should give the 99% the
opportunity to join the magical world of mathematics, but education should
<em>also</em> show the 1% how to open up the world to new members. It should teach
students to write about mathematics, finding a middle ground between dense
manuscripts weighed down by Greek-letter jargon, and airy puff-pieces that
contain nothing of substance. Where will future Ian Stewarts, Martin Gardners,
and Brian Hayes come from? I myself would almost certainly be very firmly a
“not a math person” person were it not for an Ian Stewart book in my dad’s
bookshelf that taught me about Fermat’s Last Theorem and the Mandelbrot Set
when I was very young.</p>
<p>Education should also encourage the next generation of professors to move away
from lessons that consist of the copying of lecture notes onto a chalkboard,
and provide them with the tools they need to create engaging, interactive
lessons that appeal not only to those whom we believe are pre-ordained to be
mathematicians, but to artists, musicians, writers, and athletes. It should
teach students about the bigger picture of where their mathematics fits into
society, about who produces and who consumes mathematics, and why.</p>
<p>At the same time, it should explain to the mathematical 99% what exactly those
math people are on about all the time. It should teach them the mathematical
canon, help them learn the language, and help them discover the beauty that the
1% have already found.</p>
<p>Yes, such an education will produce a more diverse generation of
mathematicians, not only in terms of demographics, but also in terms of ways of
thinking. It will produce a generation that writes not only more effective
grant proposals, but also clearer papers. That’s what the individuals get out
of it.</p>
<p>But it will also inspire the mathematical community to rise up against the
tyrrany of Alice-gives-Bob-three-bananas standardized tests, to use their
passion for mathematics and their perception of its beauty to guide the
development of curriculums and lesson plans. Not to reject non-math-people as
heretics who refuse to see the light, but rather to see them as evidence of a
broken education system that failed to convey the beauty of mathematics.</p>
<p>To redefine mathematics to be the way <em>they</em> see it, because, whether or not
they realize it, the way the mathematical 1% sees mathematics — as the
pursuit of beautiful truth — is far from what the mathematical 99% sees it as
— the painfully rushed manipulation of symbols on a midterm. To <em>care</em>.</p>
<p>That’s what society gets out of it.</p>
<blockquote>
<p>This post first appeared on <a href="http://www.whitegroupmaths.com/2017/11/guest-post-ladders-go-both-ways.html">White Group
Maths</a>.</p>
</blockquote>
]]></description>
            <link>http://hardmath123.github.io/ladders.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/ladders.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 20 Nov 2017 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Carnival of Mathematics 148]]></title>
            <description><![CDATA[<p>I had so much fun the first time, I volunteered to host it again!</p>


<blockquote>
<p>Welcome to this month’s edition of the <a href="http://aperiodical.com/carnival-of-mathematics/">Aperiodical Carnival of
Mathematics</a>! The Carnival
is a monthly roundup of exciting mathematical blog posts. Last month, it was
hosted by Lucy at <a href="http://www.cambridgemaths.org/news/view/carnival-of-maths-147/">Cambridge
Maths</a>. This
month, it is my honor to host it here at Comfortably Numbered. But first: a
story…</p>
</blockquote>
<hr>
<p>A long time ago, there lived a great and powerful king. His kingdom was rich
and his subjects content; ambassadors from afar would often come to his palace
gates to offer presents from their distant homes. One day, such a traveler
approached the king and challenged him to a game of chess. The king, always
welcoming to his guests, accepted the challenge. He sent a minister to fetch a
chessboard and pieces for the game. Soon, they were playing an intense game;
the court watched in a hushed silence as the great king and the foreign
traveler concentrated on the board. At last, the king won.</p>
<p>Now, this particular traveler was quite an arrogant young man. Outraged that
anyone could defeat him, he rose and smashed the chessboard with his fist. The
carved wood shattered into a number of pieces, splitting at the delicate joints
between the black and white squares. The pieces clattered noisily on the marble
floor.</p>
<p>Normally, such behavior would be punishable by death. But this king was a
merciful king: he understood the insecurities of youth, and he believed in the
healing powers of recreational math. So, he decided to give the traveler a
second chance. He said,</p>
<p>“Young man, do you notice anything about the shattered pieces of the
chessboard?”</p>
<p>The traveler, embarrassed, paused before answering,</p>
<p>“Your majesty, each piece seems to be a perfect square: there are three 4-by-4
squares, twelve 1-by-1 squares, and one 2-by-2 square.”</p>
<p>“Very good,” replies the king, “very good indeed. Now, if you can tell me <em>how
many ways there are</em> to break a chessboard into (indistinguishable) square
pieces, I shall spare your life.”</p>
<p>The next day, the traveler returned to the palace, escorted by guards. Standing
before the throne, he handed the king a card upon which was written his answer:
148, which coincidentally happens to be this month’s Aperiodical Carnival of
Mathematics!</p>
<hr>
<p>Ayliean MacDonald <a href="https://www.youtube.com/watch?v=sjO6XhXijDQ">entrances us</a>
by drawing a giant dragon curve! She says there was a bit of a <em>learning curve</em>
involved, but fortunately for us, her video is a magnificent timelapse.</p>
<blockquote>
<p>I love dragon curves, so I drew a gigantic one. I regretted starting it
almost immediately. Let’s just say this was my first attempt and its a
learning curve.</p>
</blockquote>
<p>Rachel Traylor
<a href="http://www.themathcitadel.com/2017/06/21/the-rigor-of-fuzzy-sets/">extends</a>
sets to “fuzzy” sets, which are just like normal sets, but, well, fuzzy!</p>
<blockquote>
<p>What if we relax the requirement that you either be in or out? Here or there?
Yes or no? What if I allow “shades of grey” to use a colloquialism? Then we
extend classical sets to fuzzy sets.</p>
</blockquote>
<p>Dr. Nira Chamberlain
<a href="http://nirachamberlain.com/mathematical-modelling-unlimited/">explains</a> how
important mathematical modeling is, and in particular, uses an extension of the
gambler’s ruin problem to show how a model’s assumptions and limitations are
related.</p>
<blockquote>
<p>To me, mathematical modelling is about looking into the real world;
translating it into mathematics, solving that mathematics and then applying
that solution back into the real world.</p>
</blockquote>
<p>Edmund Harriss
<a href="https://maxwelldemon.com/2017/07/18/functional-drawing-at-c/">exploits</a> Desmos
to draw some fantastic images.</p>
<blockquote>
<p>One of my courses was to use Desmos to help develop thinking on functions and
start to get to some of the ideas of calculus (without the need for the
algebra). Here are the example calculators that I set up for the course.</p>
</blockquote>
<p>David H. Bailey
<a href="http://mathscholar.org/pi-and-the-collapse-of-peer-review">examines</a> a
surprisingly large collection of published papers that assert (incorrect!)
values of pi.</p>
<blockquote>
<p>Aren’t we glad we live in the 21st century, with iPhones, Teslas, CRISPR
gene-editing technology, and supercomputers that can analyze the most complex
physical, biological and environmental phenomena? and where our extensive
international system of peer-reviewed journals produces an ever-growing body
of reliable scientific knowledge? Surely incidents such as the Indiana pi
episode are well behind us?</p>
</blockquote>
<p>Jeremy Kun
<a href="https://jeremykun.com/2017/07/24/boolean-logic-in-quadratic-polynomials/">embeds</a>
Boolean logic in polynomials, thus revealing why finding the roots of
multivariate polynomials is NP-hard.</p>
<blockquote>
<p>This trick is used all over CS theory to embed boolean logic within
polynomials, and it makes the name “boolean algebra” obvious, because it’s
just a subset of normal algebra.</p>
</blockquote>
<p>Ben Orlin
<a href="https://mathwithbaddrawings.com/2017/07/26/never-buy-two-lottery-tickets/">evaluates</a>
whether or not you should ever buy <em>two</em> lottery tickets.</p>
<blockquote>
<p>Gambling advice from mathematicians is usually pretty simple. In fact, it’s
rarely longer than one word: Don’t! My advice is gentler…</p>
</blockquote>
<p>Jimmy Soni and Rob Goodman
<a href="https://blogs.scientificamerican.com/voices/betty-shannon-unsung-mathematical-genius/">emphasize</a>
that Claude Shannon’s wife Betty Shannon was a brilliant mathematician herself,
and in fact instrumental in many of his successes.</p>
<blockquote>
<p>Shannon valued the help. Though his ideas were very much his own, Betty
turned them into publishable work. Shannon was prone to thinking in leaps—to
solving problems in his mind before addressing all the intermediary steps on
paper. Like many an intuitive mind before him, he loathed showing his work.
So Betty filled in the gaps.</p>
</blockquote>
<p>Peter Cameron <a href="https://cameroncounts.wordpress.com/2017/07/25/all-kinds-of-mathematics-1/">elaborates
on</a>
the conference held in honor of his 70th birthday, in Lisbon.</p>
<blockquote>
<p>There is far too much, and far too diverse, mathematics going on here for me
to describe all or even most of it. Nine plenary lectures on the first day!
… I didn’t mention the film that the organisers have made about me (based
mostly on old photographs). I am really not used to being in the spotlight to
this extent!</p>
</blockquote>
<p>Patrick Honner <a href="http://mrhonner.com/archives/17698">elucidates</a> an erroneous
exam question, and along the way tells us the story of a 16-year-old student
who exposed the error and started a Change.org petition in response.</p>
<blockquote>
<p>So, this high-stakes exam question has no correct answer. And despite the
Change.org petition started by a 16-year-old student that made national news,
the New York State Education Department refuses to issue a correction.</p>
</blockquote>
<p>Danesh Forouhari
<a href="https://twitter.com/dforouhari/status/828341250164088833">extracts</a> a
fantastic math problem from properties of the factorization of 2016.</p>
<blockquote>
<p>As we were approaching end of 2016, I was wondering if I could come up with a
math puzzle which ties up 2016 &amp; 2017, hence this puzzle.</p>
</blockquote>
<p>Rachael Horsman
<a href="http://www.cambridgemaths.org/news/view/learning-to-measure-pacing-off/">extrapolates</a>
a lesson about measuring to explain the entire number line!</p>
<blockquote>
<p>As we write the framework, activities such as pacing off form critical
junctions between various areas in mathematics … Making these explicit to
teachers and pupils helps cultivate their understanding of the connections
that make up mathematics.</p>
</blockquote>
<p>Vijay Kathotia <a href="http://www.cambridgemaths.org/news/view/what-is-the-roman-numeral-for-ten/">enlightens
us</a>
by revealing where the roman numeral for “10” might have come from.</p>
<blockquote>
<p>What is the Roman numeral for ten? If you answered ‘the letter X’, it may not
be quite right. It may well be what you write – but do you know why? There
are at least two stories for explaining how X came to represent ten.</p>
</blockquote>
<p>Lucy Rycroft-Smith
<a href="http://www.cambridgemaths.org/news/view/intersections-mathematics-and-the-artist/">engages</a>
UK-based artist MJ Forster in an interview about how math influences his art.</p>
<blockquote>
<p>His latest series of paintings seem inherently mathematical; but just how
explicit is the mathematics in his art, and how does he feel about the
subject?</p>
</blockquote>
<p>Brian Hayes <a href="http://bit-player.org/2017/approximately-yours">estimates</a> pi
using rational numbers and an HP-41C calculator!</p>
<blockquote>
<p>Today, I’m told, is Rational Approximation Day. It’s 22/7 (for those who
write dates in little-endian format), which differs from pi by about 0.04
percent. (The big-endians among us are welcome to approximate 1/pi.)</p>
</blockquote>
<hr>
<p>Finally, the world <strong>expresses its sorrow:</strong> Fields medalist Maryam Mirzakhani
<a href="http://news.stanford.edu/2017/07/15/maryam-mirzakhani-stanford-mathematician-and-fields-medal-winner-dies/">passed away this
July</a>
after a long battle with cancer. In keeping with the Carnival’s tradition, I
offer you two blog posts from the mathematical community.</p>
<p><a href="https://rjlipton.wordpress.com/2017/07/28/maryam-mirzakhani-1977-2017/">Ken
Regan:</a>
on the significance and brilliance of her work</p>
<blockquote>
<p>She made several breakthroughs in the geometric understanding of dynamical
systems. Who knows what other great results she would have found if she had
lived: we will never know. Besides her research she also was the first woman
and the first Iranian to win the Fields Medal.</p>
</blockquote>
<p><a href="https://terrytao.wordpress.com/2017/07/15/maryam-mirzakhani/">Terence Tao:</a> a
more personal perspective</p>
<blockquote>
<p>Maryam was an amazing mathematician and also a wonderful and humble human
being, who was at the peak of her powers. Today was a huge loss for Maryam’s
family and friends, as well as for mathematics.</p>
</blockquote>
<p>If you would like to learn more about her life and work, I encourage you to
read some of the articles and watch the video on <a href="http://www.ams.org/profession/mirzakhani">the AMS’ tribute to Maryam
Mirzakhani</a>.</p>
<hr>
<p>This concludes the 148th edition of the Carnival of Mathematics. Please do join
us next time, when Mel from <a href="http://justmaths.co.uk/blog/">Just Maths</a> hosts
the 149th!</p>
]]></description>
            <link>http://hardmath123.github.io/carnival-of-mathematics-148.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/carnival-of-mathematics-148.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 31 Jul 2017 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[A Fishy Function]]></title>
            <description><![CDATA[<p>The Hitchhiker’s Guide to Patience, or, How To Use Earthquakes To Calculate Compound Interest</p>


<blockquote>
<p>I started thinking about these ideas in late May, but haven’t gotten a chance
to write about them until now…</p>
</blockquote>
<p>If you want to take a boat from the Puget Sound to Lake Washington, you need to
go across the <em>Ballard Locks</em>, which separate the Pacific Ocean’s saltwater
from the freshwater lakes. The locks are an artificial barrier, built in the
early 1900s to facilitate shipping.</p>
<p>Today, the locks have a secondary function: they are a picnic spot. A while
back, I visited the locks on a sunny and warm day. A band was playing in the
park by the water, and there were booths with lemonade and carnival games.
Every few minutes, a boat would enter the locks, be raised or lowered, and
continue on its way.</p>
<p>If you walk across the locks, you can check out the <em>fish ladder</em>, a series of
raised steps designed to help fish — in this case, salmon — migrate, since
the locks cut off their natural path between the water bodies. There is usually
a crowd around the fish ladder. Around once a minute, a salmon leaps out of the
water and goes up a step; the children gasp and cheer as they watch over the
railing.</p>
<p>This is the idyllic scene that we will soon destroy with the heavy hammer of
mathematical statistics. You see, it turns out that a little bit of thought
about these salmon can give us a way to use historical earthquake data to
approximate ($ e $).</p>
<p>But I’m getting ahead of myself. Let’s start at the beginning.</p>
<hr>
<p>What is the probability that a fish jumps out of the water <em>right now?</em> This is
a tricky question to answer. Suppose there’s a 10% chance that a fish jumps out
of the water right now. That means the probability that a fish <em>doesn’t</em> jump
is 90%. In the next instant of time, there’s again a 10% chance that the fish
jumps. So, the laws of probability tell us that over the course of ($ n $)
instants, there’s a ($ 0.90^n $) probability that <em>no</em> fish-jumps occur.</p>
<p>But there’s an <em>infinite</em> number of instants in every second! Time is
continuous: you can subdivide it as much as you want. So the probability that
no fish-jumps occur in a one-second period is ($ 0.90^\infty $), which is…
zero! Following this reasoning, a fish must jump <em>at least</em> every second. And
this is clearly a lie: empirically, the average time between fish-jumps is
closer to a minute.</p>
<p>Okay, so “probability that a fish jumps <em>right now</em>“ is a slippery thing to
define. What can we do instead? Since the problem seems to be the “right now”
part of the definition, let’s try to specify a time <em>interval</em> instead of an
<em>instant</em>. For example, what is the probability that we will observe ($ n $)
fish-jumps in the next ($ t $) seconds?</p>
<p>Well, we’re going to need some assumptions. For simplicity, I’m going to assume
from now on that fish jump independently, that is, if one fish jumps, then it
does not affect the behavior of any other fish. I don’t know enough about
piscine psychology to know whether or not this is a valid assumption, but it
doesn’t sound too far-fetched.</p>
<hr>
<p>While we’re on the subject of far-fetchedness: the math that follows is going
to involve a lot of handwaving and flying-by-the-seat-of-your-pants. We’re
going to guess at functions, introduce constants whenever we feel like it,
evaluate things that may or may not converge, and, throwing caution and
continuity to the wind, take derivatives of things that might be better left
underived.</p>
<p>I think it’s more fun this way.</p>
<p>Yes, we <em>could</em> take the time to formalize the ideas with lots of definitions
and theorems and whatnot. There’s a lot to be said about mathematical rigor,
and it’s really important for you, the reader, to be extremely skeptical of
anything I say. In fact, I <em>encourage</em> you to look for mistakes: the reasoning
I’m about to show you is entirely my own, and probably has some bugs here and
there. (The <em>conclusions</em>, for the record, match what various textbooks say;
they just derive them in a slightly different way.)</p>
<p>A couple of lemmas here and there might make the arguments here much more
convincing. But they will also make this post tedious and uninspiring, and I
don’t want to go down that road. If you’re curious, you can look up the gnarly
details in a book. Until then, well, we’ve got bigger fish to fry!</p>
<hr>
<p>Okay, back to math. We can model the probability we’re talking about with a
function that takes ($ n $) and ($ t $) as inputs and tells you the
probability, ($ P(n, t) $), that you see ($ n $) fish-jumps in the time period
($ t $). What are some things we know about ($ P $)?</p>
<p>Well, for starters, ($ P(n, 0) = 0 $), since in <em>no</em> time, there’s no way
anything can happen.</p>
<p>What about ($ P(n, a + b) $)? That’s the probability that there are ($ n $)
fish-jumps in ($ a + b $) seconds. We can decompose this based on how many of
the fish-jumps occurred in the “($ a $)” and “($ b $)” periods:</p>
<p>\begin{align}
P(n, a+b)   &amp; = P(0, a)P(n, b) \\
            &amp; + P(1, a)P(n-1, b) \\
            &amp; + \ldots \\
            &amp; + P(n, a)P(0, b)
\end{align}</p>
<p>Hmm. This looks familiar… perhaps…</p>
<p>Yes! Isn’t this what you do to the coefficients of polynomials when you
multiply them? The coefficient of ($ x^n $) in ($ a(x)b(x) $) is a similar
product, in terms of the coefficients of ($ x^i $) and ($ x^{n-i} $) in ($ a(x)
$) and ($ b(x) $), respectively.</p>
<p>This can’t be a coincidence. In fact, it feels appropriate to break out this
gif again:</p>
<p><img src="static/coincidence.gif" alt="Benedict Cumberbatch being awesome"></p>
<p>Let’s try to force things into polynomial form and see what happens. Let ($
p_t(x) $) be a polynomial where the coefficient of ($ x^n $) is the
probability that ($ n $) fish-jumps occur in time ($ t $):</p>
<p>\begin{align}
p_t(x) &amp;= P(0, t)x^0 + P(1, t)x^1 + \ldots \\
        &amp;= \sum_{n=0}^\infty P(n, t)x^n
\end{align}</p>
<p>(Yes, fine, since ($ n $) can be arbitrarily large, it’s <em>technically</em> a “power
series”, which is just an infinitely long polynomial. Even more technically,
it’s a <em>generating function</em>.)</p>
<p>We know that ($ p_0(x) = 1 $), because nothing happens in no time, i.e. the
probability of zero fish-jumps is “1” and the probability of any other number
of fish-jumps is “0”. So ($ p_0(x) = 1x^0 + 0x^1 + \ldots $), which is equal
to just “1”.</p>
<p>What else do we know? It should make sense that ($ p_t(1) = 1 $), since if you
plug in “1”, you just add up the coefficients of each term of the polynomial.
Since the coefficients are the probabilities, they have to add up to “1” as
well.</p>
<p>Now, taking a leap of faith, let’s say that ($ p_{a+b}(x) = p_a(x)p_b(x) $),
because when the coefficients multiply, they work the same way as when we
decomposed the probabilities above.</p>
<p>Why is this property interesting,? We’re turning a property about addition into
a property about multiplication. That sounds awfully like something <em>else</em>
we’re used to:  logarithms!  Forgetting for a moment that ($ p $) is a power
series, maybe we can “solve” for the function ($ p_t(x) $) by messing around
with something like this:</p>
<p>\[
p_t(x) = e^{tx}
\]</p>
<p>Okay, ($ e^{tx} $) doesn’t quite work because we want ($ p_t(1) = 1 $). Maybe
($ e^{t(x-1)} $) will work? It seems to have all the properties we want…</p>
<hr>
<p>Let’s take a moment to stop and think. At this point, it’s not even clear what
we’re <em>doing</em>. The whole point of defining ($ p_t(x) $) was to look at the
coefficients, but when we “simplify” it into ($ e^{t(x-1)} $) we no longer have
a power series.</p>
<p>Or do we?</p>
<p>Recall from calculus class that you can expand out some functions using their
<em>Taylor Series</em> approximation, which is a power series. In particular, you can
show using some Fancy Math that</p>
<p>\begin{align}
e^x &amp;= \frac{x^0}{0!} + \frac{x^1}{1!} + \frac{x^2}{2!} + \ldots \\
    &amp;= \sum_{n=0}^\infty \frac{x^n}{n!}
\end{align}</p>
<p>If you haven’t taken calculus class yet, I promise this isn’t black magic. It’s
not even plain magic. It’s just a result of a clever observation about what
happens to ($ e^x $) when you increase ($ x $) by a little bit.</p>
<p>If you <em>have</em> taken calculus, bet you didn’t think this “series approximation”
stuff would ever be useful! But it <em>is</em>, because a quick transformation gives
us the series representation for ($ p_t(x) $):</p>
<p>\[
e^{t(x-1)} = e^{tx}/e^t = \sum_{n=0}^\infty \frac{(tx)^n}{n!e^t}
\]</p>
<p>and so the coefficient of ($ x^n $) gives us ($ P(n, t) = t^n/(e^t n!) $).</p>
<hr>
<p>Now we have a <em>new</em> problem: this formula doesn’t depend <em>at all</em> on the type
of events we’re observing. In particular, the formula doesn’t “know” that the
salmon at Lake Washington jump around once a minute. We never told it! Fish at
<em>other</em> lakes might jump more or less frequently, but the formula gives the
same results. So the formula must be wrong. Sad.</p>
<p>But it might be salvageable! Let’s go back and see if we can add a new constant
to represent the lake we’re in. Perhaps we can call it ($ \lambda $), the
Greek letter “L” for lake. Where could we slip this constant in?</p>
<p>Our solution for ($ p_t(x) $) was:</p>
<p>\[
p_t(x) = e^{t(x-1)}
\]</p>
<p>but in retrospect, the base ($ e $) was pretty arbitrarily chosen. We could
make the base ($ \lambda $) instead of ($ e $), but that would mess up the
Taylor Series, which only works with base ($ e $). That would be inconvenient.</p>
<p>However, we know that we can “turn” ($ e $) into any number by raising it to a
power, since ($ e^{\log b} = b $). If we want base ($ b $), we can replace ($
e $) with ($ e^{\log b} $). This suggests that ($ \lambda = \log b $) could
work, making our equation:</p>
<p>\[
p_t(x) = \left(e^\lambda\right)^{t(x-1)} = e^{(\lambda t) (x-1)}
\]</p>
<p>This seems to fit the properties we wanted above (you can check them if you
want). Going back to our Taylor Series expansion, we can just replace ($ t $)
with ($ \lambda t $) to get:</p>
<p>\[
P(n, t) = \frac{\left(\lambda t\right)^n}{e^{\lambda t} n!}
\]</p>
<hr>
<p>Let’s step back and think about what we’re claiming. Knowing <em>only</em> that fish
jump randomly, and roughly independently, we claim to have an expression for
the probability that ($ n $) fish-jumps occur in a time interval ($ t $).</p>
<p>“Okay, hold up,” you say, “something smells fishy about this. This is pretty
bold: we know nothing about how fish think, or fluid dynamics, or whatever
other factors could influence a fish’s decision to jump.  And yet we have this
scary-looking expression with ($ e $) and a factorial in there!”</p>
<p>That’s a fair point. I’m just as skeptical as you are. It would be good to back
up these claims with some data. Sadly, I <em>didn’t</em> spend my time in Seattle
recording fish-jumping times. But, in a few more sections, I promise there will
be some empirical evidence to assuage your worries. Until then, let’s press on,
and see what else we can say about fish.</p>
<hr>
<p>We have a way to get the probability of some number of fish-jumps in some
amount of time. What’s next?</p>
<p>One thing we can do is compute the <em>average</em> number of fish-jumps in that time
interval, using expected value. Recall that to find expected value, you
multiply the probabilities with the values. In this case, we want to find:</p>
<p>\[
E_t[n] = \sum_{n=0}^\infty P(n, t)n
\]</p>
<p>This looks hard… but also oddly familiar. Remember that</p>
<p>\[
p_t(x) = \sum_{n=0}^\infty P(n, t)x^n
\]</p>
<p>because, y’know, that’s how we defined it. Using some more Fancy Math (“taking
the derivative”), this means that</p>
<p>\[
\frac{dp_t(x)}{dx} = \sum_{n=0}^\infty P(n, t)nx^{n-1}
\]</p>
<p>and so ($ E_t[n] = p^\prime_t(1) $).</p>
<p>That… still looks hard. Derivatives of infinite sums are no fun. But remember
from the last section that we also have a <em>finite</em> way to represent ($ p_t(x)
$): what happens if we take <em>its</em> derivative?</p>
<p>\begin{align}
p_t(x)         &amp;= e^{(\lambda t) (x-1)} \\
p^\prime_t(x) &amp;= (\lambda t)e^{(\lambda t) (x-1)} \\
p^\prime_t(1) &amp;= E_t[n] = \lambda t
\end{align}</p>
<p>Aha! The average number of fish-jumps in time ($ t $) is ($ \lambda t $). If
($ t $) has units of time and ($ \lambda t $) has units of fish-jumps, this
means that ($ \lambda $) has units of fish-jumps-per-time. In other words, ($
\lambda $) is just the <em>rate</em> of fish-jumps in that particular lake! For Lake
Washington, ($ \lambda_w = 1/60 \approx 0.0167 $) fish-jumps-per-second,
which means that the probability of seeing two fish-jumps in the next thirty
seconds is:</p>
<p>\[
p_{30}(2) = \frac{(0.0167\times30)^2}{e^{0.0167\times30}2!} \approx 0.076
\]</p>
<p>I think that’s pretty neat.</p>
<hr>
<p>What about the <em>standard deviation</em> of the number of fish-jumps? That sounds
ambitious. But things have been working out pretty well so far, so let’s go for
it.</p>
<p><em>Standard deviation</em>, or ($ \sigma $), the Greek letter “sigma”, is a measure
of “how far, on average, are we from the mean?” and as such seems easy to
define:</p>
<p>\[
\sigma = E[n-\lambda t]
\]</p>
<p>Well, this isn’t hard to evaluate. Knowing that expected values add up, we can
do some quick math:</p>
<p>\begin{align}
\sigma &amp;= E[n] - E[\lambda t] \\
        &amp;= \lambda t - \lambda t = 0
\end{align}</p>
<p>Oops. We’re <em>definitely</em> off by a <em>little bit</em> on average, so there’s <em>no way</em>
that the standard deviation is 0. What went wrong?</p>
<p>Well, ($ n - \lambda t $) is negative if ($ n $) is <em>lower</em> than expected!
When you add the negative values to the positive ones, they cancel out.</p>
<p>This is annoying. But there’s an easy way to turn negative numbers positive: we
can square them. Let’s try that.</p>
<p>\begin{align}
\sigma^2 &amp;= E[(n-\lambda t)^2] \\
          &amp;= E[n^2 - 2n\lambda t + (\lambda t)^2]
\end{align}</p>
<p>Now what? We don’t know anything about how ($ E[n^2] $) behaves.</p>
<p>Let’s go back to how we figured out ($ E[n] $) for inspiration. The big idea
was that</p>
<p>\[
\frac{dp_t(x)}{dx} = \sum_{n=0}^\infty P(n, t)nx^{n-1}
\]</p>
<p>Hmm. What if we take another derivative?</p>
<p>\[
\frac{d^2p_t(x)}{dx^2} = \sum_{n=0}^\infty P(n, t)n(n-1)x^{n-2}
\]</p>
<p>We get an ($ n(n-1) $) term, which isn’t <em>quite</em> ($ n^2 $), but it’s
degree-two. Let’s roll with it. Following what we did last time,</p>
<p>\begin{align}
p_t(x) &amp;= e^{(\lambda t)(x - 1)} \\
p^\prime_t(x) &amp;= (\lambda t)e^{(\lambda t)(x - 1)} \\
p^{\prime\prime}_t(x) &amp;= (\lambda t)(\lambda t)e^{(\lambda t)(x - 1)} \\
E[n(n-1)] &amp;= p^{\prime\prime}_t(1) \\
          &amp;= (\lambda t)^2
\end{align}</p>
<p>And now we have to do some sketchy algebra to make things work out:</p>
<p>\begin{align}
\sigma^2 &amp;= E[(n-\lambda t)^2] \\
          &amp;= E[n^2 - 2n\lambda t + (\lambda t)^2] \\
          &amp;= E[n^2 - n - 2n\lambda t + n + (\lambda t)^2] \\
          &amp;= E[(n^2 - n) - 2n\lambda t + n + (\lambda t)^2] \\
          &amp;= E[n^2 - n] - E[2n\lambda t] + E[n] + E[(\lambda t)^2] \\
          &amp;= (\lambda t)^2 - 2(\lambda t)(\lambda t) + \lambda t + (\lambda t)^2 \\
          &amp;= \lambda t
\end{align}</p>
<p>…which means ($ \sigma = \sqrt{\lambda t} $).</p>
<p>Seems like magic.</p>
<hr>
<p>Okay, fine, we have this fancy function to model these very specific
probabilities about fish-jump-counts over time intervals. But the kids watching
the fish ladder don’t care! They want to know what’s <em>important:</em> “how long do
I need to wait until the next fish jumps?”</p>
<p>Little do they know, this question opens up a whole new can of worms…</p>
<p>Until now, we’ve been playing with ($ n $) as our random variable, with  ($ t
$) fixed. Now, we need to start exploring what happens if ($ t $) is the random
variable. This needs some new ideas.</p>
<p>Let’s start with an easier question to answer. What is the probability that you
need to wait longer than five minutes (300 seconds) to see a fish-jump? (Five
minutes is <em>way</em> longer than <em>my</em> attention span when looking at fish. But
whatever.)</p>
<p>It turns out that we already know how to answer that question. We know the
probability that <em>no</em> fish jump in five minutes: that’s equal to ($ p_{300}(0)
$). Why? Well, when we plug in ($ x = 0 $), all the ($ x $) terms go away in
the series representation, and we’re only left with (the coefficient of) the ($
x^0 $) term, which is what we want.</p>
<p>Long story short, the probability that you need to wait longer than five
minutes is ($ e^{0.0167\times300(0-1)} = 0.00674 $). This means that the
probability that you <em>will</em> see a fish-jump in the next five minutes is ($ 1 -
e^{0.0167\times300(0-1)} $), which is around 0.9932. This is the probability
that you have to wait <em>less</em> than five minutes to see a fish-jump. For an
arbitrary time interval ($ T $), we have ($ P(t&lt;T) = 1 - e^{-\lambda T} $),
where ($ t $) is the actual time you have to wait.</p>
<p>Sanity check time! This gets close to 1 as ($ T $) gets higher, which sounds
about right: the longer you’re willing to wait, the likelier it is that you’ll
see a fish jump.  Similarly, if fish jump at a higher rate, ($ \lambda $) goes
up, and the probability gets closer to 1, which makes sense. Indeed,
encouragingly enough, this equation looks very close to the equation we use for
half-lives and exponential radioactive decay…</p>
<p>Now things are going to get a bit hairy. What is the probability that you have
to wait <em>exactly</em> ($ T $), that is, ($ P(t = T) $)? This should be zero:
nothing happens in no time. But let’s be reasonable: when we say “exactly” ($
T $), we really mean a tiny window between, say, ($ T $) and ($ T + dT $) where
($ dt $) is a small amount of time, say, a millisecond.</p>
<p>The question then is, what is ($ P(T &lt; t &lt; T + dt) $), which isn’t too hard to
answer: it’s just ($ P(t &lt; T + dt) - P(t &lt; T) $), that is, you need to wait
<em>more</em> than ($ T $) but <em>less</em> than ($ T + dT $). In other words,</p>
<p>\[
P(t \approx T, dT) = P(t &lt; T+dT) - P(t &lt; T)
\]</p>
<p>where ($ dt $) is an “acceptable margin of error”.</p>
<p>This looks <em>awfully</em> like a derivative! We’re expressing the <em>change</em> in
probability as a function of <em>change</em> in time: if I wait ($ dT $) longer, how
much likelier am I to see a fish-jump?</p>
<p>Let’s rewrite our above equation to take advantage of the derivativey-ness of
this situation.</p>
<p>\begin{align}
P(t \approx T, dT) &amp;= \left(\frac{P(t &lt; T+dT) - P(t &lt; T)}{dT}\right)dT\\
                    &amp;= \left(\frac{d P(t &lt; T)}{dT}\right)dT \\
                    &amp;= \left(\frac{d (1-e^{-\lambda T})}{dT}\right)dT \\
                    &amp;= \lambda e^{-\lambda T} dT
\end{align}</p>
<p>By the way, this might give a simpler-but-slightly-less-satisfying answer to
our initial question, “what is the probability that a fish jumps out <em>right
now?</em>“ If we set ($ T $) to 0, then we get ($ P(t \approx 0, dT) = \lambda dT
$). In other words, if fish jump out of the water at a rate ($ \lambda $),
then for a tiny period of time ($ dT $), the probability of seeing a fish jump
in that time is ($ \lambda dT $). This is one of those facts that seems really
straightforward one day, and completely mindblowing the next day.</p>
<p>Anyway. Now that we have an approximation for the probability that you need to
wait a specific time ($ T $), we can find an expected value for ($ t $) by
taking the sum over discrete increments of ($ dt $):</p>
<p>\[
E[t] = \sum^\infty_{k=0} P(t \approx T, dT) \times T
\]</p>
<p>where ($ T = k\times dT $). Since we’re talking about the limit as ($ dT $)
gets smaller and smaller, it seems reasonable to assume that this thing turns
into</p>
<p>\begin{align}
E[t] &amp;= \int^\infty_0 P(t \approx T, dT) \times T \\
     &amp;= \int^\infty_0 \lambda e^{-\lambda T} dT \times T
\end{align}</p>
<p>You can integrate that by parts, or just use WolframAlpha, which tells you that
($ E[t] = \lambda^{-1} $).</p>
<hr>
<p>…which is kind of obvious, isn’t it? Remember that ($ \lambda $) was the
rate at which our fish jumped. If fish jump once a minute, shouldn’t we expect
to have to wait a minute to see a fish jump? Isn’t this similar to the way
wavelength and frequency are related?</p>
<p>The answer is, “yes and no”. “Yes”, the value ($ \lambda^{-1} $) is indeed
pretty sensible in retrospect. A simpler way to derive it might have been to
note that for any time period ($ T $), the expected number of fish-jumps is ($
\lambda T $) (as we found out above), and so the <em>average</em> time interval
<em>between</em> fish-jumps would be ($ T / (\lambda T) = \lambda^{-1} $). The fact
that the average interval between fish-jumps corresponds to the the expected
interval is captured by the surprisingly well-known acronym “PASTA”: Poisson
Arrivals See Time Averages (I’m not making this up!).</p>
<p>But “no”, it’s not “obvious” that you should have to <em>wait</em> the average
inter-fish time!</p>
<p>Suppose you, like Rip Van Winkle, you woke up after a very long sleep, and you
wanted to know “how much longer until Monday morning?”</p>
<p>Well, Monday mornings happen every 7 days, and so if you set ($ \lambda = 1/7
$), you should expect to have to wait 7 days until Monday.</p>
<p>But that’s silly! You <em>definitely</em> need to wait fewer than 7 days on average!
In fact, most people would intuitively say that you need to wait 7/2 = 3.5 days
on average: and they would be right. (The intuition is that on average, you’d
wake up halfway between two Monday mornings.)</p>
<p>This is the so-called “Hitchhiker’s Paradox”: if cars on a highway through the
desert appear roughly once an hour, how long does a hitchhiker who just woke up
need to wait until he sees a car? It seems reasonable to say “half an hour”,
since on average, you’d wake up halfway between two cars. On the other hand,
with ($ \lambda = 1 $), you’d expect to wait an hour until you see a car.</p>
<p>So which one is right? And why are the answers different?</p>
<p>Well, the “Rip Van Winkle” interpretation assumes that cars on a desert highway
— like Mondays — come at <em>regular</em> intervals. In reality, cars on a desert
highway — like the salmon of Seattle — are usually independent. They might
come in a cluster a few minutes after you wake up, or a lone car might come the
next day. Crucially, the next car doesn’t “know” anything about previous cars,
and so it doesn’t matter <em>when</em> you wake up: we call this property
“memorylessness”.</p>
<p>It turns out that since there’s a nonzero probability of having to wait a
<em>very</em> long time for a car, the average gets pulled up from half an hour. With
that in mind, it’s really quite surprising that the true mean turns out to be
exactly ($ 1/\lambda $).</p>
<hr>
<p>And now, the aftermath.</p>
<p>Very little of the above discussion was fish-specific. The only properties of
salmon that mattered here were that salmon jump randomly and independently of
each other, at some rate ($ \lambda $). But our calculations work for <em>any</em>
such process (let’s call such processes <em>Poisson processes</em>).</p>
<p>Poisson processes were studied as early as 1711 by de Moivre, who came up with
the cool theorem about complex numbers. However, they’re named after Siméon
Denis Poisson, who in 1837 studied (not fish, but) the number of wrongful
convictions in court cases.</p>
<p>Today, Poisson processes model all sorts of things. Managers use it to model
customers arriving at a grocery checkout. Programmers use it to model packets
coming into a network. Both of these are examples of <em>queueing theory</em>, wherein
Little’s Law relates ($ \lambda $) to how long things have to wait in queues.
You could probably use a Poisson process to model how frequently bad things
happen to good people, and use that to create a statistical model of how unfair
the world is.</p>
<p>The upshot is this: even though I didn’t record any fish-jumping data back in
Seattle, we can definitely try out these ideas on other “sporadic” processes.
Wikipedia, it turns out, maintains <a href="https://en.wikipedia.org/wiki/List_of_21st-century_earthquakes">a list of earthquakes that happened in the
21st century</a>.
Earthquakes are pretty sporadic, so let’s play with that dataset.</p>
<p>I <a href="static/poisson/earthquakes.txt">scraped</a> the date of each earthquake, and
wrote a small <a href="static/poisson/earthquakes.py">script</a> to count the the number
of earthquakes in each month-long interval. That is, ($ t $) is 2,592,000
seconds. By “binning” my data by month, I got lots of samples of ($ n $). This
gives an easy way to compute ($ P(n, t) $) “empirically”.</p>
<p>On the other hand, taking the total number of earthquakes and dividing by the
total time range (around 17 years, since we’re in 2017) gives us the rate ($
\lambda $), which in this case works out to about ($ 1.06\times10^{-6} $)
earthquakes per second. This gives a way to compute ($ P(n, t) $)
“theoretically” by using our fancy formula with the factorial and whatnot.</p>
<p>\[
P(n, t) = \frac{\left(\lambda t\right)^n}{e^{\lambda t} n!}
\]</p>
<p>Comparing the results gives us this pretty plot!</p>
<p><img src="static/poisson/earthquakes.png" alt="Earthquake poisson process"></p>
<p>They match up surprisingly well.</p>
<p>What else can we say? Well, the average inter-earthquake time works out to ($
1/\lambda $), or around 940,000 seconds. That’s about eleven days. On average,
a reader of this blog post can expect to wait eleven days until the next
earthquake of magnitude 7 or above hits.</p>
<p>And for those of you who have been wondering, “can we do these calculations in
reverse to approximate ($ e $)?” the answer is, <em>yes!</em> We just solve the above
equation for ($ e $).</p>
<p>\[
e\approx\left(\frac{P(n, t)n!}{(\lambda t)^n}\right)^{-(\lambda t)^{-1}}
\]</p>
<p>In my case, using earthquake data for ($ n = 1 $), I got ($ e \approx 2.75 $).
I’d say that’s pretty good for an algorithm that relies on <em>geology</em> for
accuracy (in reality, ($ e $) is around 2.718).</p>
<hr>
<p>In many ways, it is quite incredible that the Poisson process conditions —
randomness, independence, constant rate — are <em>all</em> you need to derive
conclusions for <em>any</em> Poisson process. Knowing roughly that customers at a
burger place are random, act independently, and arrive around once a minute at
lunchtime — and knowing nothing else — we can predict the probability that
four customers arrive in the next three minutes. And, magically, this
probability will have ($ e $) and a factorial in it.</p>
<p>Humans don’t evaluate expressions involving ($ e $) and factorials when they
decide when to get a burger. They are subject to the immense complexity of
human life, much like how salmon are subject to the immense complexity of the
fluid mechanics that govern Lake Washington, much like how earthquakes are
subject to the immense complexity of plate tectonics.</p>
<p>And yet, somehow statistics unites these vastly different complexities, finding
order and meaning in what is otherwise little more than chaos.</p>
<p>Isn’t that exciting?</p>
<p>~ Fin. ~</p>
<hr>
<p>Assorted references below.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Poisson_distribution">https://en.wikipedia.org/wiki/Poisson_distribution</a></li>
<li><a href="https://en.wikipedia.org/wiki/Exponential_field">https://en.wikipedia.org/wiki/Exponential_field</a></li>
<li><a href="https://www.stat.auckland.ac.nz/~fewster/325/notes/ch4.pdf">https://www.stat.auckland.ac.nz/~fewster/325/notes/ch4.pdf</a></li>
<li><a href="http://pages.cs.wisc.edu/~dsmyers/cs547/lecture_11_pasta.pdf">http://pages.cs.wisc.edu/~dsmyers/cs547/lecture_11_pasta.pdf</a></li>
<li><a href="https://www.netlab.tkk.fi/opetus/s383143/kalvot/E_poisson.pdf">https://www.netlab.tkk.fi/opetus/s383143/kalvot/E_poisson.pdf</a></li>
</ul>
<hr>
<p><strong>Postscript, two weeks later.</strong> This morning at the coffee shop I realized
that the Poisson distribution is a lot like the binomial distribution with a
<em>lot</em> of trials: the idea is that you have lots of little increments of time,
and a fish either jumps or doesn’t jump in each increment — this is called a
Bernoulli process. Presumably, over a long period of time, this should even out
to a Poisson process…</p>
<p>Recall that the probability of a fish-jump happening in some small time period
($ dt $) turned out to be ($ \lambda dt $) for our definition of ($ \lambda $)
as the <em>rate</em> of fish-jumps. Can we go the other way, and show that if the
probability of something happening is ($ \lambda dt $) for a small period of
time ($ dt $), then it happens at a rate of ($ \lambda $)?</p>
<p>Turns out, yes!</p>
<p>The <em>binomial distribution</em> is a way to figure out, say, what the probability
is that if I flip 100 counts, then exactly 29 of them land “heads” (a coin toss
is another example of a Bernoulli process). More abstractly, the binomial
distribution gives you the probability ($ B(N, k) $) that if something has
probability ($ p $) of happening, then it happens ($ k $) times out of ($ N $)
trials.</p>
<p>The formula for ($ B(N, k) $) can be derived pretty easily, and you can find
very good explanations in a lot of high-school textbooks. So, if you don’t
mind, I’m just going to give it to you for the sake of brevity:</p>
<p>\[
B(N, k) = \binom{N}{k} p^k (1-p)^{N-k}
\]</p>
<p>Now, can we apply this to a Poisson process? Well, let’s say ($ k = n $), the
number of times our event happens in time ($ t $). Then we have</p>
<p>\[
\binom{N}{n} p^n (1-p)^{N-n}
\]</p>
<p>What next? We know that ($ p = \lambda dt $). Also, for time period ($ t $),
there are ($ t / dt $) intervals of ($ dt $), so ($ N = t / dt $). That means
we can substitute ($ dt = t / N $), and thus ($ p = \lambda (t / N) $). This
gives us</p>
<p>\[
\binom{N}{n} (\lambda t / N)^n (1-\lambda t / N)^{N-n}
\]</p>
<p>Oh, and of course to approximate a Poisson process, this is the limit as ($ N
$) approaches infinity:</p>
<p>\[
\lim_{N\to\infty} \binom{N}{n} (\lambda t / N)^n (1-\lambda t / N)^{N-n}
\]</p>
<p>This isn’t a hard limit to take if we break apart the product.</p>
<p>\[
\lim_{N\to\infty} \frac{N! (\lambda t)^n}{n!(N-n)! N^n}
\lim_{N\to\infty}(1-\lambda (t / N))^{N-n}
\]</p>
<p>The right half is surprisingly enough the definition of ($ e^{-\lambda t} $),
since the ($ - n $) in the exponent doesn’t really matter. The left half is
trickier: it turns out that ($ N! / (N-n)! $) is the product ($
N(N-1)\ldots(N-n+1) $). As a polynomial, it is degree ($ n $), and the leading
term is ($ N^n $). But look!  In the denominator, we have an ($ N^n $) term as
well, so in the limit, those both go away.</p>
<p>We’re left with what simplifies to our expression for the Poisson distribution.</p>
<p>\begin{align}
\lim_{dt\to 0} B(N=t/dt, p=\lambda dt) &amp;= \frac{(\lambda t)^n}{n!}e^{-\lambda t} \\
                                          &amp;= \frac{(\lambda t)^n}{e^{\lambda t}n!} \\
                                          &amp;= P(\lambda, t)
\end{align}</p>
<p>which I think is literally magic.</p>
]]></description>
            <link>http://hardmath123.github.io/poisson.html</link>
            <guid isPermaLink="true">http://hardmath123.github.io/poisson.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Sun, 16 Jul 2017 07:00:00 GMT</pubDate>
        </item>
    </channel>
</rss>