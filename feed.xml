<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Comfortably Numbered]]></title>
        <description><![CDATA[My blog.]]></description>
        <link>https://hardmath123.github.io</link>
        <image>
            <url>https://hardmath123.github.io/static/avatar.png</url>
            <title>Comfortably Numbered</title>
            <link>https://hardmath123.github.io</link>
        </image>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 13 May 2020 05:02:57 GMT</lastBuildDate>
        <atom:link href="https://hardmath123.github.io/feed.xml" rel="self" type="application/rss+xml"/>
        <author><![CDATA[Hardmath123]]></author>
        <language><![CDATA[en]]></language>
        <item>
            <title><![CDATA[Conway's Gradient of Life]]></title>
            <description><![CDATA[<p>Approximate Atavising with Differentiable Automata</p>


<p>And now, a magic trick. Before you is a 239-by-200 Conway’s Game of Life board:</p>
<p><img src="static/conways-gradient/conway-out.png" alt="a life config"></p>
<p>What happens if we start from this configuration and take a single step of the
game? Here is a snapshot of the next generation:</p>
<p><img src="static/conways-gradient/conway-out-step.png" alt="a life config, stepped"></p>
<p>Amazing! It’s a <a href="https://mancala.fandom.com/wiki/John_Horton_Conway">portrait</a>
of John Conway! (Squint!)</p>
<hr>
<p>How does this trick work? (It’s not a hoax — you can try it yourself at
<a href="https://copy.sh/life/">copy.sh/life</a> using <a href="static/conways-gradient/conway.rle">this RLE
file</a>.)</p>
<p>Well, let’s start with how it <em>doesn’t</em> work. Reversing Life configurations
exactly — the art of “atavising” — is <a href="https://nbickford.wordpress.com/2012/04/15/reversing-the-game-of-life-for-fun-and-profit/">a hard search
problem</a>,
and doing it at this scale would be computationally infeasible. Imagine
searching through ($ 2^{239\times200} $) possible configurations! Surely that
is hopeless… indeed, the talk I linked shares some clever algorithms that
nonetheless take a full 10 minutes to find a predecessor for a tiny 10-by-10
board.</p>
<p>But it turns out that <em>approximately</em> reversing a Life configuration is much
easier — instead of a tricky discrete search problem, we have an easy
<em>continuous optimization</em> problem for which we can use our favorite algorithm,
gradient descent.</p>
<p>Here is the idea: Start with a random board configuration ($ b $) and compute
($ b^\prime $), the result after one step of Life. Now, for some target image
($ t $) compute the derivative ($ \frac{\partial}{\partial b} \sum|b^\prime-t|
$), which tells us how to change ($ b $) to make ($ b^\prime $) closer to ($ t
$). Then take a step of gradient descent, and rinse and repeat!</p>
<p>Okay, okay, I know what you’re thinking: <em>Life isn’t differentiable!</em> You’re
right. Life is played on a grid of bools, and there is no way the map ($ b
\mapsto b^\prime $) is continuous, let alone differentiable.</p>
<p>But suppose we could make a “best effort” differentiable analogue? Let us play
Life on a grid of real numbers that are 0 for “dead” cells and 1 for “live”
cells. Can we “implement” a step of Life using only differentiable operations?
Let’s try.</p>
<p>We will look at each cell ($ c $) individually. The first step is to count our
live neighbors. Well, if “live” cells are 1 and “dead” cells are 0, then we can
simply add up the values of all 8 of our neighbors to get our live neighbor
count ($ n $). Indeed, if we wanted to be clever, we could implement this
efficiently as a convolution with this kernel:</p>
<p>\[
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1
\end{bmatrix}
\]</p>
<p>Good, so we know ($ n $) and of course our own 0/1 value ($ c $). The next step
is to figure out whether this cell will be alive in the next generation. Here
are the rules:</p>
<ol>
<li>If the cell is alive, ($ c = 1 $), then it stays alive if and only if ($ n =
2 $) or ($ n = 3 $).</li>
<li>If the cell is dead, ($ c = 0 $), then it comes to life if and only if ($ n
= 3 $).</li>
</ol>
<p>Let us approach (2) first. What function is 1 when ($ n = 3 $) and 0 otherwise?
The “spike-at-3” function, of course. That’s not differentiable, but a narrow
Gaussian centered at 3 is <em>almost</em> the same thing! If scaled appropriately, the
Gaussian is one at input 3, and <em>almost</em> zero everywhere else. See this graph:</p>
<p><img src="static/conways-gradient/approx-graph.png" alt="approximating a spike with a less spiky
spike"></p>
<p>Similarly, for (1) an appropriately-scaled Gaussian centered at 2.5 gets the
job done. Finally, we can mux these two cases under gate ($ c $) by simple
linear interpolation. Because ($ c \approx 0 $) or ($ c \approx 1 $), we know
that ($ ca + (1-c)b $) is just like writing <code>if c then a else b</code>.</p>
<p>That’s it! We now have a differentiable function, which looks like this:</p>
<p><img src="static/conways-gradient/approx-map.png" alt="the same, in 2d"></p>
<p>Note that it’s worth “clamping” the output of that function to 0 or 1 using,
say, the tanh function. That way cell values always end up close to 0 or 1.</p>
<p>Okay, great! Now all that’s left to do is to write this in PyTorch and call
.backward()…</p>
<p>…and it begins to learn, within seconds! Here is a GIF of ($ b $) at every
100 steps of gradient descent.</p>
<p><img src="static/conways-gradient/learn.gif" alt="gif of it learning"></p>
<p>(Note: This GIF is where the target image ($ t $) is an all-white rectangle; my
janky PyTorch gradient descent finds sparse configurations that give birth to
an “overpopulated” field where nearly 90% of cells are alive.)</p>
<hr>
<p>Looking at that GIF, the beautiful labyrinthine patterns reminded me of
something seemingly unrelated: the skin of a giant pufferfish. Here’s a picture
from Wikipedia:</p>
<p><img src="static/conways-gradient/pufferfish.jpg" alt="pufferfish"></p>
<p>And here’s a zoomed-in 100-by-100 section of the finished product from above:</p>
<p><img src="static/conways-gradient/seed.png" alt="pufferfish in Life?"></p>
<p>Patterns like the one on the pufferfish come about as a result of
“symmetry-breaking,” when small perturbations disturb an unstable homogeneous
state. The ideas were first described by Alan Turing (and thus the patterns are
called <a href="https://en.wikipedia.org/wiki/Turing_pattern">Turing Patterns</a>). Here’s
his <a href="http://www.dna.caltech.edu/courses/cs191/paperscs191/turing.pdf">1952
paper</a>.</p>
<p>I can’t help but wonder if there’s a reaction-diffusion-model-esque effect at
work here as well, the symmetry being broken by the random initialization of ($
b $). If that’s the case, it would create quite a wonderful connection between
cells, cellular automata, Turing-completeness, and Turing patterns…</p>
<p>(Curious? <a href="static/conways-gradient/atavise.py">Here</a> is the Python program I
used to play these games. Enjoy!)</p>
]]></description>
            <link>https://hardmath123.github.io/conways-gradient.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/conways-gradient.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 05 May 2020 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Spaghetti's Shadows are Spirali-Shaped]]></title>
            <description><![CDATA[<p>Reasoning about the PCA projections of trajectories in high-dimensional spaces</p>


<p>Suppose you took a walk in a high-dimensional vector space, leaving behind
breadcrumbs. If you wanted to visualize your route, you might consider
projecting the breadcrumbs down to a lower-dimensional space.</p>
<p>One way to do this is to project the breadcrumbs onto three randomly-chosen
vectors. Then you might get something like this.</p>
<p><img src="static/spaghetti-shadows/spaghetti.png" alt="spaghetti"></p>
<p>But perhaps a better way to choose the projection is to do PCA on the
breadcrumbs as if they were any other point cloud. If you do this, you will
find that your walk was extremely loopy and spirally. Here’s an example of the
same trajectory, projected down to 3D with PCA (this data is taken from my
research work, but even a random walk will show the phenomena I discuss in this
post).</p>
<p><img src="static/spaghetti-shadows/loopy.png" alt="loops"></p>
<p>Looking at this, you might be concerned that you are walking in circles.</p>
<p>This, for example, was the situation with a <a href="https://icmlviz.github.io/icmlviz2016/assets/papers/24.pdf">2016 ICML
paper</a>,
“Visualizing Deep Network Training Trajectories,” that tried to visualize the
trajectory of neural network parameters over the course of training by doing
exactly what I described: PCA on “breadcrumb” snapshots of the parameters. The
author found some strange, hard-to-explain oscillatory behavior; even Lissajous
curves.</p>
<p><img src="static/spaghetti-shadows/nn.png" alt="loops with NNs"></p>
<p>A similar thing, by the way, happened with a 1994 text on genetics, “The
History and Geography of Human Genes” by Cavalli-Sforza et al, which applied
PCA to genetic data and found periodic effects.</p>
<p>It turns out however that there’s something deeper going on here, something
independent of the individual datasets: these loopy patterns seem to be “just
what happens” when you do PCA on such data. A <a href="https://arxiv.org/pdf/1806.08805.pdf">2018 NeurIPS
paper</a>, “PCA of high dimensional random
walks with comparison to neural network training” by Antognini and
Sohl-Dickstein, explains what’s going on with the neural networks. A <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3989108/">2008
paper</a>, “Interpreting
principal component analyses of spatial population genetic variation” by
Novembre and Stephens, explains what’s going on with genetics.</p>
<p>Here’s my understanding of the story, reconstructed in part from <a href="https://math.stackexchange.com/questions/1391701/principal-component-analysis-pca-results-in-sinusoids-what-is-the-underlying/1392332">this Stack
Exchange
answer</a>.</p>
<p>Recall how PCA works: it tries to come up with orthogonal axes that eat up as
much of the variance of the data as possible. One way to think about this is
that it tries to come up with a rotation (“orthogonal transformation”) of the
data that de-correlates the variance over each axis. In other words, it finds a
way to diagonalize the covariance matrix of the dataset.</p>
<p>Okay, in symbols: if ($ X $) is the matrix whose rows are data points and
columns are fields, and if your data is “centered” at the origin, then ($ X^TX
$) is the covariance matrix whose entries give the covariance between pairs of
fields. If ($ E $) is the transformation, then the goal is to select ($ E $)
such that ($ (XE)^T(XE) = E^T(X^TX)E $) is a diagonal matrix; that is, ($ E $)
diagonalizes ($ X^TX $). Having done that, our projected path will be the
(first few columns of) the matrix ($ XE $).</p>
<p>It turns out that ($ XE $) diagonalizes ($ X X^T $). You can see that by just
writing it out: ($ (XE)^T XX^T (XE) = E^T (X^TX)^2 E $). Of course an
eigenvalue of ($ A $) is also an eigenvalue of ($ A^2 $) and therefore ($ E $)
diagonalizes ($ (X^TX)^2 $) as desired. So, it suffices to study the
eigenvalues of ($ XX^T $) to learn what the projected path will look like.</p>
<p>Now let’s look carefully at ($ X X^T$), which is the “covariance” for each step
rather than for each field. The claim is that this matrix is more-or-less
<em>Toeplitz</em>, which means each row is the previous row but shifted by one spot to
the right. The reason for this is that adjacent data points are spatially
nearby and should have similar correlations. One way to think about this is
that the covariances are just dot-products of pairs of points, and the dot
product of points 1 and 2 should be about the same as the dot product of points
2 and 3 if your steps are approximately evenly-spaced.</p>
<p>Indeed this seems to be the case for the loopy path I showed above:</p>
<p><img src="static/spaghetti-shadows/toeplitz.png" alt="Toeplitz matrix"></p>
<p>If the matrix is truly approximately <em>Toeplitz</em>, then multiplying by it is a
convolution by the first row (just look at its structure!). But as we were all
taught in signal-processing class, convolution is the same as pointwise
multiplication in the Fourier domain. Okay, so what are the “eigenvectors” of
pointwise multiplication?  One-hot vectors, of course! (Unless your convolution
happens to have two frequencies with exactly the same amplitude, in which case
you can get some mixing.) Finally, taking the inverse Fourier transform of
these one-hot vectors, we recover sinusoidal eigenvectors. Here’s a plot of the
top 3 eigenvectors for my running example:</p>
<p><img src="static/spaghetti-shadows/eig.png" alt="eigenvectors"></p>
<p>To recap: beware of periodic structures in PCA projections! PCA on
high-dimensional trajectories gives loopy projections because their covariance
matrices tend to be Toeplitz, yielding sinusoidal eigenvectors. Or,
high-dimensional spaghetti casts spirali-shaped shadows!</p>
<hr>
<p>Here’s a bit of Python code to explore this phenomenon further on a random
walk of 200 steps in 1024-dimensional space.</p>
<pre><code>import numpy as np
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# build a path and center it
position = np.zeros(1024)
X = []
for i in range(200):
    position = position + np.random.randn(1024)
    X.append(position)
X = np.array(X)
scaler = StandardScaler(copy=False)
scaler.fit(X)
X = scaler.transform(X)

# plot the PCA projection on some components
p = PCA(n_components=5)
p.fit(X)
fig = plt.figure()
ax = fig.add_subplot(111, projection=&#39;3d&#39;)
ax.plot(
    p.transform(X)[:, 0],
    p.transform(X)[:, 3],
    p.transform(X)[:, 4]
)

# plot the Toeplitz matrix
plt.figure()
plt.imshow(np.dot(X, X.transpose()))

# plot the eigenvalues of the Toeplitz matrix
plt.figure()
plt.plot(np.linalg.eig(np.cov(X))[1][:, :4])
</code></pre>]]></description>
            <link>https://hardmath123.github.io/spaghetti-shadows.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/spaghetti-shadows.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Thu, 26 Mar 2020 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Dendrological Dexterity]]></title>
            <description><![CDATA[<p>Thoughts on the chirality of the wood grain in eucalyptus trees</p>


<p>It’s Christmastime, shall we talk about trees?</p>
<p>I was reading about Knuth’s <a href="https://www-cs-faculty.stanford.edu/~knuth/fant.html">Fantasia
Apocalypta</a>, and his
description of the “style” he wrote it in contains the wonderfully Knuthian
line “And of course a musical work on the Apocalypse should also contain
calypso.” Naturally I had to look up whether or not this was a linguistic
coincidence, and — in an odd turn of events, since these things usually tend
to work out — it turns out that <em>apocalypse</em> and <em>calypso</em> are <em>not</em>
etymologically related.  <em>Apocalypse</em> comes from the Greek root <em>kaluptein,</em>
“to cover,” and so the <em>apocalypse</em> is the “uncovering” or “revelation” (the
root <em>kel</em> is related to <em>hell</em>). The word <em>calypso</em> is of unknown origin, but
from the West Indies (this is distinct from <em>Calypso,</em> the Greek mythological
nymph, whose name is indeed related to “covered,” or “concealing”).</p>
<p>I shared this with a good friend of mine and she immediately pointed out that
we should check out the word <em>eucalyptus</em>. She was right: <em>eucalyptus</em> means
“well-covered,” referring (not ironically to the bark as I originally thought
but rather) to the calyx, which <a href="http://www.microscopy-uk.org.uk/mag/indexmag.html?http://www.microscopy-uk.org.uk/mag/artoct07/bj-eucalyptus.html">forms a lid over flowers when in
bud</a>.</p>
<p>That got me thinking about eucalyptuses…</p>
<hr>
<p>Lately I’ve spent a lot of time around the eucalyptuses in <a href="https://trees.stanford.edu/ENCYC/EUCnotes.htm">Toyon and Arboretum
Groves</a> at Stanford and I’ve
noticed that the peeling bark of what I’m 80% sure is the <a href="https://trees.stanford.edu/ENCYC/EUCglo.htm"><em>Eucalyptus
globulus</em></a> (Tasmanian Blue Gum)
seems to climb up the tree in a whirling, helical pattern. I didn’t have the
presence of mind to take a picture at the time but here’s a picture <a href="https://commons.wikimedia.org/wiki/Eucalyptus_globulus#/media/File:Starr_031002-0027_Eucalyptus_globulus.jpg">from
Wikimedia
Commons</a>
taken in Maui.</p>
<p><img src="static/eucalyptus-grove.jpg" alt="lots of whooshing eucalyptuses"></p>
<p>Really it’s just beautiful to me, the whorl reaching aspirationally towards the
sky. I’m reminded of Correggio’s <em>Ganymede Abducted by the Eagle</em>, the way the
dog and the tree stump look up together, moving, turning even while frozen in
the painting.</p>
<p><img src="static/correggio.jpg" alt="correggio&#39;s abduction of ganymede"></p>
<p>This much would be believable, just another quirk of nature. But here’s the
truly odd thing: most of the trees seem to grow according to a thumbs-up
right-hand rule; if you were to try and screw them into the ground you would
turn them righty-tighty. <em>So eucalyptuses have some kind of chirality?</em> I
rubbed my eyes, I thought I was dreaming.</p>
<p>But indeed, some hours of study later, I learned that the “spiral grain” is a
real thing in the study of wood formation.</p>
<hr>
<p>In the Real World of economics spiral grain is considered a defect because it
reduces the structural quality of wood. But let us set that aside for a moment
and marvel at the phenomenon itself. I turn to Dr. John Maddern Harris’
wonderfully comprehensive 1989 text <em>Spiral Grain and Wave Phenomena in Wood
Formation</em> (thanks Chandler for finding me a copy!) for details. Harris, citing
confusion about the convention in prior work, carefully defines LH (left-handed
“sinistral”) and RH (right-handed “dextral”) spirals (I wonder if he was aware
of the right-hand-rule convention from E&amp;M, which would simplify matters here).
Perhaps more entertainingly Harris introduces the German words as well:</p>
<blockquote>
<p>Numerous other expressions have been used to describe spiral direction,
reflecting no doubt the notorious difficulty of so doing without waving of
arms and twisting of wrists. In the German literature the terms “sonnig” and
“wider- sonnig” are frequently encountered. Their meaning is “with the sun”
and “against the sun”, and hence (in the Northern Hemisphere!) they relate to
LH and RH spiral grain respectively.</p>
</blockquote>
<p>Here’s a picture of a eucalyptus from Harris’ book.</p>
<p><img src="static/eucalyptus-spiral.png" alt="spiral-grained eucalyptus"></p>
<p>Certain species do indeed show predilections for certain handednesses.
Furthermore, certain species <em>change</em> handedness over the course of their
lives. In Harald Säll 2002 doctoral thesis <a href="https://www.diva-portal.org/smash/get/diva2:206846/FULLTEXT01.pdf">Spiral Grain in Norway
Spruce</a> I
learned the remarkable fact that the Norway Spruce begins life as a left-handed
spiral grain, but over the decades transitions to a right-handed spiral grain.
The drawing below is from his thesis:</p>
<p><img src="static/spruce-spiral.png" alt="spiral grain reversal in spruce trees"></p>
<p><a href="https://www.conifers.org/topics/spiral_grain.php">We don’t fully understand why trees
spiral</a>, much less how they
pick a direction. The most fanciful explanation I’ve come across — the one I
would like to believe — is that there is some kind of Coriolis effect at
work. But I will admit that both handednesses have been observed in both
hemispheres, which weakens that theory.</p>
<p>At the moment I’m on a quest to find a (translated, ideally) copy of Meyer’s
1949 article <em>Sprachliche und literarische Bemerkungen zum Problem “Drehwuchs”</em>
published in <em>Mitt Schweiz Centralanst Forst Veruschswes</em> (26:331-347) to learn
more about the linguistic history of chirality in relation to spiral grains.
Until then…</p>
<p><strong>Open question:</strong> What’s going on here, why are eucalyptuses right-handed?</p>
<p><strong>Further reading:</strong> Noah Rosenberg’s
<a href="https://xtrees.sites.stanford.edu/tree-gallery">Xtrees</a> site about Stanford
trees — I found this while reading about eucalyptuses!</p>
<p>Edited to add (2/19/2020):</p>
<blockquote>
<p>“DNA normally forms a right-handed spiral (although a rare left-handed
variant can occur). In other words, it twists like a conventional screw.
That, though, has not stopped it being reproduced wrongly in hundreds of
places. Dr. Tom Schneider has a website where he has collected hundreds of
examples of incorrectly drawn ‘left-handed DNA’, most being found in
scientific journals. Many are in advertisements, so we may perhaps charitably
suggest the final copy was never seen by a scientist, but that doesn’t quite
explain them all. Certainly not the editorial comment in Nature, the place
where DNA’s structure was first described, which in 2000 mentioned the clues
that ‘led Watson and Crick to deduce the left-handed double helical structure
of DNA.’ Watson, in fact, has been particularly badly served, his 1978
textbook, Molecular Biology of the Gene, having six different illustrations
with left-handed DNA, and in 1990 the American journal Science quoting Watson
as saying, ‘I have to read SCIENCE every week,’ this being illustrated with
left-handed DNA. Perhaps worst of all, a 1998 reprint of Watson’s The Double
Helix was illustrated on the front and back with left-handed DNA. Perhaps it
is not a coincidence that Watson is left-handed.”</p>
<p>— Chris McManus, in Right Hand, Left Hand: The Origins of Asymmetry in
Brains, Bodies, Atoms and Cultures. 2002.</p>
</blockquote>
]]></description>
            <link>https://hardmath123.github.io/eucalyptus.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/eucalyptus.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Wed, 25 Dec 2019 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Dancing Orchards on the I-5]]></title>
            <description><![CDATA[<p>Investigating number-theoretic patterns in large grids of trees</p>


<p>The last time I spent a long time on I-5, I started thinking about
<a href="envelope.html">envelopes</a>. This time, I started thinking about orchards. As
you drive along the central valley highway, you can see thousands and thousands
of perfect rows of trees. You can actually see them from the top on Google’s
<a href="https://www.google.com/maps/@36.6590095,-120.643549,2656a,35y,236.91h/data=!3m1!1e3">satellite
imagery</a>;
try zooming in slowly on that point, it’s really quite beautiful.</p>
<p>When you drive by them, you see the side view rather than the top view. In
fact, you see the side view <em>in motion</em> – the trees look like they’re dancing
an elaborate dance, continuously lining up into different formations to form
different rows in different angles. Really, of course, it is you who is moving;
the trees are still but by viewing them from different angles you can see
different “alignments” of the grid.</p>
<p>The effect is most pronounced with freshly-planted trees (or perhaps those are
markers?), where there is no foliage blocking the horizon.</p>
<p><img src="static/orchard.png" alt="orchard"></p>
<p>I didn’t think to make a video at the time, but you can imagine that as the car
moves forwards a little bit, the pattern re-aligns in an incredibly satisfying
way.</p>
<p>I’m intrigued by the multiple “vanishing points” you can see in that picture.
The trees are regularly spaced in a square grid, but somehow there are multiple
(!) irregularly-spaced (!!) points along the horizon to which rows of trees
appear to converge. What’s going on here — is there a pattern to the
patterns? You could either stop reading here to have food for thought on your
next road trip, or you could keep reading for my ideas.</p>
<hr>
<p>The I-5 trees are a variation on a well-known theme: a theme called <em>Euclid’s
orchard</em> (<a href="https://en.wikipedia.org/wiki/Euclid%27s_orchard">Wikipedia</a>,
<a href="http://mathworld.wolfram.com/EuclidsOrchard.html">MathWorld</a>). You stand at
the origin of the plane, and at every integer point there is a line segment
(“tree”) extending one unit upwards perpendicular to the plane. What you see
from your vantage point is Euclid’s orchard. Try to imagine what it looks like;
then, play with a JavaScript simulation of it
<a href="https://www.khanacademy.org/computer-programming/euclids-orchard/5282060266635264">here</a>.</p>
<p>The main idea of Euclid’s orchard is that trees at integer points (x, y)
occlude trees at any integer multiples of that point (nx, ny), so you don’t see
anything behind them. Also, nearby trees appear taller than farther-away trees
because that’s how perspective works. So there is a “horizon” where in each
direction you either see (1) a tree if the direction’s slope is rational, and
the tree is shorter if its denominator is larger; (2) nothing if the
direction’s slope is irrational. All sorts of fun can be had with this: see for
example a <a href="https://mathlesstraveled.com/2017/07/22/a-few-words-about-pww-20/">nice connection to the Fibonacci
numbers</a>.</p>
<p>The I-5 orchard in the picture is similar, but now we look upon it from some
height. Imagine mapping each point on the horizon to the direction in which I
would have to point my camera to look directly at that point. The “vanishing
points” have lots of trees along those directions, giving the illusion of
“rows.” What does this mean? It means that if you walked along that ray in
Euclid’s orchard, you would run into trees more frequently than other rays. In
other words, those rays represent directions whose slopes are rational with
small denominators.</p>
<p>Hmm… rational with small denominators… where have I blogged about that
before…? Right — around four years ago, when we
<a href="a-balance-of-powers.html">showed</a> that 2^12864326 begins with the digits
“10000000…” (and other number-theoretic shenanigans).  I wasn’t kidding,
rational approximation really is one of my favorite topics!</p>
<p>At the time I showed you this picture with little explanation.</p>
<p><img src="static/ford-circle.png" alt="Ford circle"></p>
<p>Here is a little more explanation of its significance: as you start filling in
circles to some level, you generate a sequence of fractions called the <a href="https://en.wikipedia.org/wiki/Farey_sequence">“Farey
sequence”</a> by listing the points
of tangency of the circle and the X axis. The Farey sequence is the list of
completely reduced fractions whose denominators are smaller than some bound.
Putting those two facts together: the points of tangency of the circles
represent points that are “rational with small denominators” — exactly what
we thought our I-9 orchard’s “vanishing points” were!</p>
<p>Looking back at the image, this is somewhat believable. We see a couple of
“strong” vanishing points, and then “weaker” vanishing points in between the
strong ones, and then <em>even</em> “weaker” vanishing points between the strong and
the weak vanishing points, and so on.</p>
<p>There are a number of cool implications of this. For example, the “least
patterned” regions of the horizon would correspond to regions that are “hard to
rationally approximate”; so, you’d expect them at plus-or-minus the golden
ratio’s slope from your line of sight.</p>
]]></description>
            <link>https://hardmath123.github.io/orchard.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/orchard.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 10 Sep 2019 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Unraveling the Un-unravelable]]></title>
            <description><![CDATA[<p>And some observations about the associativity of English affixes</p>


<p>Say what you will about <em>The Big Bang Theory</em>, it has its moments, and this is
one of my favorites (watch it to the end of the clip!):</p>
<iframe width="560" height="315"
src="https://www.youtube-nocookie.com/embed/MsawieizDeE?start=210"
frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope;
picture-in-picture" allowfullscreen></iframe>

<p>Putting aside the wonderful meta-dramatic irony of this moment, the word
“un-unravelable” is the kind of thing you can sit and think about for hours. I
know because I have.</p>
<p>See, you might think that the “un-“ prefixes cancel, such that “un-unravelable”
means the same thing as “ravelable.” This is kind of like how
“un-unforgettable” probably means “ordinary.” And indeed, <a href="https://en.wiktionary.org/wiki/un-unravelable">Wiktionary
agrees</a> with this assessment,
suggesting instead “ravelable” as the “simpler and more immediately logical
choice.”</p>
<p>But clearly that’s not what Dr. Cooper meant in this case! “Ravelable” means
<em>can be raveled</em> whereas Dr. Cooper meant <em>cannot be unraveled</em>, which are at
least intuitively separate concepts. So what’s going on?</p>
<p>First of all, we need to figure out what “ravel” means: it
<a href="https://www.websters1913.com/words/Ravel">means</a> to entangle. Something that
is “raveled” is entangled and knotted like the HDMI cords behind your cable
box. The sleep that Macbeth murders is the “sleep that knits up the raveled
sleeve of care” (2.2.37). Actually, that’s not the full story. “Ravel” <em>also</em>
means to <em>disentangle</em>, making the word a so-called “Janus word” (read more at
<a href="https://www.merriam-webster.com/words-at-play/words-own-opposites">Merriam-Webster</a>).
But let’s stick with the former definition for the moment.</p>
<p>Okay, so now we can start tacking on prefixes and suffixes. But we have to be
careful!  The dynamics of affixes are tricky — the order in which we tack
them on matters.</p>
<p>For example, “ravel-able” would mean <em>can be entangled,</em> so spaghetti is
ravelable. Then “un-ravelable” could mean <em>cannot be entangled,</em> so ravioli is
unravelable.</p>
<p>On the other hand, “un-ravel” would mean <em>to disentangle,</em> so Sherlock unravels
mysteries. Then “unravel-able” could mean <em>disentanglable,</em> so the mysteries
Sherlock solves are unravelable (by him, at least).</p>
<p>To summarize:</p>
<ul>
<li>un+(ravel+able): there is <em>no</em> way to ravel it</li>
<li>(un+ravel)+able: there <em>is</em> a way to unravel it</li>
</ul>
<p>Alas! The associative property has broken down! This means we need to examine
each of the five possible paranthesisifications of “un+un+ravel+able”
separately (<a href="https://en.wikipedia.org/wiki/Catalan_number">why five?</a>).</p>
<p>Here they are:</p>
<ol>
<li><strong>(un+un)+(ravel+able)</strong> and</li>
<li><strong>((un+un)+ravel)+able</strong> are easy: neither of them typecheck, because
“un+un” is meaningless.</li>
<li><strong>un+(un+(ravel+able)):</strong> “ravelable” means <em>can be entangled,</em> so
“un-ravelable” means <em>cannot be entangled,</em> and thus “un-unravelable”
means <em>is not such that it cannot be entangled,</em> i.e. <em>can be
entangled.</em> For example, your new favorite organizational scheme might seem
perfect at first, but after a few weeks you will find that no scheme can
fend off the entropic tendencies of the universe, and that your files are by
nature un-un-ravelable.</li>
<li><strong>(un+(un+ravel))+able:</strong> “unravel” means <em>disentangle,</em> so “un-unravel”
means <em>re-entangle</em> (by analogy to “un-undo” meaning “redo”) and thus
“ununravel-able” means <em>re-entanglable.</em> For example, a jigsaw puzzle that
you mix up again after solving is ununravel-able.</li>
<li><strong>un+((un+ravel)+able):</strong> “unravel” means <em>disentangle,</em> so “unravel-able”
means <em>can be disentangled,</em> and thus “un-unravelable” means <em>cannot be
disentangled.</em> For example, Dr. Cooper’s web of lies is un-unravelable.</li>
</ol>
<p>So what’s really going on here is that the prefix <em>un-</em> has many faces (a Janus
prefix?); it reverses verbs and negates adjectives. Perhaps this is clearer in
the language of first-order logic, where <em>-able</em> introduces an existential
quantifier over <em>a way to do the thing</em> (and thus do(w, x) does w to x).</p>
<ol>
<li>N/A</li>
<li>N/A</li>
<li><strong>(un+(un+(ravel+able)))(x)</strong> means &not;&not;&exist;w. raveled(do(w, x))</li>
<li><strong>((un+(un+ravel))+able)(x)</strong> means &exist;w. &not;&not;raveled(do(w, x))</li>
<li><strong>(un+((un+ravel)+able))(x)</strong> means &not;&exist;w. &not;raveled(do(w, x))</li>
</ol>
<p>Only (5) puts an odd number of negations outside the existential quantifier,
flipping it to a universal quantifier, and expressing that you <em>cannot</em> unravel
whatever needs unraveling.</p>
<p>Why doesn’t this effect happen with “un-unforgettable”? It’s because “unforget”
isn’t a common word, and so we reach for interpretation (3) rather than (5).
Though now that I think about it, “unforget” is an excellent replacement for
“remember.”</p>
]]></description>
            <link>https://hardmath123.github.io/un-unravelable.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/un-unravelable.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 27 Aug 2019 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Words that do Handstands]]></title>
            <description><![CDATA[<p>Dreaming ambigrams by gradient descent</p>


<p>The other day I ordered a book on art history from Amazon… which reminded me
of the movie <em>Angels and Demons</em>, which reminded me of ambigrams. Ambigrams are
those stylized words that look the same when you turn them upside down. Here is
the classic, <a href="http://www.johnlangdon.net/works/angels-demons/">by John
Langdon</a>.</p>
<p><img src="static/ambigrams/langdon.png" alt="earth air fire water"></p>
<p>I’ve been thinking a lot about the absurd power of gradient descent these days,
and so almost immediately I felt the need to try and synthesize ambigrams
on-demand by gradient descent.</p>
<p>It turns out that this isn’t too hard to implement. Let’s start with just
single-character ambigrams. Here’s the plan: first, we train some sort of
neural net to recognize characters. This training is by gradient descent on the
weights / convolution filter / whatnot; specifically, we imagine a function of
the weights that outputs some loss and take its derivative <em>with respect to the
weights</em> to do gradient descent. Once the neural net is trained, however, we
can just treat it as a single function from tensors (input image) to tensors (a
one-hot encoding of the classification… or some softmaxed approximation).
Taking that one-hot output and converting it into a cross-entropy loss against
some desired output, we can now do gradient descent to minimize this loss <em>with
respect to the input image</em> (keeping the weights constant). This process lets
us “dream” of images representing whichever characters we want.</p>
<p>Similarly, we can do the same thing but with the tensor turned upside-down, to
dream images of upside-down characters. Okay, so now we can just combine these
two processes — literally, by adding the loss functions — to dream an image
that matches one character right-side-up and another character upside-down.
That’s it! Now we let it run all day to generate an image for each alphanumeric
character pair (36^2 = 1296) and we start to get a nice little “ambigram font”:</p>
<p><img src="static/ambigrams/grid.png" alt="grid"></p>
<p>Combining letters from this “font” lets us make arbitrary ambigrams.</p>
<p>This was a low-effort operation, hacked together over an evening and a morning,
plus a full day of “training.” I’m sure it’s possible to do better. Here are
some technical notes for anyone who wants to have a go at it. My code is all
online, <a href="https://github.com/kach/neural-ambigrams">here</a>.</p>
<ul>
<li>It’s all written in PyTorch. The neural net itself is literally PyTorch’s
built-in <a href="https://github.com/pytorch/examples/tree/master/mnist">MNIST
example</a>.</li>
<li>I used the <a href="https://www.nist.gov/node/1298471/emnist-dataset">EMNIST</a>
dataset, which is MNIST that also adds in the alphabets from the NIST
database (and comes with PyTorch). I used the “balanced” split.</li>
<li>I added an L1 regularizer to force the output images to be sparse, i.e.
mostly “blank paper” with some “ink.” Without this, you get very dark images.</li>
</ul>
<p>Okay, now time for some pictures. Some turned out better than others, and there
is plenty of scope for improvement…</p>
<p>P.S. Depending on your browser, images in this post may or may not flip
upside-down automatically if you hover over them.</p>
<style>
img {
  transform: rotate(0deg);
  transition: transform 0.5s;
}

img:hover {
  transform: rotate(180deg);
  transition: transform 0.5s;
}
</style>

<h2 id="angel-and-demon">ANGEL AND DEMON</h2>
<p><img src="static/ambigrams/angel-and-demon.png" alt="ANGEL AND DEMON"></p>
<h2 id="banana">BANANA</h2>
<p><img src="static/ambigrams/banana.png" alt="BANANA"></p>
<h2 id="year-2016">YEAR, 2016</h2>
<p><img src="static/ambigrams/year-2016.png" alt="YEAR, 2016"></p>
<h2 id="good-evil">GOOD, EVIL</h2>
<p><img src="static/ambigrams/good-evil.png" alt="GOOD, EVIL"></p>
<h2 id="teamwork">TEAMWORK</h2>
<p><img src="static/ambigrams/teamwork.png" alt="TEAMWORK"></p>
<h2 id="blues-skate">BLUES, SKATE</h2>
<p><img src="static/ambigrams/blues-skate.png" alt="BLUES, SKATE"></p>
<h2 id="dream-world">DREAM, WORLD</h2>
<p><img src="static/ambigrams/dream-world.png" alt="DREAM, WORLD"></p>
<hr>
<p>Update (3/26/20): Alex Pietrow created <a href="https://makeambigrams.com/ambigram-generator/">a
website</a> that lets you use this
font to build your own ambigrams (along with more ambigram fun).</p>
]]></description>
            <link>https://hardmath123.github.io/ambigrams.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/ambigrams.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Mon, 26 Aug 2019 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Buffering the Sock Stream]]></title>
            <description><![CDATA[<p>How much space do I need to do my laundry?</p>


<p>It’s the end of week 4 of the quarter and you’ve decided it’s finally time to
do some laundry. You somehow transport the massive mound of clothing in your
closet to the laundry room, occupy all three machines and also the dishwasher,
and finally transport the now-slightly-less-smelly mound of clothing back up
the stairs to your room. Then you turn on your favorite Pink Floyd album and
begin folding…</p>
<p>You pull out items of clothing from the heap one by one. Shirts and pants you
can fold immediately, but socks pose some difficulty: you can’t put them away
until they have been paired. So you put them aside on your bedside table to be
processed later. Soon, however, the pile of socks on your bedside table grows
too large, and the table can only fit so much sock on it.</p>
<p>So you decide to try and manage the situation by eagerly pairing up socks. If
you pull a sock out of the heap of clothes and see its partner on the bedside
table, you pair them up and put them away. Otherwise, you put the unpaired sock
on the bedside table.</p>
<p>Suppose you own an infinite number of socks, each of which is uniformly one of
($ k $) colors. How big does your bedside table need to be? Let’s call the pile
of clean clothes the “stream,” and the bedside table the “buffer.” Then the
question is, what is the distribution of the buffer size over time as a
function of ($ k $)?</p>
<p>Here is one way to think about this: if ($ k = 1 $) then the buffer size is 0
half the time and 1 half the time; the distribution is normal at mean 0.5 and
variance 0.25. For larger ($ k $), we can think of this as just a superposition
of multiple buffers, one for each ($ k $). The means and variances add, so we
expect the distribution to be normal with mean ($ k/2 $) and standard deviation
($ \sqrt{k}/2 $).</p>
<p>Here is another way to think about this: we have a Markov process where the
transition from state ($ i $) to ($ i + 1 $) is with probability ($ (k - i)/k
$) and to ($ i - 1 $) is with probability ($ i/k $) for ($ 0 \leq i \leq k $).
We can create a matrix representing this transition system; then, eigenvectors
of this matrix (with eigenvalue 1, and normalized to sum 1) represent
equilibrium probability distributions.</p>
<p>So, incredibly, we’ve shown that the coordinates of the ($ \lambda=1 $)
eigenvector for this matrix can be computed using the normal CDF. What a
fascinating correspondence!</p>
<p>If you don’t believe it, the graph below shows the normal-CDF predictions
(orange line) and the elements of the eigenvector (blue dots) for ($ k = 100
$). The IPython notebook that generated this graph is
<a href="static/sock-graph.ipynb">here</a>.</p>
<p><img src="static/sock-graph.png" alt="sock graph"></p>
]]></description>
            <link>https://hardmath123.github.io/sock-buffer.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/sock-buffer.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Sat, 13 Apr 2019 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Windmills on my Mind]]></title>
            <description><![CDATA[<p>Some fast recreational physics</p>


<p>Quick back-of-the-envelope analysis of a windmill, just for fun.</p>
<p>According to <a href="https://www.wind-watch.org/faq-size.php">National Wind Watch</a> a
typical windmill is a GE 1.5-megawatt model which has 116-ft blades.</p>
<p>Let’s say the wind speed is ($ v $), which is typically 10-15 m/s on a good
day. Then in time ($ t $) the volume of air that passes across the windmill is
a cylinder of height ($ tv $) and area ($ \pi r^2$). Air’s density, ($ \rho $),
is around 1.225 kilograms per cubic meter. This gives a total mass of ($ M =
\rho tv\pi r^2 $). Taking ($ v $) to be a modest 10 m/s, this works out to 48
metric <em>tons</em> of air per <em>second</em>.</p>
<p>Now let’s apply conservation of energy on this mass of air. Initial energy is
($ Mv^2 / 2 $) and final energy is ($ Mw^2/2 $), and the difference is on the
order of the energy output, which is a function of the power output and ($ t $)
assuming reasonable efficiency in the turbine.</p>
<p>So, we have ($ \rho tv\pi r^2 v^2 / 2 - \rho tv\pi r^2 w^2 /2 = Pt $). Time
cancels, which is comforting since this is a continuous process. The equation
that’s left suggests that power output of a windmill is proportional to the
<em>cube</em> of the wind speed!</p>
<p>Another thing to think about is how much the wind slows down by. Solving for ($
w $) and taking ($ P $) to be 1.5 MW, we have ($ w = \sqrt{ \frac{2P/\rho \pi
r^2 - v^3 }{-v}} $). For 15 m/s winds, this means wind slows down by around 10%
because of the windmill.</p>
<p>Actually this wasn’t “just for fun,” it was to show off initial progress on my
side-project. :-)</p>
<p><img src="static/windmill-math.png" alt="screenshot"></p>
<p>Oh, and an unresolved question: because air is coming into the windmill faster
than it’s going out, the momentum flux across the plane of the windmill is net
negative.  Applying Gauss’ law, we would expect a buildup of air in the
windmill, which obviously doesn’t happen. What’s wrong?</p>
]]></description>
            <link>https://hardmath123.github.io/windmills.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/windmills.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Wed, 27 Mar 2019 07:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Caustics and Casinos on the I-5]]></title>
            <description><![CDATA[<p>An analytical analysis of an anomalous advertisement</p>


<p>I was in Los Angeles over winter break, and on the long drive back home I began
thinking about a billboard I saw just at the edge of the city advertising the
<strong>“closest casino to anywhere in LA.”</strong></p>
<p>This is a fascinating claim. Let me rephrase it, at least the way I interpret
it: the claim is that <em>wherever</em> you are in LA, the closest casino is the one
advertised on the billboard. (This is confusingly distinct from the claim that
the casino is closest to <em>anywhere</em> in LA, in the sense that the placement of
the casino minimizes the distance to the nearest bit of LA soil. Of course,
practically speaking this latter claim is useless because any casino <em>within</em>
LA trivially has the minimal distance of zero to “anywhere in LA.”)</p>
<p>The question is this: what region does the billboard’s claim imply is devoid of
casinos?</p>
<p>Let’s start with a simple case to get some intuition. Suppose LA is a
15-mile-radius disk, and the casino is at the center of the disk. Then,
according to the billboard, there must be <em>no</em> other casinos within LA’s
15-mile radius (otherwise, if you were in LA, you <em>might</em> be closer to that
other casino!). But actually, the claim is quite a bit stronger: there cannot
be any casinos within <em>thirty</em> miles of the center. Why? Well, suppose there
was a casino 20 miles from the center of LA. Then, someone just within the city
borders would be 5 miles away from that <em>other</em> casino, but 15 miles from the
city-center casino.</p>
<p>One more simple case: suppose LA is a line segment of length 15 miles, and the
casino is located at one endpoint. Can you imagine what the region in question
must be? It is a disk of radius 15 miles, centered at the <em>other</em> endpoint!
This is not entirely obvious, and you might need to draw a picture to convince
yourself that this is true.</p>
<p>Now let’s consider a much trickier case: suppose LA is a circle again, but the
casino is located along the <em>circumference</em>. Suddenly, it’s <em>much</em> harder to
picture what’s going on — sitting in a car without pencil or paper, I had no
idea what the region might look like. My instinct was “circle centered at the
diametrically-opposite point on the circumference,” but it turns out that this
is <em>wrong!</em></p>
<p>In the rest of this post, we’ll build up some mathematical machinery to answer
this question correctly. If you don’t want to work through the math, however,
then feel free to just scroll to the red-bordered squares and enjoy the
interactive demos.</p>
<hr>
<p><strong>Definition.</strong> Given some region ($ R \subset \mathbb{R}^2 $), the <em>casino
closure</em> with respect to some point ($ p \in \mathbb{R}^2 $) is defined as</p>
<p>\[
R^p = \left\{
  c \in \mathbb{R}^2 ~\middle|~
  \exists z \in R, || p - z || &gt; || c - z ||
\right\}
\]</p>
<p>Or, informally: the casino closure is the set of <em>possible casino locations</em> ($
c $) such that there exists a person ($ z $) in the region ($ R $) who is
closer to ($ c $) than to ($ p $).</p>
<p>From here on out, I’m only going to worry about “nice” regions, i.e. closed,
connected regions with smooth boundaries. I’m also going to use ($ x, p, z, c
$) to range over points in ($ \mathbb{R}^2 $), but really most math that
follows is equally applicable in ($ \mathbb{R}^n $). It’s just harder to
visualize.</p>
<p><strong>Lemma.</strong> Let the disk ($ D(z, r) $) be the set of all points in ($
\mathbb{R}^2 $) within ($ r $) of point ($ z $). Then,
\[
R^p = \bigcup_{z~\in~R} D(z, ||p - z||)
\]</p>
<p>That is, we can construct ($ R^p $) by combining all the disks at all the
points in ($ R $) that have ($ p $) on their circumference.</p>
<p><strong>Proof sketch.</strong> This should make sense “by construction.” If point ($ c $) is
in this constructed ($ R^p $), then it lies in some disk ($ d $), and thus the
point ($ z \in R $) that created disk ($ d $) fulfills the existence criterion
of our definition.</p>
<p><strong>Lemma.</strong> If region ($ R $) has a boundary ($ B \subset R $), then ($
B^p = R^p $).</p>
<p><strong>Proof sketch.</strong> Clearly, ($ B^p \subset R^p $) if you believe the first lemma
— adding more points to ($ B $) should only increase the union of disks.</p>
<p>The harder direction to show is ($ R^p \subset B^p $). Consider some point ($ z
\in R$). Extend the ray ($ \overrightarrow{pz} $) until it intersects with ($ B
$) at point ($ z^\prime $). Then, we can relate the disks created by ($ z $)
and ($ z^\prime $): ($ D(z, ||p-z||) \subset D(z^\prime, ||p-z^\prime||) $).
Why? Because the disk from ($ z $) is smaller and internally tangent to the
disk from ($ z^\prime $).</p>
<p>Mapping this argument over the entire union, it makes sense that ($ R^p \subset
B^p $).</p>
<p>Okay, time for some empirical verification of all this theory. Try drawing the
boundary of a region (in black) here! The disks will show up in red. Notice how
filling in your region with black dots doesn’t change the red blob at all.
(Click to clear.)</p>
<canvas id="world-freestyle" width=300 height=300 style="border: 1px solid red;"></canvas>

<hr>
<p>The boundary-circle lemma gives us a nicer characterization of ($ R^p $): it is
the region bounded by the curve that is tangent to all of the disks created by
the points on ($ B $). It turns out that there is a very nice mathematical
theory of “the curve that is tangent to all curves in a given family of
curves,” and that is the theory of <em>envelopes</em>. The account below is
paraphrased from <a href="https://www.jstor.org/stable/3617131?seq=1#metadata_info_tab_contents">“What is an
Envelope?”</a>,
a lovely 1981 paper.</p>
<p>Let the function ($ F(x, t) : \mathbb{R}^2 \times \mathbb{R} \rightarrow
\mathbb{R} $) define a family of curves parameterized by ($ t $), in the sense
that ($ F(x, 0) = 0 $) defines a curve and ($ F(x, 1) = 0 $) defines another
curve, and so on. Then, we seek to characterize the envelope curve which is
tangent to every curve in ($ F $).</p>
<p><strong>Lemma.</strong> If the boundary of ($ R $) can be (periodically) parameterized as ($
B(t), t \in \mathbb{R} $) then the boundary of ($ R^p $) is the <em>envelope</em> with
respect to ($ t $) of
\[
F(x, t) = || x - B(t) ||^2 - || p - B(t) ||^2
\]</p>
<p><strong>Proof sketch.</strong> Again, this should make sense “by construction”: ($ F $) is
chosen to correspond to circles centered at ($ B(t) $) passing through ($ p $).</p>
<hr>
<p>Ah, but how do we find the envelope? Here we need a tiny bit of multivariable
calculus.</p>
<p>Let ($ X(t) $) be the parameterization of the envelope of ($ F $). Then for all
($ t $), we have that ($ F(X(t), t) = 0 $) because the envelope must lie on the
respective curve in the family (“tangent” means “touch”!). We also have that
the curve ($ X(t) $) must be parallel to the member of family ($ F $) at ($ t
$). We can then express this condition by saying that the gradient (with
respect to ($ X $)) of ($ F $) at ($ t $) is perpendicular to the derivative of
($ X $) at ($ t $). Or:</p>
<p>\[
d X(t) / dt    \cdot    \nabla_X F(X(t), t) = 0
\]</p>
<p>In two dimensions, with ($ X(t) = (x(t), y(t)) $), this equation manifests
itself as ($ x^\prime(t)\partial F(x, y, t) / \partial x + y^\prime(t)\partial
F(x, y, t) / \partial y $).</p>
<p>The left hand side is oddly reminiscent of the multivariable chain rule.
Indeed, if we took the partial derivative of our equation ($ F(X(t), t) = 0 $)
with respect to ($ t $), we would get:</p>
<p>\[
dF(X, t)/dt = dX/dt\cdot\nabla_X F(X, t) + dt/dt\cdot \partial F(X, t)/\partial t = 0
\]</p>
<p>So we must have ($ \partial F(X, t) / \partial t = 0$).</p>
<p>There is a simpler but less rigorous derivation if you believe that the
envelope is exactly the points of intersection of infinitesimally close curves
in the family ($ F $). Then we want every point ($ X $) on the envelope to
satisfy both ($ F(X, t) $) and ($ F(X, t + \delta) $) for some ($ t $) and some
infinitesimal ($ \delta $). Taking the limit as ($ \delta $) approaches zero
gives the same condition that ($ \partial F(X, t) / \partial t = 0 $). The
paper above discusses how this notion is subtly different in some strange
cases, but it suffices to say that for all “nice” ($ R $), we’re fine.</p>
<p><strong>Almost-a-theorem.</strong> The boundary of ($ R^p $) is given by the parameterized
vectors ($ X $) that satisfy ($ F(X, t) = 0 $) and ($ \partial F (X, t) /
\partial t = 0 $) for the ($ F $) defined above.</p>
<p><strong>Almost-a-proof-sketch.</strong> Almost! In general, the solution for ($ X $) might
self-intersect, so we want to take only the “outermost” part of ($ X $). But
this is easy to work out on a case-by-case basis.</p>
<p><strong>Example.</strong> Suppose ($ R $) is the unit disk centered at ($ (a, 0) $), and ($
p $) is located at the origin. Then ($ B(t) = (\cos t + a, \sin t) $) and ($ p
= (0, 0) $). We have</p>
<p>\[
F((x, y), t) =
  ((x - (\cos t + a))^2 + (y - \sin t)^2) - ((\cos t + a)^2 + \sin^2 t)
\]</p>
<p>\[
F((x, y), t) = -2ax + x^2 - 2x\cos t + y^2 - 2y \sin t = 0
\]</p>
<p>We also have</p>
<p>\[
\partial F((x, y), t) / \partial t = 2x\sin t - 2y\cos t = 0
\]</p>
<p>Solving these by eliminating ($ t $) is a simple exercise in polar coordinates.
Discover from the second equation that ($ t = \theta $), then recall that ($
r^2 = x^2 + y^2 $). The resulting boundary of ($ R^p $) is (almost!) the curve
($ r = 2(1 + a\cos\theta) $). In other words, it’s (almost!) a limaçon! As ($ a
$) varies, the character of the limaçon varies, and at the critical points ($ a
= \pm 1 $), we get a cardioid with a cusp. Beyond those critical points, the
curve has an inner loop that we have to ignore; hence, “almost!”</p>
<p>Okay, time for more empiricism. Move your mouse around in the square below to
see how the relative placement of LA and the casino affects the envelope.
Notice also the inner loop predicted by the envelope, which we should of course
ignore for the purposes of bounding ($ R^p $).</p>
<canvas id="world-cardioid" width=300 height=300 style="border: 1px solid red;"></canvas>

<p>An amazing fact is that a cardioid is the same shape you get on the surface of
your coffee mug when you put it under a light! Well, not quite — it depends
subtly on where the light source is. Read more about caustics at
<a href="http://chalkdustmagazine.com/features/cardioids-coffee-cups/">Chalkdust</a>, from
whom I also borrowed the image below:</p>
<p><img src="https://i0.wp.com/chalkdustmagazine.com/wp-content/uploads/2017/10/photo-1.jpg?resize=768%2C576" alt="coffee
cardioid"></p>
<hr>
<p>Further reading: Wikipedia has <a href="https://en.wikipedia.org/wiki/Cardioid#Cardioid_as_envelope_of_a_pencil_of_circles">great
diagrams</a>
to accompany.  Dan Kalman’s article <a href="http://dankalman.net/AUhome/pdffiles/ladder_paper_MM.pdf">“Solving the Ladder Problem on the Back of
an Envelope.”</a>
includes a nice pedagogically-oriented discussion of envelope subtleties. It
cites Courant’s <a href="https://archive.org/details/DifferentialIntegralCalculusVol2/page/n183">Differential and Integral Calculus (vol
2)</a>,
which is freely available on Archive.org.</p>
<script>
  var worldFreestyle = document.getElementById('world-freestyle');
  var worldCardioid = document.getElementById('world-cardioid');

  function mark(world, ctx, X, Y) {
    var originX = world.width / 2;
    var originY = world.height / 2;

    ctx.strokeStyle = 'rgba(255, 0, 0, 0.3)'
    ctx.beginPath();
    ctx.arc(
      X, Y,
      Math.sqrt(
          (X - originX) * (X - originX) +
        (Y - originY) * (Y - originY)),
      0, Math.PI * 2
    );
    ctx.stroke();
    ctx.fillRect(X, Y, 2, 2);
  }

  function clear(world, ctx) {
    var originX = world.width / 2;
    var originY = world.height / 2;

    // Clear
    world.height = world.height;
    ctx.textAlign = 'center';
    ctx.textBaseline = 'middle';

    // Casino
    ctx.strokeRect(originX - 2, originY - 2, 4, 4);
    ctx.fillText("Casino", originX, originY - 12);
  }

  worldFreestyle.addEventListener('click', function(event) {
    var ctx = this.getContext('2d');
    clear(this, ctx);
  }, false);
  clear(worldFreestyle, worldFreestyle.getContext('2d'));
  worldFreestyle.addEventListener('mousemove', function(event) {
    var ctx = this.getContext('2d');
    var X = event.clientX - this.offsetLeft + window.scrollX;
    var Y = event.clientY - this.offsetTop  + window.scrollY;
    mark(this, ctx, X, Y);
  }, false);

  worldCardioid.addEventListener('mousemove', function(event) {
    var ctx = this.getContext('2d');
    var X = event.clientX - this.offsetLeft + window.scrollX;
    var Y = event.clientY - this.offsetTop  + window.scrollY;
    var originX = this.width / 2;
    var originY = this.height / 2;
    clear(this, ctx);

    // Region
    var R = this.width / 6;
    var N = 18;
    for (var i = 0; i < N; i++) {
      mark(
        this, ctx,
        X + Math.sin(i / N * Math.PI * 2) * R,
        Y + Math.cos(i / N * Math.PI * 2) * R
      );
    }

    // Circle
    ctx.beginPath();
    ctx.arc(X, Y, R, 0, Math.PI * 2);
    ctx.stroke();
    ctx.fillStyle = 'black';
    ctx.fillText("Los Angeles", X, Y - 12);


    // Limacon
    ctx.save();
    ctx.lineWidth = 10;
    ctx.beginPath();
    for (var theta = 0; theta < 2 * Math.PI; theta += 0.01) {
      var phi = Math.atan2(originX - X, originY - Y);
      var a = Math.sqrt(Math.pow(originX - X, 2) + Math.pow(originY - Y, 2)) / R;
      var r = 2 * (1 + a * Math.cos(theta)) * R;
      ctx.lineTo(
        originX + r * Math.cos(theta - phi - Math.PI / 2),
        originY + r * Math.sin(theta - phi - Math.PI / 2)
      );
    }
    ctx.stroke();
    ctx.restore();
  }, false);
</script>
]]></description>
            <link>https://hardmath123.github.io/envelope.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/envelope.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Fri, 04 Jan 2019 08:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Tuning a Typewriter]]></title>
            <description><![CDATA[<p>A Markovian adventure in optimizing historical typewriter layouts</p>


<p>I recently read this <a href="https://www.theatlantic.com/technology/archive/2016/11/chinese-computers/504851/">Atlantic
piece</a>
on input methods for computers, and it reminded me of a mathematical adventure
I had this summer that I should have blogged about at the time.</p>
<p>Back in July, I was exploring the City Museum of New York when I came across
this lovely Crown typewriter from the late 1800s. By our modern standards, it
uses a rather clunky input mechanism: you manually shift the pointer to the
character you want to type, press the button, rinse, repeat.</p>
<p><img src="static/crown-typewriter/crown-typewriter.png" alt="Crown Typewriter"></p>
<p>“What a waste of time,” you say. Ah, but even in the late 19th centure time was
the essential ingredient, and in the modern world there was no time. Notice,
then, that the designers of the Crown typewriter did not place the letters in
alphabetical order. Instead, the they placed commonly-together letters near
each other. You can type “AND” and “THE” rather quickly with just 2 shifts
apiece; “GIG,” on the other hand, takes 15 shifts from the G to the I, and 15
shifts back to the G.</p>
<p>The question, then, begs to be asked: is this the <em>most efficient</em> permutation
of letters? For example, Q and U are on opposite ends of the typewriter; surely
it would be better to put them together?</p>
<p>I have a doubly disappointing answer to this question: first, this is <em>not</em> the
most efficient permutation of letters (based on my digraph frequency map), but
I also don’t <em>know</em> what the most efficient permutation is! That is, I’ve found
permutations that are more efficient, but cannot prove that they are optimal
— after all, exploring all 26-factorial permutations is not an option.</p>
<p>My “cost” metric here is defined as “Ignoring non-alphabetic characters, how
many shifts does it take to type all of <em>Pride and Prejudice</em> followed by all
of <em>The Time Traveller</em> followed by all of <em>The Adventures of Sherlock
Holmes</em>?” (All three are classic 19th-century novels available from Project
Gutenberg.)</p>
<p>By this metric, the naive alphabetical order (<code>ABCDEFGHIJKLMNOPQRSTUVWXYZ</code>)
requires a whopping 9,630,941 shifts. In comparison, the Crown Typewriter
(<code>XQKGBPMCOFLANDTHERISUWYJVZ</code>) requires only 6,283,692 shifts. But the
ComfortablyNumbered typewriter (<code>ZKVGWCDNIAHTESROLUMFYBPXJQ</code>) requires a mere
5,499,341 shifts. This means we can save nearly one in eight shifts from the
Crown typewriter!</p>
<p>I found this permutation with the following algorithm: start with a random
permutation, then “optimize” it by repeatedly swapping characters such that the
post-swap permutation has a lower cost than the pre-swap permutation. Continue
“optimizing” until no further swaps can be made. Now, do this procedure for a
large number of random starting permutations, and pick the lowest-cost
optimized permutation.</p>
<p>Encouragingly, it turns out that many different random starts get optimized to
my solution. What do I mean by that? Well, the graph below shows two datasets:
the costs of 1,000 random permutations and the costs of those permutations once
optimized. Clearly, optimization is doing great good.</p>
<p><img src="static/crown-typewriter/typewriter-graph-1.png" alt="First graph"></p>
<p>But let’s zoom in on the optimized permutations. Notice that the two
lowest-cost bars are almost an order of magnitude more frequent than the
remaining bars. In fact, the lower-cost bar’s “bucket” in the histogram is
populated <em>only</em> with permutations of cost 5499341! This suggests that 5499341
is a “hard ceiling” for my optimizer, rather than the furthest point on the tip
of a long tail that my optimizer samples from.</p>
<p><img src="static/crown-typewriter/typewriter-graph-2.png" alt="Second graph"></p>
<p>Of course, this is no proof: there might be a single highly-efficient
permutation that is hard to reach by optimizing a random permutation. But that
<em>feels</em> unlikely!</p>
<p>So: I leave this as an open problem for readers to explore.</p>
<blockquote>
<p>Update (Dec 15): I found this <a href="https://yurichev.com/blog/cabling_Z3/">blog
post</a> by Dennis Yurichev that tackles
the same problem, but restated as “in what order do I mount these devices on
a rack if I want to minimize the total length of cables between them”? Dennis
finds an optimal solution for 8 devices with Z3… perhaps the solution
scales to 26 “devices”? Intriguingly, his post was published just weeks
before my visit to the City Museum!</p>
<p>Update (Jan 28): @rjp on Github has posted a <a href="http://rjp.is/blogging/posts/2019/01/linear-typewriters/">blog
post</a> with
<em>several</em> new approaches to this problem!</p>
</blockquote>
]]></description>
            <link>https://hardmath123.github.io/crown-typewriter.html</link>
            <guid isPermaLink="true">https://hardmath123.github.io/crown-typewriter.html</guid>
            <dc:creator><![CDATA[Hardmath123]]></dc:creator>
            <pubDate>Tue, 20 Nov 2018 08:00:00 GMT</pubDate>
        </item>
    </channel>
</rss>